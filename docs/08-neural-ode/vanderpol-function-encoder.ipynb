{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Neural ODEs for the Van der Pol Oscillator\n",
    "**Learning objectives**\n",
    "- Implement the function-encoder framework where a latent code summarises context pairs $(t_i, x_i)$ and conditions a Neural ODE\n",
    "- Generate Van der Pol trajectories across varying stiffness parameter $\\mu$ and initial states\n",
    "- Train a conditional Neural ODE that reconstructs full trajectories from a handful of context points\n",
    "- Compare against a vanilla Neural ODE baseline that lacks function encoding, highlighting why zero-shot conditioning matters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "47a46062",
   "source": "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kks32-courses/sciml/blob/main/docs/08-neural-ode/vanderpol-function-encoder.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation\n",
    "Zero-shot Neural ODEs learn an *encoder* that ingests a few function evaluations $(t_i, x_i)$ and produces a latent summary vector\n",
    "$$c = \\frac{1}{K} \\sum_{i=1}^{K} \\phi([t_i, x_i]).$$\n",
    "A conditional ODE $\\dot{x} = f_\\theta(x, t, c)$ then reconstructs the full trajectory and extrapolates to unseen timestamps or parameter values. This notebook rebuilds the Van der Pol example from scratch to show how the encoder, conditional dynamics, and baseline model interact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment setup\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import sys\nimport time\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nREPO_ROOT = Path.cwd()\nTORCHDIFFEQ_DIR = (REPO_ROOT / 'docs' / '08-neural-ode' / 'torchdiffeq').resolve()\nif TORCHDIFFEQ_DIR.exists() and str(TORCHDIFFEQ_DIR) not in sys.path:\n    sys.path.insert(0, str(TORCHDIFFEQ_DIR))\n\nfrom torchdiffeq import odeint\n\nplt.style.use('seaborn-v0_8')\nplt.rcParams.update({'axes.grid': True, 'figure.figsize': (6, 4)})\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nDEVICE"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def set_seed(seed=123):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Van der Pol dynamics and data generation\n",
    "The Van der Pol oscillator obeys\n",
    "$$\\dot{x} = y, \\qquad \\dot{y} = \\mu (1 - x^2) y - x$$\n",
    "where $\\mu$ controls nonlinearity/stiffness. We'll sample multiple $(\\mu, x_0, y_0)$ combinations, integrate the system with a high-accuracy Dormand–Prince solver, and store the full trajectories as ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VanDerPol(nn.Module):\n",
    "    def __init__(self, mu: float):\n",
    "        super().__init__()\n",
    "        self.register_buffer('mu', torch.tensor(mu, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        x = y[:, 0]\n",
    "        v = y[:, 1]\n",
    "        dx = v\n",
    "        dy = self.mu * (1 - x ** 2) * v - x\n",
    "        return torch.stack([dx, dy], dim=-1)\n",
    "\n",
    "\n",
    "def simulate_vdp(mu, y0, t_eval):\n",
    "    y0 = y0.unsqueeze(0)\n",
    "    dyn = VanDerPol(mu)\n",
    "    with torch.no_grad():\n",
    "        sol = odeint(dyn, y0, t_eval, method='dopri5', rtol=1e-7, atol=1e-9)\n",
    "    return sol.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "T_END = 20.0\n",
    "T_STEPS = 300\n",
    "T_GRID_CPU = torch.linspace(0.0, T_END, T_STEPS)\n",
    "DT = float((T_GRID_CPU[1] - T_GRID_CPU[0]).item())\n",
    "\n",
    "train_split = {'num_samples': 48, 'mu_range': (0.5, 3.0)}\n",
    "test_split = {'num_samples': 16, 'mu_range': (3.0, 5.0)}\n",
    "\n",
    "\n",
    "def sample_initial_state():\n",
    "    x0 = np.random.uniform(-2.0, 2.0)\n",
    "    v0 = np.random.uniform(-2.0, 2.0)\n",
    "    return torch.tensor([x0, v0], dtype=torch.float32)\n",
    "\n",
    "\n",
    "def build_dataset(num_samples, mu_range):\n",
    "    trajs, y0s, mus = [], [], []\n",
    "    for _ in range(num_samples):\n",
    "        mu = np.random.uniform(*mu_range)\n",
    "        y0 = sample_initial_state()\n",
    "        traj = simulate_vdp(mu, y0, T_GRID_CPU)\n",
    "        trajs.append(traj)\n",
    "        y0s.append(y0)\n",
    "        mus.append(mu)\n",
    "    return (\n",
    "        torch.stack(trajs, dim=0),\n",
    "        torch.stack(y0s, dim=0),\n",
    "        torch.tensor(mus, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "train_trajs, train_y0, train_mus = build_dataset(**train_split)\n",
    "test_trajs, test_y0, test_mus = build_dataset(**test_split)\n",
    "train_trajs.shape, test_trajs.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize sample trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_samples(trajs, title, n=3):\n",
    "    t_np = T_GRID_CPU.numpy()\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    for i in range(n):\n",
    "        plt.plot(t_np, trajs[i, :, 0], label=f'x sample {i}')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('time')\n",
    "    plt.ylabel('x(t)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_samples(train_trajs, 'Train trajectories (x component)')\n",
    "plot_samples(test_trajs, 'Held-out trajectories (x component)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset wrappers\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, trajs, y0s, mus):\n",
    "        self.trajs = trajs\n",
    "        self.y0s = y0s\n",
    "        self.mus = mus\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.trajs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'traj': self.trajs[idx],\n",
    "            'y0': self.y0s[idx],\n",
    "            'mu': self.mus[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = TrajectoryDataset(train_trajs, train_y0, train_mus)\n",
    "test_dataset = TrajectoryDataset(test_trajs, test_y0, test_mus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Context sampling helper\n",
    "The function encoder observes $K$ context points per trajectory. We randomly select indices, gather $(t, x, y)$ tuples, and normalize time to $[0, 1]$ before feeding them to the encoder (per Section 3 of the zero-shot paper).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function encoder and conditional dynamics (math view)\n",
    "Given context points $(t_i, x_i)$ we normalize time to $[0, 1]$, embed each pair with an MLP $\\phi([t_i, x_i])$, and average to obtain a latent code\n",
    "$$c = \\frac{1}{K} \\sum_{i=1}^{K} \\phi([t_i, x_i]).$$\n",
    "The conditional Neural ODE then integrates\n",
    "$$\\dot{x} = f_\\theta(x, t, c)$$\n",
    "with the same initial condition as the ground-truth system. Changing the context set changes $c$, so the decoder adapts to different $\\mu$ values without retraining. We'll compare this zero-shot model against a universal Neural ODE that lacks the latent code and therefore must memorize a single vector field.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "T_GRID = T_GRID_CPU.to(DEVICE)\n",
    "TIME_MIN = T_GRID[0]\n",
    "TIME_RANGE = T_GRID[-1] - T_GRID[0]\n",
    "\n",
    "\n",
    "def sample_contexts(traj_batch, num_context):\n",
    "    batch, steps, state_dim = traj_batch.shape\n",
    "    device = traj_batch.device\n",
    "    idx = torch.stack([torch.randperm(steps, device=device)[:num_context]\n",
    "                       for _ in range(batch)], dim=0)\n",
    "    idx, _ = torch.sort(idx, dim=1)\n",
    "    times = T_GRID[idx]\n",
    "    times_norm = ((times - TIME_MIN) / TIME_RANGE).unsqueeze(-1)\n",
    "    gather_idx = idx.unsqueeze(-1).expand(-1, -1, state_dim)\n",
    "    values = traj_batch.gather(1, gather_idx)\n",
    "    return times_norm, values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Function encoder + conditional Neural ODE\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class FunctionEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=16, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(3, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.project = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, t_ctx, x_ctx):\n",
    "        inp = torch.cat([t_ctx, x_ctx], dim=-1)\n",
    "        feats = self.embed(inp)\n",
    "        pooled = feats.mean(dim=1)\n",
    "        return self.project(pooled)\n",
    "\n",
    "\n",
    "class ConditionalODEFunc(nn.Module):\n",
    "    def __init__(self, latent_dim=16, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 2 + 1, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "        self.current_latent = None\n",
    "\n",
    "    def set_latent(self, latent):\n",
    "        self.current_latent = latent\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        if self.current_latent is None:\n",
    "            raise RuntimeError('latent code not set')\n",
    "        lat = self.current_latent\n",
    "        if lat.shape[0] != y.shape[0]:\n",
    "            lat = lat.expand(y.shape[0], -1)\n",
    "        t_input = torch.full((y.shape[0], 1), float(t), device=y.device)\n",
    "        inp = torch.cat([y, lat, t_input], dim=-1)\n",
    "        return self.net(inp)\n",
    "\n",
    "\n",
    "class ZeroShotVanDerPol(nn.Module):\n",
    "    def __init__(self, latent_dim=16, hidden_dim=64, solver='rk4', step_size=DT):\n",
    "        super().__init__()\n",
    "        self.encoder = FunctionEncoder(latent_dim, hidden_dim)\n",
    "        self.func = ConditionalODEFunc(latent_dim, hidden_dim)\n",
    "        self.solver = solver\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def forward(self, t_ctx, x_ctx, t_eval, y0):\n",
    "        latent = self.encoder(t_ctx, x_ctx)\n",
    "        self.func.set_latent(latent)\n",
    "        options = {'step_size': self.step_size} if self.solver == 'rk4' else None\n",
    "        traj = odeint(self.func, y0, t_eval, method=self.solver, options=options)\n",
    "        self.func.set_latent(None)\n",
    "        return traj.permute(1, 0, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Baseline: universal Neural ODE without function encoding\n",
    "To address the issue noted in the previous notebook, we also train a *vanilla* Neural ODE that uses a single vector field for every trajectory (no latent code, no context). This highlights how poorly it generalizes to unseen $\\mu$ values.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VanillaODEFunc(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2 + 1, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        t_input = torch.full((y.shape[0], 1), float(t), device=y.device)\n",
    "        inp = torch.cat([y, t_input], dim=-1)\n",
    "        return self.net(inp)\n",
    "\n",
    "\n",
    "class UniversalVanDerPol(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, solver='rk4', step_size=DT):\n",
    "        super().__init__()\n",
    "        self.func = VanillaODEFunc(hidden_dim)\n",
    "        self.solver = solver\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def forward(self, t_eval, y0):\n",
    "        options = {'step_size': self.step_size} if self.solver == 'rk4' else None\n",
    "        traj = odeint(self.func, y0, t_eval, method=self.solver, options=options)\n",
    "        return traj.permute(1, 0, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_zero_shot(model, dataset, epochs=150, batch_size=8, n_context=20, lr=1e-3):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    history = []\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in loader:\n",
    "            traj = batch['traj'].to(DEVICE)\n",
    "            y0 = batch['y0'].to(DEVICE)\n",
    "            t_ctx, x_ctx = sample_contexts(traj, n_context)\n",
    "            pred = model(t_ctx, x_ctx, T_GRID, y0)\n",
    "            loss = F.mse_loss(pred, traj)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= len(loader)\n",
    "        history.append(epoch_loss)\n",
    "        if epoch % 25 == 0:\n",
    "            print(f\"Zero-shot epoch {epoch:03d} | MSE {epoch_loss:.4f}\")\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_baseline(model, dataset, epochs=150, batch_size=8, lr=1e-3):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    history = []\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in loader:\n",
    "            traj = batch['traj'].to(DEVICE)\n",
    "            y0 = batch['y0'].to(DEVICE)\n",
    "            pred = model(T_GRID, y0)\n",
    "            loss = F.mse_loss(pred, traj)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= len(loader)\n",
    "        history.append(epoch_loss)\n",
    "        if epoch % 25 == 0:\n",
    "            print(f\"Baseline epoch {epoch:03d} | MSE {epoch_loss:.4f}\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train both models\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "set_seed(21)\n",
    "zero_shot_model = ZeroShotVanDerPol(latent_dim=16, hidden_dim=64).to(DEVICE)\n",
    "zs_history = train_zero_shot(zero_shot_model, train_dataset, epochs=150, batch_size=8, n_context=20, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "set_seed(21)\n",
    "baseline_model = UniversalVanDerPol(hidden_dim=64).to(DEVICE)\n",
    "baseline_history = train_baseline(baseline_model, train_dataset, epochs=150, batch_size=8, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training curves\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(zs_history, label='Zero-shot Neural ODE')\n",
    "plt.plot(baseline_history, label='Universal baseline')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.title('Training dynamics')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation on held-out $\\mu$\n",
    "We randomly sample new context points for each batch during evaluation to mimic the zero-shot setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_zero_shot(model, dataset, n_context=20):\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "    model.eval()\n",
    "    mses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            traj = batch['traj'].to(DEVICE)\n",
    "            y0 = batch['y0'].to(DEVICE)\n",
    "            t_ctx, x_ctx = sample_contexts(traj, n_context)\n",
    "            pred = model(t_ctx, x_ctx, T_GRID, y0)\n",
    "            mse = F.mse_loss(pred, traj, reduction='none').mean(dim=[1, 2])\n",
    "            mses.append(mse.cpu())\n",
    "    return torch.cat(mses).mean().item()\n",
    "\n",
    "\n",
    "def evaluate_baseline(model, dataset):\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "    model.eval()\n",
    "    mses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            traj = batch['traj'].to(DEVICE)\n",
    "            y0 = batch['y0'].to(DEVICE)\n",
    "            pred = model(T_GRID, y0)\n",
    "            mse = F.mse_loss(pred, traj, reduction='none').mean(dim=[1, 2])\n",
    "            mses.append(mse.cpu())\n",
    "    return torch.cat(mses).mean().item()\n",
    "\n",
    "train_mse_zs = evaluate_zero_shot(zero_shot_model, train_dataset)\n",
    "test_mse_zs = evaluate_zero_shot(zero_shot_model, test_dataset)\n",
    "train_mse_base = evaluate_baseline(baseline_model, train_dataset)\n",
    "test_mse_base = evaluate_baseline(baseline_model, test_dataset)\n",
    "\n",
    "print(f\"Zero-shot model   | train MSE {train_mse_zs:.4f} | test MSE {test_mse_zs:.4f}\")\n",
    "print(f\"Universal baseline | train MSE {train_mse_base:.4f} | test MSE {test_mse_base:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Qualitative comparison on a held-out trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_prediction(sample_idx=0):\n",
    "    sample = test_dataset[sample_idx]\n",
    "    traj = sample['traj'].unsqueeze(0).to(DEVICE)\n",
    "    y0 = sample['y0'].unsqueeze(0).to(DEVICE)\n",
    "    t_ctx, x_ctx = sample_contexts(traj, num_context=20)\n",
    "    with torch.no_grad():\n",
    "        zs_pred = zero_shot_model(t_ctx, x_ctx, T_GRID, y0).squeeze(0).cpu()\n",
    "        base_pred = baseline_model(T_GRID, y0).squeeze(0).cpu()\n",
    "    true_traj = traj.squeeze(0).cpu()\n",
    "    t_np = T_GRID_CPU.numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "    axes[0].plot(t_np, true_traj[:, 0], label='true x(t)')\n",
    "    axes[0].plot(t_np, zs_pred[:, 0], '--', label='zero-shot')\n",
    "    axes[0].set_title('Zero-shot vs truth (x component)')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(t_np, true_traj[:, 0], label='true x(t)')\n",
    "    axes[1].plot(t_np, base_pred[:, 0], '--', label='baseline')\n",
    "    axes[1].set_title('Baseline vs truth (x component)')\n",
    "    axes[1].legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_prediction(sample_idx=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase portrait\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_phase(sample_idx=1):\n",
    "    sample = test_dataset[sample_idx]\n",
    "    traj = sample['traj'].unsqueeze(0).to(DEVICE)\n",
    "    y0 = sample['y0'].unsqueeze(0).to(DEVICE)\n",
    "    t_ctx, x_ctx = sample_contexts(traj, num_context=20)\n",
    "    with torch.no_grad():\n",
    "        zs_pred = zero_shot_model(t_ctx, x_ctx, T_GRID, y0).squeeze(0).cpu()\n",
    "        base_pred = baseline_model(T_GRID, y0).squeeze(0).cpu()\n",
    "    true_traj = traj.squeeze(0).cpu()\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(true_traj[:, 0], true_traj[:, 1], label='true')\n",
    "    plt.plot(zs_pred[:, 0], zs_pred[:, 1], '--', label='zero-shot')\n",
    "    plt.plot(base_pred[:, 0], base_pred[:, 1], ':', label='baseline')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Phase portrait comparison (held-out $\\mu$)')\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "plot_phase(sample_idx=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Exercises\n",
    "- **Context budget:** vary the number of context points $K\\in\\{5, 10, 20, 40\\}$ and measure test MSE to reproduce the ablation from the paper.\n",
    "- **Latent prior:** add an $\\ell_2$ penalty on the latent code to avoid degenerate solutions and inspect the effect on extrapolation.\n",
    "- **Adaptive solvers:** swap the RK4 solver for `dopri5` and track the number of function evaluations vs. accuracy.\n",
    "- **Different dynamics:** replace Van der Pol with FitzHugh–Nagumo or Lotka–Volterra to test how well the function encoder transfers.\n",
    "\n",
    "These prompts align with the other course sections and encourage deeper experimentation with zero-shot Neural ODEs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}