{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B2B Operator Learning: Basis-to-Basis Transformations\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master the B2B (Basis-to-Basis) framework for operator learning\n",
    "- Learn operators as explicit transformation matrices\n",
    "- Implement the same examples as DeepONet for direct comparison\n",
    "- Understand zero-shot and few-shot operator learning\n",
    "- Compare B2B with DeepONet on performance and interpretability\n",
    "\n",
    "**Examples covered (same as DeepONet):**\n",
    "1. Derivative operator\n",
    "2. Poisson equation solver  \n",
    "3. 1D nonlinear Darcy flow\n",
    "\n",
    "---\n",
    "\n",
    "## The B2B Framework\n",
    "\n",
    "**Core idea:** Decompose operator learning into three steps:\n",
    "\n",
    "1. **Encode source:** $f \\xrightarrow{E_1} c_f \\in \\mathbb{R}^{n_1}$\n",
    "2. **Transform:** $c_f \\xrightarrow{A} c_g \\in \\mathbb{R}^{n_2}$\n",
    "3. **Decode target:** $c_g \\xrightarrow{D_2} g$\n",
    "\n",
    "The operator $\\mathcal{G}$ is represented as: $\\mathcal{G}[f] \\approx D_2(A \\cdot E_1(f))$\n",
    "\n",
    "**Key advantage:** The transformation matrix $A$ is explicit and interpretable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.stats import multivariate_normal\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                     \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Function Encoder Architecture\n",
    "\n",
    "First, we need function encoders to learn representations of source and target function spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionEncoder(nn.Module):\n",
    "    \"\"\"Function encoder for learning basis representations\"\"\"\n",
    "    \n",
    "    def __init__(self, sensor_dim, n_basis, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.sensor_dim = sensor_dim\n",
    "        self.n_basis = n_basis\n",
    "        \n",
    "        # Encoder: maps function samples to coefficients\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(sensor_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, n_basis)\n",
    "        )\n",
    "        \n",
    "        # Decoder: generates basis functions at query points\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, n_basis)\n",
    "        )\n",
    "    \n",
    "    def encode(self, function_samples):\n",
    "        \"\"\"Extract coefficients from function samples\"\"\"\n",
    "        return self.encoder(function_samples)\n",
    "    \n",
    "    def decode_basis(self, x):\n",
    "        \"\"\"Get basis function values at points x\"\"\"\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def reconstruct(self, coefficients, x):\n",
    "        \"\"\"Reconstruct function from coefficients\"\"\"\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        \n",
    "        batch_size, n_points, _ = x.shape\n",
    "        basis_values = self.decoder(x.reshape(-1, 1))\n",
    "        basis_values = basis_values.view(batch_size, n_points, self.n_basis)\n",
    "        \n",
    "        if coefficients.dim() == 1:\n",
    "            coefficients = coefficients.unsqueeze(0)\n",
    "        \n",
    "        return torch.einsum('bn,bpn->bp', coefficients, basis_values)\n",
    "    \n",
    "    def forward(self, function_samples, query_points):\n",
    "        coeffs = self.encode(function_samples)\n",
    "        return self.reconstruct(coeffs, query_points)\n",
    "\n",
    "\n",
    "class B2BOperator:\n",
    "    \"\"\"B2B Operator Learning Framework\"\"\"\n",
    "    \n",
    "    def __init__(self, source_encoder, target_encoder):\n",
    "        self.source_encoder = source_encoder\n",
    "        self.target_encoder = target_encoder\n",
    "        self.transformation_matrix = None\n",
    "    \n",
    "    def learn_transformation(self, source_functions, target_functions, regularization=1e-6):\n",
    "        \"\"\"Learn transformation matrix A using least squares\"\"\"\n",
    "        \n",
    "        self.source_encoder.eval()\n",
    "        self.target_encoder.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode all functions\n",
    "            source_coeffs = self.source_encoder.encode(source_functions)\n",
    "            target_coeffs = self.target_encoder.encode(target_functions)\n",
    "        \n",
    "        # Solve least squares: Y = X @ A.T\n",
    "        # Add regularization for stability\n",
    "        X = source_coeffs.cpu()\n",
    "        Y = target_coeffs.cpu()\n",
    "        \n",
    "        # Regularized least squares\n",
    "        XtX = X.T @ X + regularization * torch.eye(X.shape[1])\n",
    "        XtY = X.T @ Y\n",
    "        A = torch.linalg.solve(XtX, XtY).T\n",
    "        \n",
    "        self.transformation_matrix = A.to(device)\n",
    "        \n",
    "        # Compute fitting error\n",
    "        Y_pred = X @ A.T\n",
    "        mse = F.mse_loss(Y_pred, Y).item()\n",
    "        \n",
    "        return A, mse\n",
    "    \n",
    "    def apply(self, source_function, query_points):\n",
    "        \"\"\"Apply the learned operator\"\"\"\n",
    "        if self.transformation_matrix is None:\n",
    "            raise ValueError(\"Transformation matrix not learned yet\")\n",
    "        \n",
    "        self.source_encoder.eval()\n",
    "        self.target_encoder.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode source\n",
    "            source_coeffs = self.source_encoder.encode(source_function)\n",
    "            \n",
    "            # Transform\n",
    "            target_coeffs = source_coeffs @ self.transformation_matrix.T\n",
    "            \n",
    "            # Decode\n",
    "            return self.target_encoder.reconstruct(target_coeffs, query_points)\n",
    "\n",
    "\n",
    "print(\"B2B Framework initialized\")\n",
    "print(\"Components: Encoder → Transformation → Decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Helper Functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder(encoder, functions, x_points, n_epochs=500, lr=1e-3, name=\"Encoder\"):\n",
    "    \"\"\"Train a function encoder to reconstruct functions\"\"\"\n",
    "    \n",
    "    optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.5)\n",
    "    \n",
    "    losses = []\n",
    "    encoder.train()\n",
    "    \n",
    "    x_tensor = torch.tensor(x_points, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "    \n",
    "    pbar = tqdm(range(n_epochs), desc=f\"Training {name}\")\n",
    "    for epoch in pbar:\n",
    "        # Random batch\n",
    "        idx = np.random.choice(len(functions), min(32, len(functions)))\n",
    "        batch_functions = torch.tensor(functions[idx], dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Prepare query points\n",
    "        batch_x = x_tensor.unsqueeze(0).repeat(len(idx), 1, 1)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = encoder(batch_functions, batch_x)\n",
    "        loss = F.mse_loss(pred, batch_functions)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.6f}'})\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "def visualize_basis_functions(encoder, x_range=(-2, 2), name=\"Encoder\"):\n",
    "    \"\"\"Visualize learned basis functions\"\"\"\n",
    "    \n",
    "    x = torch.linspace(x_range[0], x_range[1], 200).unsqueeze(-1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        basis = encoder.decode_basis(x).cpu().numpy()\n",
    "    \n",
    "    x = x.cpu().numpy().squeeze()\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(min(basis.shape[1], 10)):\n",
    "        plt.plot(x, basis[:, i], linewidth=2, alpha=0.7, label=f'φ_{i+1}')\n",
    "    \n",
    "    plt.title(f'{name} Basis Functions')\n",
    "    plt.xlabel('x')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if basis.shape[1] <= 10:\n",
    "        plt.legend(ncol=2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: The Derivative Operator\n",
    "\n",
    "Same as DeepONet: Learn $\\mathcal{D}[u] = \\frac{du}{dx}$ for cubic polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate polynomial data (same as DeepONet)\n",
    "def generate_polynomial_data(num_functions=2000, num_points=100, x_range=(-2, 2)):\n",
    "    \"\"\"Generate cubic polynomials and their derivatives\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    coeffs = np.random.randn(num_functions, 4) * 0.5\n",
    "    x = np.linspace(x_range[0], x_range[1], num_points)\n",
    "    \n",
    "    functions = np.zeros((num_functions, num_points))\n",
    "    derivatives = np.zeros((num_functions, num_points))\n",
    "    \n",
    "    for i in range(num_functions):\n",
    "        a, b, c, d = coeffs[i]\n",
    "        functions[i] = a * x**3 + b * x**2 + c * x + d\n",
    "        derivatives[i] = 3 * a * x**2 + 2 * b * x + c\n",
    "    \n",
    "    return coeffs, x, functions, derivatives\n",
    "\n",
    "print(\"=== DERIVATIVE OPERATOR EXAMPLE ===\")\n",
    "coeffs, x, functions, derivatives = generate_polynomial_data()\n",
    "\n",
    "# Split data\n",
    "n_train = 1600\n",
    "train_functions = functions[:n_train]\n",
    "train_derivatives = derivatives[:n_train]\n",
    "test_functions = functions[n_train:]\n",
    "test_derivatives = derivatives[n_train:]\n",
    "\n",
    "print(f\"Data: {n_train} training, {len(test_functions)} test functions\")\n",
    "print(f\"Domain: x ∈ [{x[0]:.1f}, {x[-1]:.1f}]\")\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "for i in range(3):\n",
    "    ax = axes[i]\n",
    "    ax.plot(x, functions[i], 'b-', linewidth=2, label='f(x)')\n",
    "    ax.plot(x, derivatives[i], 'r-', linewidth=2, label=\"f'(x)\")\n",
    "    ax.set_title(f'Sample {i+1}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining function encoders...\")\n",
    "\n",
    "# Create encoders\n",
    "derivative_source_encoder = FunctionEncoder(sensor_dim=100, n_basis=4, hidden_dim=32).to(device)\n",
    "derivative_target_encoder = FunctionEncoder(sensor_dim=100, n_basis=3, hidden_dim=32).to(device)\n",
    "\n",
    "# Train source encoder (cubic polynomials)\n",
    "print(\"\\n1. Source encoder (cubic space):\")\n",
    "source_losses = train_encoder(derivative_source_encoder, train_functions, x, \n",
    "                             n_epochs=300, name=\"Cubic Encoder\")\n",
    "\n",
    "# Train target encoder (quadratic polynomials)\n",
    "print(\"\\n2. Target encoder (quadratic space):\")\n",
    "target_losses = train_encoder(derivative_target_encoder, train_derivatives, x, \n",
    "                             n_epochs=300, name=\"Quadratic Encoder\")\n",
    "\n",
    "# Visualize basis functions\n",
    "visualize_basis_functions(derivative_source_encoder, name=\"Source (Cubic)\")\n",
    "visualize_basis_functions(derivative_target_encoder, name=\"Target (Quadratic)\")\n",
    "\n",
    "print(f\"\\nFinal losses - Source: {source_losses[-1]:.6f}, Target: {target_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn and Apply the Derivative Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create B2B operator\n",
    "derivative_b2b = B2BOperator(derivative_source_encoder, derivative_target_encoder)\n",
    "\n",
    "# Learn transformation matrix\n",
    "print(\"Learning transformation matrix...\")\n",
    "train_source_tensor = torch.tensor(train_functions, dtype=torch.float32).to(device)\n",
    "train_target_tensor = torch.tensor(train_derivatives, dtype=torch.float32).to(device)\n",
    "\n",
    "A_derivative, fit_error = derivative_b2b.learn_transformation(\n",
    "    train_source_tensor, train_target_tensor\n",
    ")\n",
    "\n",
    "print(f\"Transformation matrix shape: {A_derivative.shape}\")\n",
    "print(f\"Fitting error: {fit_error:.6f}\")\n",
    "\n",
    "# Visualize transformation matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(A_derivative.cpu().numpy(), cmap='RdBu_r', aspect='auto')\n",
    "plt.colorbar(label='Weight')\n",
    "plt.title('Derivative Operator Transformation Matrix')\n",
    "plt.xlabel('Source Basis (Cubic)')\n",
    "plt.ylabel('Target Basis (Quadratic)')\n",
    "\n",
    "# Annotate values\n",
    "for i in range(A_derivative.shape[0]):\n",
    "    for j in range(A_derivative.shape[1]):\n",
    "        plt.text(j, i, f'{A_derivative[i,j].item():.2f}', \n",
    "                ha='center', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Derivative Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on unseen functions\n",
    "n_test_vis = 6\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "test_errors = []\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "\n",
    "for i in range(n_test_vis):\n",
    "    test_idx = i * 10\n",
    "    \n",
    "    # Apply B2B operator\n",
    "    source_func = torch.tensor(test_functions[test_idx:test_idx+1], dtype=torch.float32).to(device)\n",
    "    pred_derivative = derivative_b2b.apply(source_func, x_tensor).squeeze().cpu().numpy()\n",
    "    \n",
    "    true_derivative = test_derivatives[test_idx]\n",
    "    \n",
    "    # Compute error\n",
    "    mse = np.mean((pred_derivative - true_derivative)**2)\n",
    "    test_errors.append(mse)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i]\n",
    "    ax.plot(x, test_functions[test_idx], 'b-', linewidth=2, alpha=0.7, label='f(x)')\n",
    "    ax.plot(x, true_derivative, 'g-', linewidth=2, label=\"True f'(x)\")\n",
    "    ax.plot(x, pred_derivative, 'r--', linewidth=2, label=\"B2B f'(x)\")\n",
    "    \n",
    "    ax.set_title(f'Test {i+1}: MSE = {mse:.6f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "plt.suptitle('B2B Derivative Operator Results', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average test MSE: {np.mean(test_errors):.6f} ± {np.std(test_errors):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Poisson Equation Solver\n",
    "\n",
    "Learn the solution operator for the Poisson equation:\n",
    "$$-\\nabla^2 u = f \\text{ in } \\Omega, \\quad u = 0 \\text{ on } \\partial\\Omega$$\n",
    "\n",
    "In 1D: $-\\frac{d^2u}{dx^2} = f(x)$ with $u(0) = u(1) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poisson_data(n_samples=1000, n_points=100):\n",
    "    \"\"\"Generate Poisson equation data\"\"\"\n",
    "    \n",
    "    x = np.linspace(0, 1, n_points)\n",
    "    dx = x[1] - x[0]\n",
    "    \n",
    "    # Create finite difference matrix for -d²/dx²\n",
    "    main_diag = 2 * np.ones(n_points - 2) / dx**2\n",
    "    off_diag = -np.ones(n_points - 3) / dx**2\n",
    "    A_fd = diags([off_diag, main_diag, off_diag], [-1, 0, 1]).toarray()\n",
    "    A_inv = np.linalg.inv(A_fd)\n",
    "    \n",
    "    sources = []\n",
    "    solutions = []\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Generate random source function (combination of sines)\n",
    "        f = np.zeros(n_points)\n",
    "        n_modes = np.random.randint(2, 6)\n",
    "        for k in range(n_modes):\n",
    "            mode = np.random.randint(1, 10)\n",
    "            amplitude = np.random.randn()\n",
    "            phase = np.random.rand() * 2 * np.pi\n",
    "            f += amplitude * np.sin(mode * np.pi * x + phase)\n",
    "        \n",
    "        # Solve Poisson equation\n",
    "        u = np.zeros(n_points)\n",
    "        u[1:-1] = A_inv @ f[1:-1]\n",
    "        \n",
    "        sources.append(f)\n",
    "        solutions.append(u)\n",
    "    \n",
    "    return np.array(sources), np.array(solutions), x\n",
    "\n",
    "\n",
    "print(\"\\n=== POISSON EQUATION EXAMPLE ===\")\n",
    "poisson_sources, poisson_solutions, x_poisson = generate_poisson_data(n_samples=1500)\n",
    "\n",
    "# Split data\n",
    "n_train_poisson = 1200\n",
    "train_sources_poisson = poisson_sources[:n_train_poisson]\n",
    "train_solutions_poisson = poisson_solutions[:n_train_poisson]\n",
    "test_sources_poisson = poisson_sources[n_train_poisson:]\n",
    "test_solutions_poisson = poisson_solutions[n_train_poisson:]\n",
    "\n",
    "print(f\"Data: {n_train_poisson} training, {len(test_sources_poisson)} test\")\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "for i in range(3):\n",
    "    ax = axes[i]\n",
    "    ax.plot(x_poisson, poisson_sources[i], 'r-', linewidth=2, label='f(x)')\n",
    "    ax.plot(x_poisson, poisson_solutions[i], 'b-', linewidth=2, label='u(x)')\n",
    "    ax.set_title(f'Sample {i+1}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "plt.suptitle('Poisson Equation: Source → Solution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Encoders for Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Poisson encoders...\")\n",
    "\n",
    "# Create encoders with more basis functions for this problem\n",
    "poisson_source_encoder = FunctionEncoder(sensor_dim=100, n_basis=15, hidden_dim=64).to(device)\n",
    "poisson_solution_encoder = FunctionEncoder(sensor_dim=100, n_basis=15, hidden_dim=64).to(device)\n",
    "\n",
    "# Train encoders\n",
    "print(\"1. Source encoder (f space):\")\n",
    "poisson_source_losses = train_encoder(poisson_source_encoder, train_sources_poisson, x_poisson,\n",
    "                                     n_epochs=400, name=\"Source Encoder\")\n",
    "\n",
    "print(\"\\n2. Solution encoder (u space):\")\n",
    "poisson_solution_losses = train_encoder(poisson_solution_encoder, train_solutions_poisson, x_poisson,\n",
    "                                       n_epochs=400, name=\"Solution Encoder\")\n",
    "\n",
    "print(f\"\\nFinal losses - Source: {poisson_source_losses[-1]:.6f}, Solution: {poisson_solution_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn and Apply Poisson Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create B2B operator for Poisson\n",
    "poisson_b2b = B2BOperator(poisson_source_encoder, poisson_solution_encoder)\n",
    "\n",
    "# Learn transformation\n",
    "print(\"Learning Poisson transformation matrix...\")\n",
    "train_source_poisson_tensor = torch.tensor(train_sources_poisson, dtype=torch.float32).to(device)\n",
    "train_solution_poisson_tensor = torch.tensor(train_solutions_poisson, dtype=torch.float32).to(device)\n",
    "\n",
    "A_poisson, fit_error_poisson = poisson_b2b.learn_transformation(\n",
    "    train_source_poisson_tensor, train_solution_poisson_tensor\n",
    ")\n",
    "\n",
    "print(f\"Transformation matrix shape: {A_poisson.shape}\")\n",
    "print(f\"Fitting error: {fit_error_poisson:.6f}\")\n",
    "\n",
    "# Visualize transformation matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Matrix heatmap\n",
    "ax = axes[0]\n",
    "im = ax.imshow(A_poisson.cpu().numpy(), cmap='RdBu_r', aspect='auto')\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_title('Poisson Solver Transformation Matrix')\n",
    "ax.set_xlabel('Source Basis')\n",
    "ax.set_ylabel('Solution Basis')\n",
    "\n",
    "# Singular values\n",
    "ax = axes[1]\n",
    "U, S, Vt = torch.linalg.svd(A_poisson.cpu())\n",
    "ax.bar(range(len(S)), S.numpy())\n",
    "ax.set_title('Singular Values')\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Poisson Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Poisson solver\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "poisson_test_errors = []\n",
    "x_poisson_tensor = torch.tensor(x_poisson, dtype=torch.float32).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "\n",
    "for i in range(6):\n",
    "    test_idx = i * 10\n",
    "    \n",
    "    # Apply B2B operator\n",
    "    source_func = torch.tensor(test_sources_poisson[test_idx:test_idx+1], dtype=torch.float32).to(device)\n",
    "    pred_solution = poisson_b2b.apply(source_func, x_poisson_tensor).squeeze().cpu().numpy()\n",
    "    \n",
    "    true_solution = test_solutions_poisson[test_idx]\n",
    "    \n",
    "    # Compute error\n",
    "    mse = np.mean((pred_solution - true_solution)**2)\n",
    "    rel_error = np.sqrt(mse) / np.sqrt(np.mean(true_solution**2) + 1e-8)\n",
    "    poisson_test_errors.append(rel_error)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i]\n",
    "    ax.plot(x_poisson, test_sources_poisson[test_idx], 'r-', linewidth=2, alpha=0.7, label='f(x)')\n",
    "    ax.plot(x_poisson, true_solution, 'b-', linewidth=2, label='True u(x)')\n",
    "    ax.plot(x_poisson, pred_solution, 'g--', linewidth=2, label='B2B u(x)')\n",
    "    \n",
    "    ax.set_title(f'Test {i+1}: Rel. Error = {rel_error:.4f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "plt.suptitle('B2B Poisson Solver Results', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average relative error: {np.mean(poisson_test_errors):.4f} ± {np.std(poisson_test_errors):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: 1D Nonlinear Darcy Flow\n",
    "\n",
    "Same as DeepONet: Solve the nonlinear Darcy equation with solution-dependent permeability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_darcy_data(n_funcs=1000, n_points=40):\n",
    "    \"\"\"Generate 1D nonlinear Darcy flow data\"\"\"\n",
    "    \n",
    "    def permeability(s):\n",
    "        return 0.2 + s**2\n",
    "    \n",
    "    # Gaussian process for source function\n",
    "    x = np.linspace(0, 1, n_points)\n",
    "    l, sigma = 0.04, 1.0\n",
    "    K = sigma**2 * np.exp(-0.5 * (x[:, None] - x[None, :])**2 / l**2)\n",
    "    K += 1e-6 * np.eye(n_points)\n",
    "    \n",
    "    def solve_darcy(u_func):\n",
    "        dx = x[1] - x[0]\n",
    "        s = np.zeros(n_points)\n",
    "        \n",
    "        for _ in range(100):  # Fixed point iteration\n",
    "            kappa = permeability(s)\n",
    "            main_diag = (kappa[1:] + kappa[:-1]) / dx**2\n",
    "            upper_diag = -kappa[1:-1] / dx**2\n",
    "            lower_diag = -kappa[1:-1] / dx**2\n",
    "            \n",
    "            A = diags([lower_diag, main_diag, upper_diag], [-1, 0, 1], \n",
    "                     shape=(n_points-2, n_points-2))\n",
    "            \n",
    "            s_interior = spsolve(A, u_func[1:-1])\n",
    "            s_new = np.zeros(n_points)\n",
    "            s_new[1:-1] = s_interior\n",
    "            s = 0.5 * s_new + 0.5 * s\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    # Generate dataset\n",
    "    np.random.seed(42)\n",
    "    U, S = [], []\n",
    "    \n",
    "    print(\"Generating Darcy dataset...\")\n",
    "    for i in tqdm(range(n_funcs), desc=\"Solving PDEs\"):\n",
    "        u = multivariate_normal.rvs(mean=np.zeros(n_points), cov=K)\n",
    "        s = solve_darcy(u)\n",
    "        U.append(u)\n",
    "        S.append(s)\n",
    "    \n",
    "    return np.array(U), np.array(S), x\n",
    "\n",
    "\n",
    "print(\"\\n=== 1D NONLINEAR DARCY EXAMPLE ===\")\n",
    "darcy_sources, darcy_solutions, x_darcy = generate_darcy_data(n_funcs=1000)\n",
    "\n",
    "# Split data\n",
    "n_train_darcy = 800\n",
    "train_sources_darcy = darcy_sources[:n_train_darcy]\n",
    "train_solutions_darcy = darcy_solutions[:n_train_darcy]\n",
    "test_sources_darcy = darcy_sources[n_train_darcy:]\n",
    "test_solutions_darcy = darcy_solutions[n_train_darcy:]\n",
    "\n",
    "print(f\"\\nData: {n_train_darcy} training, {len(test_sources_darcy)} test\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "for i in range(3):\n",
    "    ax = axes[i]\n",
    "    ax.plot(x_darcy, darcy_sources[i], 'g-', linewidth=2, label='f(x)')\n",
    "    ax.plot(x_darcy, darcy_solutions[i], 'b-', linewidth=2, label='u(x)')\n",
    "    ax.set_title(f'Sample {i+1}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "plt.suptitle('1D Nonlinear Darcy Flow')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Encoders for Darcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Darcy encoders...\")\n",
    "\n",
    "# Create encoders\n",
    "darcy_source_encoder = FunctionEncoder(sensor_dim=40, n_basis=20, hidden_dim=128).to(device)\n",
    "darcy_solution_encoder = FunctionEncoder(sensor_dim=40, n_basis=20, hidden_dim=128).to(device)\n",
    "\n",
    "# Train\n",
    "print(\"1. Source encoder:\")\n",
    "darcy_source_losses = train_encoder(darcy_source_encoder, train_sources_darcy, x_darcy,\n",
    "                                   n_epochs=500, lr=0.001, name=\"Darcy Source\")\n",
    "\n",
    "print(\"\\n2. Solution encoder:\")\n",
    "darcy_solution_losses = train_encoder(darcy_solution_encoder, train_solutions_darcy, x_darcy,\n",
    "                                     n_epochs=500, lr=0.001, name=\"Darcy Solution\")\n",
    "\n",
    "print(f\"\\nFinal losses - Source: {darcy_source_losses[-1]:.6f}, Solution: {darcy_solution_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn and Apply Darcy Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create B2B operator for Darcy\n",
    "darcy_b2b = B2BOperator(darcy_source_encoder, darcy_solution_encoder)\n",
    "\n",
    "# Learn transformation\n",
    "print(\"Learning Darcy transformation matrix...\")\n",
    "train_source_darcy_tensor = torch.tensor(train_sources_darcy, dtype=torch.float32).to(device)\n",
    "train_solution_darcy_tensor = torch.tensor(train_solutions_darcy, dtype=torch.float32).to(device)\n",
    "\n",
    "A_darcy, fit_error_darcy = darcy_b2b.learn_transformation(\n",
    "    train_source_darcy_tensor, train_solution_darcy_tensor\n",
    ")\n",
    "\n",
    "print(f\"Transformation matrix shape: {A_darcy.shape}\")\n",
    "print(f\"Fitting error: {fit_error_darcy:.6f}\")\n",
    "\n",
    "# Analyze transformation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "im = ax.imshow(A_darcy.cpu().numpy(), cmap='RdBu_r', aspect='auto')\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_title('Darcy Operator Transformation Matrix')\n",
    "ax.set_xlabel('Source Basis')\n",
    "ax.set_ylabel('Solution Basis')\n",
    "\n",
    "ax = axes[1]\n",
    "U, S, Vt = torch.linalg.svd(A_darcy.cpu())\n",
    "ax.bar(range(len(S)), S.numpy())\n",
    "ax.set_title('Singular Values')\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMatrix rank: {torch.linalg.matrix_rank(A_darcy).item()}\")\n",
    "print(f\"Condition number: {torch.linalg.cond(A_darcy).item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Darcy Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Darcy operator\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "darcy_test_errors = []\n",
    "x_darcy_tensor = torch.tensor(x_darcy, dtype=torch.float32).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "\n",
    "for i in range(6):\n",
    "    test_idx = i * 5\n",
    "    \n",
    "    # Apply B2B\n",
    "    source_func = torch.tensor(test_sources_darcy[test_idx:test_idx+1], dtype=torch.float32).to(device)\n",
    "    pred_solution = darcy_b2b.apply(source_func, x_darcy_tensor).squeeze().cpu().numpy()\n",
    "    \n",
    "    true_solution = test_solutions_darcy[test_idx]\n",
    "    \n",
    "    # Error\n",
    "    mse = np.mean((pred_solution - true_solution)**2)\n",
    "    rel_error = np.sqrt(mse) / np.sqrt(np.mean(true_solution**2) + 1e-8)\n",
    "    darcy_test_errors.append(rel_error)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i]\n",
    "    ax.plot(x_darcy, test_sources_darcy[test_idx], 'g-', linewidth=2, alpha=0.7, label='f(x)')\n",
    "    ax.plot(x_darcy, true_solution, 'b-', linewidth=2, label='True u(x)')\n",
    "    ax.plot(x_darcy, pred_solution, 'r--', linewidth=2, label='B2B u(x)')\n",
    "    \n",
    "    ax.set_title(f'Test {i+1}: Rel. Error = {rel_error:.4f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "plt.suptitle('B2B Darcy Operator Results', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average relative error: {np.mean(darcy_test_errors):.4f} ± {np.std(darcy_test_errors):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot and Few-Shot Learning\n",
    "\n",
    "One of B2B's key advantages: learn new operators with minimal data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_few_shot_learning(source_encoder, target_encoder, \n",
    "                          train_sources, train_targets,\n",
    "                          test_sources, test_targets, \n",
    "                          x_points, operator_name=\"Operator\"):\n",
    "    \"\"\"Test operator learning with varying amounts of data\"\"\"\n",
    "    \n",
    "    sample_sizes = [5, 10, 25, 50, 100, 200, 400]\n",
    "    errors_by_size = []\n",
    "    \n",
    "    x_tensor = torch.tensor(x_points, dtype=torch.float32).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "    \n",
    "    for n_samples in sample_sizes:\n",
    "        if n_samples > len(train_sources):\n",
    "            continue\n",
    "            \n",
    "        # Learn with limited data\n",
    "        b2b = B2BOperator(source_encoder, target_encoder)\n",
    "        \n",
    "        train_source_subset = torch.tensor(train_sources[:n_samples], dtype=torch.float32).to(device)\n",
    "        train_target_subset = torch.tensor(train_targets[:n_samples], dtype=torch.float32).to(device)\n",
    "        \n",
    "        b2b.learn_transformation(train_source_subset, train_target_subset)\n",
    "        \n",
    "        # Test\n",
    "        test_errors = []\n",
    "        for i in range(min(50, len(test_sources))):\n",
    "            source = torch.tensor(test_sources[i:i+1], dtype=torch.float32).to(device)\n",
    "            pred = b2b.apply(source, x_tensor).squeeze().cpu().numpy()\n",
    "            true = test_targets[i]\n",
    "            \n",
    "            mse = np.mean((pred - true)**2)\n",
    "            rel_error = np.sqrt(mse) / (np.sqrt(np.mean(true**2)) + 1e-8)\n",
    "            test_errors.append(rel_error)\n",
    "        \n",
    "        avg_error = np.mean(test_errors)\n",
    "        errors_by_size.append((n_samples, avg_error))\n",
    "        print(f\"{n_samples:3d} samples: {avg_error:.6f}\")\n",
    "    \n",
    "    return errors_by_size\n",
    "\n",
    "\n",
    "print(\"\\n=== FEW-SHOT LEARNING EXPERIMENTS ===\")\n",
    "\n",
    "print(\"\\nDerivative Operator:\")\n",
    "deriv_few_shot = test_few_shot_learning(\n",
    "    derivative_source_encoder, derivative_target_encoder,\n",
    "    train_functions, train_derivatives,\n",
    "    test_functions, test_derivatives,\n",
    "    x, \"Derivative\"\n",
    "\n",
    ")\n",
    "\n",
    "print(\"\\nPoisson Solver:\")\n",
    "poisson_few_shot = test_few_shot_learning(\n",
    "    poisson_source_encoder, poisson_solution_encoder,\n",
    "    train_sources_poisson, train_solutions_poisson,\n",
    "    test_sources_poisson, test_solutions_poisson,\n",
    "    x_poisson, \"Poisson\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Few-Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot few-shot learning results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Derivative\n",
    "ax = axes[0]\n",
    "sizes, errors = zip(*deriv_few_shot)\n",
    "ax.plot(sizes, errors, 'o-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Number of Training Samples')\n",
    "ax.set_ylabel('Test Relative Error')\n",
    "ax.set_title('Derivative Operator - Few-Shot Learning')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0.01, color='r', linestyle='--', alpha=0.5, label='1% error')\n",
    "ax.legend()\n",
    "\n",
    "# Poisson\n",
    "ax = axes[1]\n",
    "sizes, errors = zip(*poisson_few_shot)\n",
    "ax.plot(sizes, errors, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "ax.set_xlabel('Number of Training Samples')\n",
    "ax.set_ylabel('Test Relative Error')\n",
    "ax.set_title('Poisson Solver - Few-Shot Learning')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0.01, color='r', linestyle='--', alpha=0.5, label='1% error')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: B2B achieves good accuracy with very few samples!\")\n",
    "print(\"This is because the encoders already capture the function space structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Summary: B2B vs DeepONet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = [\n",
    "    ['Aspect', 'B2B Framework', 'DeepONet'],\n",
    "    ['Architecture', 'Encoder → Transform → Decoder', 'Branch-Trunk'],\n",
    "    ['Operator Representation', 'Explicit matrix A', 'Implicit in weights'],\n",
    "    ['Interpretability', 'High (visible transformation)', 'Medium (basis visible)'],\n",
    "    ['Few-shot learning', 'Excellent', 'Limited'],\n",
    "    ['Transfer learning', 'Natural (reuse encoders)', 'Difficult'],\n",
    "    ['Training', 'Two-stage', 'End-to-end'],\n",
    "    ['Best for', 'Linear/weakly nonlinear', 'Highly nonlinear']\n",
    "]\n",
    "\n",
    "# Display as formatted table\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=comparison_data[1:], colLabels=comparison_data[0],\n",
    "                cellLoc='left', loc='center', colWidths=[0.25, 0.375, 0.375])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Style header\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor('#40466e')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Alternate row colors\n",
    "for i in range(1, len(comparison_data)):\n",
    "    for j in range(3):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#f0f0f0')\n",
    "\n",
    "plt.title('B2B vs DeepONet Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDerivative Operator:\")\n",
    "print(f\"  B2B Test MSE: {np.mean(test_errors):.6f}\")\n",
    "print(f\"  With 10 samples: {dict(deriv_few_shot)[10]:.6f}\")\n",
    "\n",
    "print(f\"\\nPoisson Solver:\")\n",
    "print(f\"  B2B Test Error: {np.mean(poisson_test_errors):.4f}\")\n",
    "print(f\"  With 25 samples: {dict(poisson_few_shot)[25]:.4f}\")\n",
    "\n",
    "print(f\"\\nDarcy Flow:\")\n",
    "print(f\"  B2B Test Error: {np.mean(darcy_test_errors):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "1. **B2B Framework** decomposes operator learning into:\n",
    "   - Function encoding (learning basis representations)\n",
    "   - Transformation learning (explicit matrix)\n",
    "   - Function decoding (reconstruction from coefficients)\n",
    "\n",
    "2. **Three Examples** (same as DeepONet):\n",
    "   - **Derivative operator:** Perfect for B2B (linear operator)\n",
    "   - **Poisson solver:** Good performance with interpretable structure\n",
    "   - **Darcy flow:** Handles nonlinearity through learned representations\n",
    "\n",
    "3. **Key Advantages:**\n",
    "   - **Interpretability:** Transformation matrix reveals operator structure\n",
    "   - **Few-shot learning:** Excellent performance with minimal data\n",
    "   - **Transfer learning:** Reuse encoders for related operators\n",
    "   - **Modularity:** Separate concerns enable flexibility\n",
    "\n",
    "### When to Use B2B\n",
    "\n",
    "✅ **Ideal for:**\n",
    "- Linear or approximately linear operators\n",
    "- Multiple related operators on same spaces\n",
    "- Limited training data scenarios\n",
    "- Need for interpretable operator representations\n",
    "- Transfer learning applications\n",
    "\n",
    "❌ **Consider alternatives when:**\n",
    "- Operators are highly nonlinear\n",
    "- Single operator with abundant data\n",
    "- End-to-end optimization preferred\n",
    "\n",
    "### Practical Guidelines\n",
    "\n",
    "1. **Basis size selection:**\n",
    "   - Start with expected dimensionality\n",
    "   - Increase if reconstruction error is high\n",
    "   - Use cross-validation for optimal size\n",
    "\n",
    "2. **Encoder training:**\n",
    "   - Ensure good reconstruction before operator learning\n",
    "   - Pre-train on diverse function samples\n",
    "   - Share encoders across related problems\n",
    "\n",
    "3. **Transformation learning:**\n",
    "   - Use regularization for stability\n",
    "   - Check singular values for conditioning\n",
    "   - Visualize matrix for insights\n",
    "\n",
    "---\n",
    "\n",
    "**The Big Picture:** B2B provides an interpretable, sample-efficient alternative to DeepONet, especially powerful for linear operators and transfer learning scenarios!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}