{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B2B Operator Learning: Basis-to-Basis Transformations\n",
    "\n",
    "\n",
    "**Exercise:** [![Open in Colab](https://img.shields.io/badge/Open%20in-Colab-F9AB00?style=flat-square&logo=googlecolab)](https://colab.research.google.com/github/kks32-courses/sciml/blob/main/docs/03-function-encoder/03-b2b-operator-exercise.ipynb)\n",
    "**Solution:** [![Open in Colab](https://img.shields.io/badge/Open%20in-Colab-F9AB00?style=flat-square&logo=googlecolab)](https://colab.research.google.com/github/kks32-courses/sciml/blob/main/docs/03-function-encoder/03-b2b-operator.ipynb)\n",
    "\n",
    "**Slides:** [![View Slides](https://img.shields.io/badge/View-Presentation-yellow?style=flat-square&logo=googleslides&logoColor=white)](https://raw.githubusercontent.com/kks32-courses/sciml/main/docs/03-function-encoder/b2b.pdf)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master the B2B (Basis-to-Basis) framework for operator learning\n",
    "- Learn operators as explicit transformation matrices\n",
    "- Implement the same examples as DeepONet for direct comparison\n",
    "- Understand zero-shot and few-shot operator learning\n",
    "- Compare B2B with DeepONet on performance and interpretability\n",
    "\n",
    "**Examples covered:**\n",
    "1. Derivative operator\n",
    "2. Poisson equation solver  \n",
    "3. 1D nonlinear Darcy flow\n",
    "\n",
    "---\n",
    "\n",
    "## The B2B Framework\n",
    "\n",
    "**Core idea:** Decompose operator learning into three steps:\n",
    "\n",
    "1. **Encode source:** $f \\xrightarrow{E_1} c_f \\in \\mathbb{R}^{n_1}$\n",
    "2. **Transform:** $c_f \\xrightarrow{A} c_g \\in \\mathbb{R}^{n_2}$\n",
    "3. **Decode target:** $c_g \\xrightarrow{D_2} g$\n",
    "\n",
    "The operator $\\mathcal{G}$ is represented as: $\\mathcal{G}[f] \\approx D_2(A \\cdot E_1(f))$\n",
    "\n",
    "**Key advantage:** The transformation matrix $A$ is explicit and interpretable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: B2B Operator Learning Framework\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "The B2B framework learns operators between function spaces by decomposing the problem into basis representations. Given an operator $\\mathcal{T}: \\mathcal{U} \\rightarrow \\mathcal{V}$, we learn:\n",
    "\n",
    "1. **Source encoding:** $u \\mapsto \\alpha^u = E_\\mathcal{U}(u)$\n",
    "2. **Transformation:** $\\alpha^u \\mapsto \\alpha^v = A \\alpha^u$  \n",
    "3. **Target decoding:** $\\alpha^v \\mapsto v = D_\\mathcal{V}(\\alpha^v)$\n",
    "\n",
    "The complete operator: $\\mathcal{T}[u] \\approx D_\\mathcal{V}(A \\cdot E_\\mathcal{U}(u))$\n",
    "\n",
    "### Learning the Transformation Matrix\n",
    "\n",
    "Given training pairs $\\{(u_i, v_i)\\}_{i=1}^N$, we solve:\n",
    "\n",
    "$$\\min_A \\sum_{i=1}^N \\|E_\\mathcal{V}(v_i) - A \\cdot E_\\mathcal{U}(u_i)\\|^2 + \\lambda \\|A\\|_F^2$$\n",
    "\n",
    "Solution via regularized least squares:\n",
    "$$A = (C_U^T C_U + \\lambda I)^{-1} C_U^T C_V$$\n",
    "\n",
    "where $C_U$ and $C_V$ are matrices of encoded coefficients.\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "- **Interpretability:** The matrix $A$ explicitly represents the operator's action\n",
    "- **Sample efficiency:** Pre-trained encoders enable few-shot operator learning\n",
    "- **Transfer learning:** Encoders can be reused across related operators\n",
    "- **Spectral analysis:** SVD of $A$ reveals operator characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:21.241978Z",
     "iopub.status.busy": "2025-10-14T11:44:21.241659Z",
     "iopub.status.idle": "2025-10-14T11:44:22.473460Z",
     "shell.execute_reply": "2025-10-14T11:44:22.473203Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.stats import multivariate_normal\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                     \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Function Encoder Architecture\n",
    "\n",
    "First, we need function encoders to learn representations of source and target function spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:22.490813Z",
     "iopub.status.busy": "2025-10-14T11:44:22.490652Z",
     "iopub.status.idle": "2025-10-14T11:44:22.495589Z",
     "shell.execute_reply": "2025-10-14T11:44:22.495376Z"
    }
   },
   "outputs": [],
   "source": [
    "class FunctionEncoder(nn.Module):\n",
    "    \"\"\"Function encoder for learning basis representations\"\"\"\n",
    "    \n",
    "    def __init__(self, sensor_dim, n_basis, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.sensor_dim = sensor_dim\n",
    "        self.n_basis = n_basis\n",
    "        \n",
    "        # Encoder: maps function samples to coefficients\n",
    "        \n",
    "        \n",
    "        # Decoder: generates basis functions at query points\n",
    "        \n",
    "    \n",
    "    def encode(self, function_samples):\n",
    "        \"\"\"Extract coefficients from function samples\"\"\"\n",
    "        return self.encoder(function_samples)\n",
    "    \n",
    "    def decode_basis(self, x):\n",
    "        \"\"\"Get basis function values at points x\"\"\"\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def reconstruct(self, coefficients, x):\n",
    "        \"\"\"Reconstruct function from coefficients\"\"\"\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, function_samples, query_points):\n",
    "        \n",
    "\n",
    "\n",
    "class B2BOperator:\n",
    "    \"\"\"B2B Operator Learning Framework\"\"\"\n",
    "    \n",
    "    def __init__(self, source_encoder, target_encoder):\n",
    "        self.source_encoder = source_encoder\n",
    "        self.target_encoder = target_encoder\n",
    "        self.transformation_matrix = None\n",
    "    \n",
    "    def learn_transformation(self, source_functions, target_functions, regularization=1e-6):\n",
    "        \"\"\"Learn transformation matrix A using least squares\"\"\"\n",
    "        \n",
    "        self.source_encoder.eval()\n",
    "        self.target_encoder.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode all functions\n",
    "            source_coeffs = self.source_encoder.encode(source_functions)\n",
    "            target_coeffs = self.target_encoder.encode(target_functions)\n",
    "        \n",
    "        # Solve least squares: Y = X @ A.T\n",
    "        # Add regularization for stability\n",
    "        X = source_coeffs.cpu()\n",
    "        Y = target_coeffs.cpu()\n",
    "        \n",
    "        # Regularized least squares\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Compute fitting error\n",
    "       \n",
    "       \n",
    "        \n",
    "        return A, mse\n",
    "    \n",
    "    def apply(self, source_function, query_points):\n",
    "        \"\"\"Apply the learned operator\"\"\"\n",
    "        if self.transformation_matrix is None:\n",
    "            raise ValueError(\"Transformation matrix not learned yet\")\n",
    "        \n",
    "        self.source_encoder.eval()\n",
    "        self.target_encoder.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode source\n",
    "            source_coeffs = self.source_encoder.encode(source_function)\n",
    "            \n",
    "            # Transform\n",
    "            target_coeffs = source_coeffs @ self.transformation_matrix.T\n",
    "            \n",
    "            # Decode\n",
    "            return self.target_encoder.reconstruct(target_coeffs, query_points)\n",
    "\n",
    "\n",
    "print(\"B2B Framework initialized\")\n",
    "print(\"Components: Encoder → Transformation → Decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Helper Functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:22.496842Z",
     "iopub.status.busy": "2025-10-14T11:44:22.496755Z",
     "iopub.status.idle": "2025-10-14T11:44:22.500226Z",
     "shell.execute_reply": "2025-10-14T11:44:22.500027Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_encoder(encoder, functions, x_points, n_epochs=500, lr=1e-3, name=\"Encoder\"):\n",
    "    \"\"\"Train a function encoder to reconstruct functions\"\"\"\n",
    "    \n",
    "    optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.5)\n",
    "    \n",
    "    losses = []\n",
    "    encoder.train()\n",
    "    \n",
    "    x_tensor = torch.tensor(x_points, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "    \n",
    "    pbar = tqdm(range(n_epochs), desc=f\"Training {name}\")\n",
    "    for epoch in pbar:\n",
    "        # Random batch\n",
    "        idx = np.random.choice(len(functions), min(32, len(functions)))\n",
    "        batch_functions = torch.tensor(functions[idx], dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Prepare query points\n",
    "        batch_x = x_tensor.unsqueeze(0).repeat(len(idx), 1, 1)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = encoder(batch_functions, batch_x)\n",
    "        loss = F.mse_loss(pred, batch_functions)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.6f}'})\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "def visualize_basis_functions(encoder, x_range=(-2, 2), name=\"Encoder\"):\n",
    "    \"\"\"Visualize learned basis functions\"\"\"\n",
    "    \n",
    "    x = torch.linspace(x_range[0], x_range[1], 200).unsqueeze(-1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        basis = encoder.decode_basis(x).cpu().numpy()\n",
    "    \n",
    "    x = x.cpu().numpy().squeeze()\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(min(basis.shape[1], 10)):\n",
    "        plt.plot(x, basis[:, i], linewidth=2, alpha=0.7, label=f'φ_{i+1}')\n",
    "    \n",
    "    plt.title(f'{name} Basis Functions')\n",
    "    plt.xlabel('x')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if basis.shape[1] <= 10:\n",
    "        plt.legend(ncol=2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Derivative Operator\n",
    "\n",
    "Learn $\\mathcal{T}[f] = f'$ where $f'(x) = \\frac{df}{dx}$ (derivative operator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:22.501330Z",
     "iopub.status.busy": "2025-10-14T11:44:22.501255Z",
     "iopub.status.idle": "2025-10-14T11:44:22.625796Z",
     "shell.execute_reply": "2025-10-14T11:44:22.625572Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate derivative data - SAME AS DEEPONET\n",
    "def generate_derivative_data(num_functions=2000, num_points=100, x_range=(-2, 2)):\n",
    "    \"\"\"Generate cubic polynomials and their derivatives (same as DeepONet)\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Random cubic polynomial coefficients - SAME AS DEEPONET\n",
    "    coeffs = np.random.randn(num_functions, 4) * 0.5\n",
    "    x = np.linspace(x_range[0], x_range[1], num_points)\n",
    "    \n",
    "    functions = np.zeros((num_functions, num_points))\n",
    "    derivatives = np.zeros((num_functions, num_points))\n",
    "    \n",
    "    for i in range(num_functions):\n",
    "        a, b, c, d = coeffs[i]\n",
    "        # f(x) = ax^3 + bx^2 + cx + d\n",
    "        functions[i] = a * x**3 + b * x**2 + c * x + d\n",
    "        # f'(x) = 3ax^2 + 2bx + c\n",
    "        derivatives[i] = 3 * a * x**2 + 2 * b * x + c\n",
    "    \n",
    "    return coeffs, x, functions, derivatives\n",
    "\n",
    "print(\"=== DERIVATIVE OPERATOR EXAMPLE ===\")\n",
    "print(\"Learning: f(x) → f'(x)\")\n",
    "coeffs, x, functions, derivatives = generate_derivative_data()\n",
    "\n",
    "# Split data - same 80/20 split \n",
    "n_train = int(0.8 * len(functions))\n",
    "train_functions = functions[:n_train]\n",
    "train_derivatives = derivatives[:n_train]\n",
    "test_functions = functions[n_train:]\n",
    "test_derivatives = derivatives[n_train:]\n",
    "\n",
    "print(f\"Data: {n_train} training, {len(test_functions)} test functions\")\n",
    "print(f\"Domain: x ∈ [{x[0]:.1f}, {x[-1]:.1f}]\")\n",
    "print(f\"Coefficients: ax³ + bx² + cx + d with scale 0.5\")\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "for i in range(3):\n",
    "    ax = axes[i]\n",
    "    ax.plot(x, functions[i], 'b-', linewidth=2, label='f(x)')\n",
    "    ax.plot(x, derivatives[i], 'r-', linewidth=2, label=\"f'(x)\")\n",
    "    a, b, c, d = coeffs[i]\n",
    "    ax.set_title(f'Sample {i+1}: [{a:.2f},{b:.2f},{c:.2f},{d:.2f}]')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "plt.suptitle('Derivative Operator: Function → Derivative', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:22.626980Z",
     "iopub.status.busy": "2025-10-14T11:44:22.626901Z",
     "iopub.status.idle": "2025-10-14T11:44:25.721749Z",
     "shell.execute_reply": "2025-10-14T11:44:25.721469Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nTraining function encoders...\")\n",
    "\n",
    "# Create encoders - use small basis first like DeepONet (p=3)\n",
    "# Then increase if needed\n",
    "deriv_source_encoder = FunctionEncoder(sensor_dim=100, n_basis=10, hidden_dim=64).to(device)\n",
    "deriv_target_encoder = FunctionEncoder(sensor_dim=100, n_basis=10, hidden_dim=64).to(device)\n",
    "\n",
    "# Train source encoder (cubic polynomials)\n",
    "print(\"\\n1. Source encoder (cubic space - functions):\")\n",
    "source_losses = train_encoder(deriv_source_encoder, train_functions, x, \n",
    "                             n_epochs=500, lr=0.001, name=\"Function Encoder\")\n",
    "\n",
    "# Train target encoder (quadratic polynomials - derivatives)\n",
    "print(\"\\n2. Target encoder (quadratic space - derivatives):\")\n",
    "target_losses = train_encoder(deriv_target_encoder, train_derivatives, x, \n",
    "                             n_epochs=500, lr=0.001, name=\"Derivative Encoder\")\n",
    "\n",
    "# Visualize basis functions\n",
    "visualize_basis_functions(deriv_source_encoder, x_range=(-2, 2), name=\"Source (Cubic)\")\n",
    "visualize_basis_functions(deriv_target_encoder, x_range=(-2, 2), name=\"Target (Quadratic)\")\n",
    "\n",
    "print(f\"\\nFinal losses - Source: {source_losses[-1]:.6f}, Target: {target_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn and Apply the Derivative Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:25.723059Z",
     "iopub.status.busy": "2025-10-14T11:44:25.722926Z",
     "iopub.status.idle": "2025-10-14T11:44:25.999642Z",
     "shell.execute_reply": "2025-10-14T11:44:25.999427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create B2B operator\n",
    "deriv_b2b = B2BOperator(deriv_source_encoder, deriv_target_encoder)\n",
    "\n",
    "# Learn transformation matrix\n",
    "print(\"Learning transformation matrix...\")\n",
    "train_source_tensor = torch.tensor(train_functions, dtype=torch.float32).to(device)\n",
    "train_target_tensor = torch.tensor(train_derivatives, dtype=torch.float32).to(device)\n",
    "\n",
    "A_deriv, fit_error = deriv_b2b.learn_transformation(\n",
    "    train_source_tensor, train_target_tensor\n",
    ")\n",
    "\n",
    "print(f\"Transformation matrix shape: {A_deriv.shape}\")\n",
    "print(f\"Fitting error: {fit_error:.6f}\")\n",
    "\n",
    "# Visualize transformation matrix and SVD\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Matrix heatmap\n",
    "ax = axes[0]\n",
    "im = ax.imshow(A_deriv.cpu().numpy(), cmap='RdBu_r', aspect='auto')\n",
    "plt.colorbar(im, ax=ax, label='Weight')\n",
    "ax.set_title('Derivative Operator Transformation Matrix')\n",
    "ax.set_xlabel('Source Basis')\n",
    "ax.set_ylabel('Target Basis')\n",
    "\n",
    "# Singular values (line plot)\n",
    "ax = axes[1]\n",
    "U, S, Vt = torch.linalg.svd(A_deriv.cpu())\n",
    "ax.plot(range(1, len(S)+1), S.numpy(), 'o-', linewidth=2, markersize=6)\n",
    "ax.set_title('Singular Value Decay')\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Singular Value')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative energy\n",
    "ax = axes[2]\n",
    "cumsum = torch.cumsum(S**2, dim=0) / torch.sum(S**2)\n",
    "ax.plot(range(1, len(S)+1), cumsum.numpy(), 'o-', linewidth=2, markersize=6, color='green')\n",
    "ax.axhline(0.99, color='r', linestyle='--', label='99% energy')\n",
    "ax.set_title('Cumulative Energy')\n",
    "ax.set_xlabel('Number of Singular Values')\n",
    "ax.set_ylabel('Fraction of Total Energy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMatrix rank: {torch.linalg.matrix_rank(A_deriv).item()}\")\n",
    "print(f\"Condition number: {torch.linalg.cond(A_deriv).item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Derivative Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:26.000894Z",
     "iopub.status.busy": "2025-10-14T11:44:26.000808Z",
     "iopub.status.idle": "2025-10-14T11:44:26.193947Z",
     "shell.execute_reply": "2025-10-14T11:44:26.193717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test on unseen functions\n",
    "n_test_vis = 6\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "test_errors = []\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "\n",
    "for i in range(n_test_vis):\n",
    "    test_idx = i * 10\n",
    "    \n",
    "    # Apply B2B operator\n",
    "    source_func = torch.tensor(test_functions[test_idx:test_idx+1], dtype=torch.float32).to(device)\n",
    "    pred_deriv = deriv_b2b.apply(source_func, x_tensor).squeeze().cpu().numpy()\n",
    "    \n",
    "    true_deriv = test_derivatives[test_idx]\n",
    "    \n",
    "    # Compute error\n",
    "    mse = np.mean((pred_deriv - true_deriv)**2)\n",
    "    test_errors.append(mse)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i]\n",
    "    ax.plot(x, test_functions[test_idx], 'b-', linewidth=2, alpha=0.7, label='f(x)')\n",
    "    ax.plot(x, true_deriv, 'r-', linewidth=2, label=\"True f'(x)\")\n",
    "    ax.plot(x, pred_deriv, 'g--', linewidth=2, label=\"B2B f'(x)\")\n",
    "    \n",
    "    ax.set_title(f'Test {i+1}: MSE = {mse:.6f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "plt.suptitle('B2B Derivative Operator Results', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average test MSE: {np.mean(test_errors):.6f} ± {np.std(test_errors):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Matrix Decompositions: SVD and Eigendecomposition\n",
    "\n",
    "Before we compare transfer methods, let's build intuition for two fundamental matrix decompositions and how they're used in the B2B framework.\n",
    "\n",
    "---\n",
    "\n",
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "**The geometric picture:** Every matrix $A$ describes a linear transformation. SVD reveals this transformation as three simple steps:\n",
    "\n",
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "1. **$V^T$**: Rotate input to align with \"principal input directions\"\n",
    "2. **$\\Sigma$**: Stretch along these directions (diagonal matrix of singular values)\n",
    "3. **$U$**: Rotate to \"principal output directions\"\n",
    "\n",
    "**Key insight:** Any linear transformation = rotation → stretch → rotation.\n",
    "\n",
    "**Example in B2B:** Our transformation matrix $A: \\mathbb{R}^{10} \\to \\mathbb{R}^{10}$ maps source coefficients to target coefficients.\n",
    "\n",
    "- Columns of $V$ are the \"best input modes\" (right singular vectors)\n",
    "- Columns of $U$ are the corresponding \"output modes\" (left singular vectors)  \n",
    "- Diagonal entries $\\sigma_i$ in $\\Sigma$ measure the importance of each mode\n",
    "\n",
    "**Why it matters:**\n",
    "- **Rank and compactness**: Small singular values $\\sigma_i \\approx 0$ indicate the operator is compact—those directions contribute little\n",
    "- **Regularization**: Drop small $\\sigma_i$ to remove noise and improve stability\n",
    "- **Conditioning**: The ratio $\\sigma_{\\text{max}}/\\sigma_{\\text{min}}$ reveals numerical sensitivity\n",
    "\n",
    "**The rank-$k$ approximation:**\n",
    "$$A \\approx U_k \\Sigma_k V_k^T = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$$\n",
    "\n",
    "This expresses the operator as a sum of rank-1 operators, ordered by importance.\n",
    "\n",
    "---\n",
    "\n",
    "### Two Ways to Use SVD in B2B\n",
    "\n",
    "**Approach 1: Post-hoc SVD Analysis (What we use for B2B)**\n",
    "\n",
    "After learning the transformation matrix $A$ via least squares, compute its SVD to analyze the operator:\n",
    "\n",
    "1. Train encoders independently for input and output spaces\n",
    "2. Learn transformation: $A = (C_U^T C_U + \\lambda I)^{-1} C_U^T C_V$\n",
    "3. **Analyze** $A$ via SVD: $A = U\\Sigma V^T$\n",
    "4. Examine singular values for insight: decay rate, effective rank, conditioning\n",
    "\n",
    "This gives us the \"Direct Matrix\" approach with spectral analysis afterward.\n",
    "\n",
    "**Approach 2: End-to-End SVD Learning (The SVD variant)**\n",
    "\n",
    "Learn the SVD structure directly by parameterizing $\\{u_i\\}, \\{v_i\\}, \\{\\sigma_i\\}$ as neural networks and trainable parameters:\n",
    "\n",
    "1. Initialize basis networks $\\{u_i(y | \\theta_i^u)\\}$ and $\\{v_i(x | \\theta_i^v)\\}$\n",
    "2. Initialize singular values $\\sigma = [\\sigma_1, \\sigma_2, \\ldots, \\sigma_k]$ as learnable parameters\n",
    "3. For input $f$, compute coefficients: $\\alpha = \\arg\\min_\\alpha \\|f - \\sum_j \\alpha_j v_j\\|^2$\n",
    "4. Predict output: $\\hat{T}f = \\sum_i \\sigma_i \\alpha_i u_i$\n",
    "5. Train **end-to-end**: minimize $\\|Tf - \\hat{T}f\\|^2$ via gradient descent on $\\{\\theta^u\\}, \\{\\theta^v\\}, \\{\\sigma\\}$\n",
    "\n",
    "**Trade-off:** End-to-end training learns the SVD that minimizes operator error directly, but doesn't explicitly train bases to span the input/output spaces independently. This can hurt generalization outside the training distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### Eigendecomposition\n",
    "\n",
    "**The geometric picture:** For square matrices, eigendecomposition finds special directions that only stretch—no rotation.\n",
    "\n",
    "$$A = V \\Lambda V^{-1}$$\n",
    "\n",
    "where $\\Lambda$ is diagonal (eigenvalues) and columns of $V$ are eigenvectors.\n",
    "\n",
    "**Key insight:** If $\\mathbf{v}$ is an eigenvector with eigenvalue $\\lambda$, then $A\\mathbf{v} = \\lambda \\mathbf{v}$. The transformation preserves the direction, only changing magnitude.\n",
    "\n",
    "**Example:** For a self-adjoint operator (input and output spaces are the same), eigenvectors reveal the \"natural modes.\"\n",
    "\n",
    "- Each eigenvector $\\mathbf{v}_i$ is an invariant direction\n",
    "- The eigenvalue $\\lambda_i$ is the \"gain\" in that direction\n",
    "- Large $|\\lambda_i|$ → mode amplified; small → suppressed\n",
    "\n",
    "---\n",
    "\n",
    "### Eigendecomposition in B2B: Two Approaches\n",
    "\n",
    "**Approach 1: Via the Gram Matrix (Regularization)**\n",
    "\n",
    "For regularized least squares, we can solve via eigendecomposition of $X^TX$:\n",
    "\n",
    "$$X^T X = V \\Lambda V^T$$\n",
    "\n",
    "Since $X^TX$ is symmetric positive semi-definite:\n",
    "- $V$ is orthogonal\n",
    "- $\\Lambda$ has non-negative eigenvalues\n",
    "\n",
    "**The solution:**\n",
    "$$A = (X^TX + \\alpha I)^{-1} X^T Y = V(\\Lambda + \\alpha I)^{-1} V^T X^T Y$$\n",
    "\n",
    "**Tikhonov regularization:** The term $\\alpha I$ prevents division by tiny eigenvalues, stabilizing the solution.\n",
    "\n",
    "**Truncation:** Keep only the top $k$ eigenvalues for a low-rank approximation.\n",
    "\n",
    "**Approach 2: End-to-End Eigendecomposition Learning (The ED variant)**\n",
    "\n",
    "For self-adjoint operators (same input/output space), learn the eigendecomposition directly:\n",
    "\n",
    "1. Initialize basis networks $\\{v_i(x | \\theta_i)\\}$\n",
    "2. Initialize eigenvalues $\\lambda = [\\lambda_1, \\lambda_2, \\ldots, \\lambda_k]$ as learnable parameters\n",
    "3. For input $f$, compute coefficients: $\\alpha = \\arg\\min_\\alpha \\|f - \\sum_j \\alpha_j v_j\\|^2$\n",
    "4. Predict output: $\\hat{T}f = \\sum_i \\lambda_i \\alpha_i v_i$\n",
    "5. Train **end-to-end**: minimize $\\|Tf - \\hat{T}f\\|^2$ via gradient descent on $\\{\\theta\\}, \\{\\lambda\\}$\n",
    "\n",
    "**Constraint:** Input and output must be on the same domain (self-adjoint operator).\n",
    "\n",
    "**Advantage:** Directly learn physically meaningful eigenmodes and their eigenvalues.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison: Three Approaches\n",
    "\n",
    "| Property | Direct Matrix (B2B) | SVD (End-to-End) | ED (End-to-End) |\n",
    "|----------|---------------------|------------------|-----------------|\n",
    "| **Training** | Two-stage: encoders then matrix | End-to-end | End-to-end |\n",
    "| **Transformation** | $A$ via least squares | $\\{U, \\Sigma, V\\}$ via gradient descent | $\\{V, \\Lambda\\}$ via gradient descent |\n",
    "| **Input/Output spaces** | Can be different | Can be different | Must be the same |\n",
    "| **Basis quality** | Explicitly trained to span spaces | Trained only for operator loss | Trained only for operator loss |\n",
    "| **Generalization** | Best for unseen functions | May struggle outside training | May struggle outside training |\n",
    "| **Interpretability** | SVD analysis post-hoc | Direct access to singular values | Direct access to eigenvalues |\n",
    "| **Best for** | General linear operators | Linear operators needing spectral analysis | Self-adjoint operators |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**Direct Matrix (B2B):** Learns good representations of function spaces, then finds the transformation. Better generalization.\n",
    "\n",
    "**SVD/ED (End-to-End):** Learns the spectral decomposition directly to minimize operator error. More interpretable eigenstructure, but may not capture the full input/output spaces as well.\n",
    "\n",
    "Both approaches give you spectral information, but they optimize different objectives!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Matrix Decompositions: SVD and Eigendecomposition\n",
    "\n",
    "Before we compare transfer methods, let's build intuition for two fundamental matrix decompositions.\n",
    "\n",
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "**The geometric picture:** Every matrix $A$ describes a linear transformation. SVD reveals this transformation in three simple steps:\n",
    "\n",
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "1. **$V^T$**: Rotate to align with the \"input directions\"\n",
    "2. **$\\Sigma$**: Stretch along these directions (diagonal matrix of singular values)\n",
    "3. **$U$**: Rotate to the \"output directions\"\n",
    "\n",
    "**Key insight:** Any linear transformation is fundamentally a rotation, followed by axis-aligned stretching, followed by another rotation.\n",
    "\n",
    "**Example:** Consider our transformation matrix $A: \\mathbb{R}^{10} \\to \\mathbb{R}^{10}$ from source basis to target basis.\n",
    "\n",
    "- The columns of $V$ are the \"best input directions\" (right singular vectors)\n",
    "- The columns of $U$ are the corresponding \"output directions\" (left singular vectors)  \n",
    "- The diagonal entries $\\sigma_i$ in $\\Sigma$ tell us how much each direction gets stretched\n",
    "\n",
    "**Why it matters for operators:**\n",
    "- **Truncation**: Small singular values $\\sigma_i \\approx 0$ mean those directions contribute little. Drop them for a low-rank approximation.\n",
    "- **Regularization**: Discard noisy modes by keeping only the top $k$ singular values.\n",
    "- **Conditioning**: If $\\sigma_{\\text{max}}/\\sigma_{\\text{min}}$ is large, the operator is ill-conditioned (sensitive to noise).\n",
    "\n",
    "**The SVD recipe for operator learning:**\n",
    "$$A \\approx U_k \\Sigma_k V_k^T = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$$\n",
    "\n",
    "This is a sum of rank-1 operators, ordered by importance. Keep the first $k$ terms for a stable approximation.\n",
    "\n",
    "---\n",
    "\n",
    "### Eigendecomposition\n",
    "\n",
    "**The geometric picture:** For square matrices, eigendecomposition finds special directions that don't rotate—they only stretch.\n",
    "\n",
    "$$A = V \\Lambda V^{-1}$$\n",
    "\n",
    "where $\\Lambda$ is diagonal (eigenvalues) and columns of $V$ are eigenvectors.\n",
    "\n",
    "**Key insight:** If $\\mathbf{v}$ is an eigenvector with eigenvalue $\\lambda$, then $A\\mathbf{v} = \\lambda \\mathbf{v}$. The direction doesn't change, only the magnitude.\n",
    "\n",
    "**Example:** Our transformation matrix $A$ maps source coefficients to target coefficients. Eigenvectors reveal the \"natural modes\" of this transformation.\n",
    "\n",
    "- Each eigenvector $\\mathbf{v}_i$ is a direction in coefficient space\n",
    "- The eigenvalue $\\lambda_i$ tells us the \"gain\" in that direction\n",
    "- Large $|\\lambda_i|$ means that mode is amplified; small means suppressed\n",
    "\n",
    "**Why eigendecomposition for operator learning?**\n",
    "\n",
    "Our approach uses eigendecomposition of the **Gram matrix** $X^T X$ (not $A$ directly):\n",
    "\n",
    "$$X^T X = V \\Lambda V^T$$\n",
    "\n",
    "Since $X^T X$ is symmetric positive semi-definite, $V$ is orthogonal and $\\Lambda$ has non-negative eigenvalues.\n",
    "\n",
    "**The connection to least squares:**\n",
    "$$A = (X^T X)^{-1} X^T Y = V \\Lambda^{-1} V^T X^T Y$$\n",
    "\n",
    "Small eigenvalues in $\\Lambda$ cause instability (division by near-zero). The fix: truncate or regularize.\n",
    "\n",
    "**Tikhonov regularization via eigendecomposition:**\n",
    "$$A = V (\\Lambda + \\alpha I)^{-1} V^T X^T Y$$\n",
    "\n",
    "The term $\\alpha I$ prevents division by tiny eigenvalues.\n",
    "\n",
    "---\n",
    "\n",
    "### SVD vs Eigendecomposition: When to Use Each\n",
    "\n",
    "| Property | SVD | Eigendecomposition |\n",
    "|----------|-----|-------------------|\n",
    "| **Works for** | Any matrix (rectangular OK) | Square matrices only |\n",
    "| **Geometric meaning** | Input→Output direction pairs | Invariant directions |\n",
    "| **Decomposition** | $A = U\\Sigma V^T$ | $A = V\\Lambda V^{-1}$ |\n",
    "| **Orthogonality** | $U, V$ always orthogonal | $V$ orthogonal only if $A$ symmetric |\n",
    "| **Stability** | Always numerically stable | Can be unstable for non-symmetric $A$ |\n",
    "| **Our use case** | Direct analysis of $A$ | Regularized least squares via $X^TX$ |\n",
    "\n",
    "**For our B2B framework:**\n",
    "- **SVD of $A$**: Reveals the most important basis transformations\n",
    "- **Eigen of $X^T X$**: Regularizes the least squares solution by controlling small eigenvalues\n",
    "\n",
    "Both give us tools to understand and regularize the operator transformation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Methods Comparison\n",
    "\n",
    "We'll compare four approaches for learning the transformation:\n",
    "\n",
    "1. **Direct Matrix** (Least Squares): $A = (C_U^T C_U)^{-1} C_U^T C_V$\n",
    "2. **SVD-based**: Truncate small singular values for regularization\n",
    "3. **Eigendecomposition**: Use spectral decomposition for symmetric approximation\n",
    "4. **Non-linear MLP**: Learn non-linear transformation between coefficient spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:26.195375Z",
     "iopub.status.busy": "2025-10-14T11:44:26.195287Z",
     "iopub.status.idle": "2025-10-14T11:44:26.201067Z",
     "shell.execute_reply": "2025-10-14T11:44:26.200869Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransferMethodComparison:\n",
    "    \"\"\"Compare different transformation learning methods\"\"\"\n",
    "    \n",
    "    def __init__(self, source_encoder, target_encoder):\n",
    "        self.source_encoder = source_encoder\n",
    "        self.target_encoder = target_encoder\n",
    "        self.methods = {}\n",
    "    \n",
    "    def learn_direct_matrix(self, source_funcs, target_funcs, reg=1e-6):\n",
    "        \"\"\"Method 1: Direct least squares matrix\"\"\"\n",
    "        with torch.no_grad():\n",
    "            C_U = self.source_encoder.encode(source_funcs)\n",
    "            C_V = self.target_encoder.encode(target_funcs)\n",
    "        \n",
    "        X = C_U.cpu()\n",
    "        Y = C_V.cpu()\n",
    "        \n",
    "        # Regularized least squares\n",
    "        XtX = X.T @ X + reg * torch.eye(X.shape[1])\n",
    "        XtY = X.T @ Y\n",
    "        A = torch.linalg.solve(XtX, XtY).T\n",
    "        \n",
    "        self.methods['matrix'] = A.to(device)\n",
    "        return A.to(device)\n",
    "    \n",
    "    def learn_svd_truncated(self, source_funcs, target_funcs, k=None):\n",
    "        \"\"\"Method 2: SVD with truncation\"\"\"\n",
    "        with torch.no_grad():\n",
    "            C_U = self.source_encoder.encode(source_funcs)\n",
    "            C_V = self.target_encoder.encode(target_funcs)\n",
    "        \n",
    "        X = C_U.cpu()\n",
    "        Y = C_V.cpu()\n",
    "        \n",
    "        # SVD of correlation matrix\n",
    "        U_x, S_x, Vt_x = torch.linalg.svd(X, full_matrices=False)\n",
    "        U_y, S_y, Vt_y = torch.linalg.svd(Y, full_matrices=False)\n",
    "        \n",
    "        # Truncate to k components\n",
    "        if k is None:\n",
    "            k = min(X.shape[1], Y.shape[1])\n",
    "        \n",
    "        # Compute transformation via SVD\n",
    "        # A = Y^T X (X^T X)^{-1} ≈ V_y S_y U_y^T U_x S_x^{-1} V_x^T\n",
    "        S_x_inv = torch.zeros_like(S_x)\n",
    "        S_x_inv[:k] = 1.0 / (S_x[:k] + 1e-6)\n",
    "        \n",
    "        A = Vt_y[:k, :].T @ torch.diag(S_y[:k]) @ U_y[:, :k].T @ U_x[:, :k] @ torch.diag(S_x_inv[:k]) @ Vt_x[:k, :]\n",
    "        \n",
    "        self.methods['svd'] = A.to(device)\n",
    "        return A.to(device)\n",
    "    \n",
    "    def learn_eigendecomposition(self, source_funcs, target_funcs, k=None):\n",
    "        \"\"\"Method 3: Eigenvalue-based regularization (Tikhonov)\n",
    "        \n",
    "        Uses eigendecomposition for regularized least squares:\n",
    "        1. Decompose X^T X = V D V^T\n",
    "        2. Truncate small eigenvalues\n",
    "        3. Solve: A = Y^T X V D_k^{-1} V^T\n",
    "        \n",
    "        This is mathematically equivalent to SVD but uses eigendecomposition.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            C_U = self.source_encoder.encode(source_funcs)\n",
    "            C_V = self.target_encoder.encode(target_funcs)\n",
    "        \n",
    "        X = C_U.cpu()\n",
    "        Y = C_V.cpu()\n",
    "        \n",
    "        # Eigendecomposition of Gram matrix X^T X\n",
    "        Gram = X.T @ X\n",
    "        eigvals, eigvecs = torch.linalg.eigh(Gram)\n",
    "        \n",
    "        # Sort descending\n",
    "        idx = torch.argsort(eigvals, descending=True)\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        \n",
    "        if k is None:\n",
    "            k = len(eigvals)\n",
    "        \n",
    "        # Truncate to k components and regularize\n",
    "        eigvals_k = eigvals[:k]\n",
    "        eigvecs_k = eigvecs[:, :k]\n",
    "        \n",
    "        # Compute A = Y^T X V D^{-1} V^T\n",
    "        # This is equivalent to regularized least squares with eigenvalue truncation\n",
    "        D_inv = torch.diag(1.0 / (eigvals_k + 1e-6))\n",
    "        A = Y.T @ X @ eigvecs_k @ D_inv @ eigvecs_k.T\n",
    "        \n",
    "        self.methods['eigen'] = A.to(device)\n",
    "        return A.to(device)\n",
    "    \n",
    "    def learn_mlp(self, source_funcs, target_funcs, hidden_dim=64, n_epochs=500, lr=1e-3):\n",
    "        \"\"\"Method 4: Non-linear MLP transformation\"\"\"\n",
    "        \n",
    "        # Get coefficients\n",
    "        with torch.no_grad():\n",
    "            C_U = self.source_encoder.encode(source_funcs)\n",
    "            C_V = self.target_encoder.encode(target_funcs)\n",
    "        \n",
    "        n_source = C_U.shape[1]\n",
    "        n_target = C_V.shape[1]\n",
    "        \n",
    "        # MLP architecture\n",
    "        mlp = nn.Sequential(\n",
    "            nn.Linear(n_source, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, n_target)\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
    "        \n",
    "        # Train\n",
    "        dataset = TensorDataset(C_U, C_V)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        mlp.train()\n",
    "        for epoch in tqdm(range(n_epochs), desc=\"Training MLP\"):\n",
    "            for batch_u, batch_v in loader:\n",
    "                pred_v = mlp(batch_u)\n",
    "                loss = F.mse_loss(pred_v, batch_v)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        self.methods['mlp'] = mlp\n",
    "        return mlp\n",
    "    \n",
    "    def apply_method(self, method_name, source_func, query_points):\n",
    "        \"\"\"Apply a specific method\"\"\"\n",
    "        self.source_encoder.eval()\n",
    "        self.target_encoder.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            source_coeffs = self.source_encoder.encode(source_func)\n",
    "            \n",
    "            if method_name == 'mlp':\n",
    "                target_coeffs = self.methods[method_name](source_coeffs)\n",
    "            else:\n",
    "                target_coeffs = source_coeffs @ self.methods[method_name].T\n",
    "            \n",
    "            return self.target_encoder.reconstruct(target_coeffs, query_points)\n",
    "\n",
    "\n",
    "print(\"Transfer method comparison framework ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:26.202066Z",
     "iopub.status.busy": "2025-10-14T11:44:26.202003Z",
     "iopub.status.idle": "2025-10-14T11:44:40.630210Z",
     "shell.execute_reply": "2025-10-14T11:44:40.629966Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train all transfer methods on derivative operator\n",
    "print(\"=== COMPARING TRANSFER METHODS ===\\n\")\n",
    "\n",
    "transfer_comp = TransferMethodComparison(deriv_source_encoder, deriv_target_encoder)\n",
    "\n",
    "print(\"Learning transformations...\")\n",
    "print(\"\\n1. Direct Matrix (Least Squares)\")\n",
    "A_matrix = transfer_comp.learn_direct_matrix(train_source_tensor, train_target_tensor)\n",
    "print(f\"   Matrix shape: {A_matrix.shape}\")\n",
    "\n",
    "print(\"\\n2. SVD-based (k=7 components)\")\n",
    "A_svd = transfer_comp.learn_svd_truncated(train_source_tensor, train_target_tensor, k=7)\n",
    "print(f\"   Matrix shape: {A_svd.shape}\")\n",
    "\n",
    "print(\"\\n3. Eigendecomposition (k=7 components)\")\n",
    "A_eigen = transfer_comp.learn_eigendecomposition(train_source_tensor, train_target_tensor, k=7)\n",
    "print(f\"   Matrix shape: {A_eigen.shape}\")\n",
    "\n",
    "print(\"\\n4. Non-linear MLP\")\n",
    "mlp_transform = transfer_comp.learn_mlp(train_source_tensor, train_target_tensor, \n",
    "                                       hidden_dim=32, n_epochs=300)\n",
    "print(f\"   MLP parameters: {sum(p.numel() for p in mlp_transform.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:40.631359Z",
     "iopub.status.busy": "2025-10-14T11:44:40.631277Z",
     "iopub.status.idle": "2025-10-14T11:44:40.959902Z",
     "shell.execute_reply": "2025-10-14T11:44:40.959661Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize transformation matrices and SVD decay\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "methods_to_plot = [('matrix', 'Direct Matrix'), ('svd', 'SVD (k=7)'), ('eigen', 'Eigen (k=7)')]\n",
    "\n",
    "# Top row: Transformation matrices\n",
    "for col, (method, title) in enumerate(methods_to_plot):\n",
    "    ax = axes[0, col]\n",
    "    A = transfer_comp.methods[method].cpu().numpy()\n",
    "    im = ax.imshow(A, cmap='RdBu_r', aspect='auto', vmin=-2, vmax=2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Source Basis')\n",
    "    ax.set_ylabel('Target Basis')\n",
    "    plt.colorbar(im, ax=ax, label='Weight')\n",
    "\n",
    "# Bottom row: SVD decay (line plots)\n",
    "ax = axes[1, 0]\n",
    "for method, label, color in [('matrix', 'Direct', 'blue'), ('svd', 'SVD', 'orange'), ('eigen', 'Eigen', 'green')]:\n",
    "    A = transfer_comp.methods[method].cpu()\n",
    "    _, S, _ = torch.linalg.svd(A)\n",
    "    ax.plot(range(1, len(S)+1), S.numpy(), 'o-', label=label, linewidth=2, markersize=6, color=color)\n",
    "\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Singular Value')\n",
    "ax.set_title('Singular Value Decay Comparison')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Condition numbers\n",
    "ax = axes[1, 1]\n",
    "cond_nums = []\n",
    "method_labels = []\n",
    "for method, label in [('matrix', 'Direct'), ('svd', 'SVD'), ('eigen', 'Eigen')]:\n",
    "    A = transfer_comp.methods[method].cpu()\n",
    "    cond = torch.linalg.cond(A).item()\n",
    "    cond_nums.append(cond)\n",
    "    method_labels.append(label)\n",
    "\n",
    "ax.bar(method_labels, cond_nums, alpha=0.7, color=['blue', 'orange', 'green'])\n",
    "ax.set_ylabel('Condition Number')\n",
    "ax.set_title('Matrix Conditioning')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Rank comparison\n",
    "ax = axes[1, 2]\n",
    "ranks = []\n",
    "for method, label in [('matrix', 'Direct'), ('svd', 'SVD'), ('eigen', 'Eigen')]:\n",
    "    A = transfer_comp.methods[method].cpu()\n",
    "    rank = torch.linalg.matrix_rank(A).item()\n",
    "    ranks.append(rank)\n",
    "\n",
    "ax.bar(method_labels, ranks, alpha=0.7, color=['blue', 'orange', 'green'])\n",
    "ax.set_ylabel('Matrix Rank')\n",
    "ax.set_title('Effective Rank')\n",
    "ax.set_ylim([0, max(ranks) + 2])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Transfer Methods: Matrices and Spectral Analysis', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:40.961195Z",
     "iopub.status.busy": "2025-10-14T11:44:40.961091Z",
     "iopub.status.idle": "2025-10-14T11:44:41.488516Z",
     "shell.execute_reply": "2025-10-14T11:44:41.488185Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test all methods\n",
    "methods = ['matrix', 'svd', 'eigen', 'mlp']\n",
    "method_names = ['Direct Matrix', 'SVD (k=7)', 'Eigendecomp (k=7)', 'Non-linear MLP']\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "\n",
    "test_indices = [5, 25, 45, 65]\n",
    "method_errors = {m: [] for m in methods}\n",
    "\n",
    "for row, method in enumerate(methods):\n",
    "    for col, test_idx in enumerate(test_indices):\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Apply method\n",
    "        source_func = torch.tensor(test_functions[test_idx:test_idx+1], dtype=torch.float32).to(device)\n",
    "        pred_deriv = transfer_comp.apply_method(method, source_func, x_tensor).squeeze().cpu().numpy()\n",
    "        true_deriv = test_derivatives[test_idx]\n",
    "        \n",
    "        # Compute error\n",
    "        mse = np.mean((pred_deriv - true_deriv)**2)\n",
    "        method_errors[method].append(mse)\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot(x, test_functions[test_idx], 'b-', linewidth=2, alpha=0.6, label='f(x)')\n",
    "        ax.plot(x, true_deriv, 'r-', linewidth=2, label=\"True f'\")\n",
    "        ax.plot(x, pred_deriv, 'g--', linewidth=2, label=method_names[row])\n",
    "        \n",
    "        if row == 0:\n",
    "            ax.set_title(f'Test {col+1}', fontsize=12)\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(method_names[row], fontsize=11)\n",
    "        \n",
    "        ax.text(0.95, 0.05, f'MSE={mse:.6f}', transform=ax.transAxes, \n",
    "               ha='right', va='bottom', fontsize=9, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        if row == 0 and col == 0:\n",
    "            ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlabel('x')\n",
    "\n",
    "plt.suptitle('Transfer Methods Comparison: Derivative Operator', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n=== PERFORMANCE SUMMARY ===\")\n",
    "print(f\"{'Method':<20} {'Mean MSE':>12} {'Std MSE':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for method, name in zip(methods, method_names):\n",
    "    errors = method_errors[method]\n",
    "    print(f\"{name:<20} {np.mean(errors):>12.6f} {np.std(errors):>12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Poisson Equation Solver\n",
    "\n",
    "Learn the solution operator for the Poisson equation:\n",
    "$$-\\nabla^2 u = f \\text{ in } \\Omega, \\quad u = 0 \\text{ on } \\partial\\Omega$$\n",
    "\n",
    "In 1D: $-\\frac{d^2u}{dx^2} = f(x)$ with $u(0) = u(1) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:41.491181Z",
     "iopub.status.busy": "2025-10-14T11:44:41.491105Z",
     "iopub.status.idle": "2025-10-14T11:44:41.604857Z",
     "shell.execute_reply": "2025-10-14T11:44:41.604594Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_poisson_data(n_samples=1000, n_points=100):\n",
    "    \"\"\"Generate Poisson equation data\"\"\"\n",
    "    \n",
    "    x = np.linspace(0, 1, n_points)\n",
    "    dx = x[1] - x[0]\n",
    "    \n",
    "    # Create finite difference matrix for -d²/dx²\n",
    "    main_diag = 2 * np.ones(n_points - 2) / dx**2\n",
    "    off_diag = -np.ones(n_points - 3) / dx**2\n",
    "    A_fd = diags([off_diag, main_diag, off_diag], [-1, 0, 1]).toarray()\n",
    "    A_inv = np.linalg.inv(A_fd)\n",
    "    \n",
    "    sources = []\n",
    "    solutions = []\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Generate random source function (combination of sines)\n",
    "        f = np.zeros(n_points)\n",
    "        n_modes = np.random.randint(2, 6)\n",
    "        for k in range(n_modes):\n",
    "            mode = np.random.randint(1, 10)\n",
    "            amplitude = np.random.randn()\n",
    "            phase = np.random.rand() * 2 * np.pi\n",
    "            f += amplitude * np.sin(mode * np.pi * x + phase)\n",
    "        \n",
    "        # Solve Poisson equation\n",
    "        u = np.zeros(n_points)\n",
    "        u[1:-1] = A_inv @ f[1:-1]\n",
    "        \n",
    "        sources.append(f)\n",
    "        solutions.append(u)\n",
    "    \n",
    "    return np.array(sources), np.array(solutions), x\n",
    "\n",
    "\n",
    "print(\"\\n=== POISSON EQUATION EXAMPLE ===\")\n",
    "poisson_sources, poisson_solutions, x_poisson = generate_poisson_data(n_samples=1500)\n",
    "\n",
    "# Split data\n",
    "n_train_poisson = 1200\n",
    "train_sources_poisson = poisson_sources[:n_train_poisson]\n",
    "train_solutions_poisson = poisson_solutions[:n_train_poisson]\n",
    "test_sources_poisson = poisson_sources[n_train_poisson:]\n",
    "test_solutions_poisson = poisson_solutions[n_train_poisson:]\n",
    "\n",
    "print(f\"Data: {n_train_poisson} training, {len(test_sources_poisson)} test\")\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "for i in range(3):\n",
    "    ax = axes[i]\n",
    "    ax.plot(x_poisson, poisson_sources[i], 'r-', linewidth=2, label='f(x)')\n",
    "    ax.plot(x_poisson, poisson_solutions[i], 'b-', linewidth=2, label='u(x)')\n",
    "    ax.set_title(f'Sample {i+1}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "plt.suptitle('Poisson Equation: Source → Solution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Encoders for Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:41.606127Z",
     "iopub.status.busy": "2025-10-14T11:44:41.606029Z",
     "iopub.status.idle": "2025-10-14T11:44:43.510796Z",
     "shell.execute_reply": "2025-10-14T11:44:43.510564Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nTraining Poisson encoders...\")\n",
    "\n",
    "# Create encoders with more basis functions for this problem\n",
    "poisson_source_encoder = FunctionEncoder(sensor_dim=100, n_basis=15, hidden_dim=64).to(device)\n",
    "poisson_solution_encoder = FunctionEncoder(sensor_dim=100, n_basis=15, hidden_dim=64).to(device)\n",
    "\n",
    "# Train encoders\n",
    "print(\"1. Source encoder (f space):\")\n",
    "poisson_source_losses = train_encoder(poisson_source_encoder, train_sources_poisson, x_poisson,\n",
    "                                     n_epochs=400, name=\"Source Encoder\")\n",
    "\n",
    "print(\"\\n2. Solution encoder (u space):\")\n",
    "poisson_solution_losses = train_encoder(poisson_solution_encoder, train_solutions_poisson, x_poisson,\n",
    "                                       n_epochs=400, name=\"Solution Encoder\")\n",
    "\n",
    "print(f\"\\nFinal losses - Source: {poisson_source_losses[-1]:.6f}, Solution: {poisson_solution_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn and Apply Poisson Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:43.512053Z",
     "iopub.status.busy": "2025-10-14T11:44:43.511973Z",
     "iopub.status.idle": "2025-10-14T11:44:43.710970Z",
     "shell.execute_reply": "2025-10-14T11:44:43.710583Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create B2B operator for Poisson\n",
    "poisson_b2b = B2BOperator(poisson_source_encoder, poisson_solution_encoder)\n",
    "\n",
    "# Learn transformation\n",
    "print(\"Learning Poisson transformation matrix...\")\n",
    "train_source_poisson_tensor = torch.tensor(train_sources_poisson, dtype=torch.float32).to(device)\n",
    "train_solution_poisson_tensor = torch.tensor(train_solutions_poisson, dtype=torch.float32).to(device)\n",
    "\n",
    "A_poisson, fit_error_poisson = poisson_b2b.learn_transformation(\n",
    "    train_source_poisson_tensor, train_solution_poisson_tensor\n",
    ")\n",
    "\n",
    "print(f\"Transformation matrix shape: {A_poisson.shape}\")\n",
    "print(f\"Fitting error: {fit_error_poisson:.6f}\")\n",
    "\n",
    "# Visualize transformation matrix and SVD\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Matrix heatmap\n",
    "ax = axes[0]\n",
    "im = ax.imshow(A_poisson.cpu().numpy(), cmap='RdBu_r', aspect='auto')\n",
    "plt.colorbar(im, ax=ax, label='Weight')\n",
    "ax.set_title('Poisson Solver Transformation Matrix')\n",
    "ax.set_xlabel('Source Basis')\n",
    "ax.set_ylabel('Solution Basis')\n",
    "\n",
    "# Singular values (line plot)\n",
    "ax = axes[1]\n",
    "U, S, Vt = torch.linalg.svd(A_poisson.cpu())\n",
    "ax.plot(range(1, len(S)+1), S.numpy(), 'o-', linewidth=2, markersize=6, color='orange')\n",
    "ax.set_title('Singular Value Decay')\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Singular Value')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative energy\n",
    "ax = axes[2]\n",
    "cumsum = torch.cumsum(S**2, dim=0) / torch.sum(S**2)\n",
    "ax.plot(range(1, len(S)+1), cumsum.numpy(), 'o-', linewidth=2, markersize=6, color='green')\n",
    "ax.axhline(0.99, color='r', linestyle='--', label='99% energy')\n",
    "ax.set_title('Cumulative Energy')\n",
    "ax.set_xlabel('Number of Singular Values')\n",
    "ax.set_ylabel('Fraction of Total Energy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMatrix rank: {torch.linalg.matrix_rank(A_poisson).item()}\")\n",
    "print(f\"Condition number: {torch.linalg.cond(A_poisson).item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Poisson Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:43.712386Z",
     "iopub.status.busy": "2025-10-14T11:44:43.712286Z",
     "iopub.status.idle": "2025-10-14T11:44:43.912622Z",
     "shell.execute_reply": "2025-10-14T11:44:43.912378Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test Poisson solver\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "poisson_test_errors = []\n",
    "x_poisson_tensor = torch.tensor(x_poisson, dtype=torch.float32).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "\n",
    "for i in range(6):\n",
    "    test_idx = i * 10\n",
    "    \n",
    "    # Apply B2B operator\n",
    "    source_func = torch.tensor(test_sources_poisson[test_idx:test_idx+1], dtype=torch.float32).to(device)\n",
    "    pred_solution = poisson_b2b.apply(source_func, x_poisson_tensor).squeeze().cpu().numpy()\n",
    "    \n",
    "    true_solution = test_solutions_poisson[test_idx]\n",
    "    \n",
    "    # Compute error\n",
    "    mse = np.mean((pred_solution - true_solution)**2)\n",
    "    rel_error = np.sqrt(mse) / np.sqrt(np.mean(true_solution**2) + 1e-8)\n",
    "    poisson_test_errors.append(rel_error)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i]\n",
    "    ax.plot(x_poisson, test_sources_poisson[test_idx], 'r-', linewidth=2, alpha=0.7, label='f(x)')\n",
    "    ax.plot(x_poisson, true_solution, 'b-', linewidth=2, label='True u(x)')\n",
    "    ax.plot(x_poisson, pred_solution, 'g--', linewidth=2, label='B2B u(x)')\n",
    "    \n",
    "    ax.set_title(f'Test {i+1}: Rel. Error = {rel_error:.4f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "plt.suptitle('B2B Poisson Solver Results', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average relative error: {np.mean(poisson_test_errors):.4f} ± {np.std(poisson_test_errors):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: 1D Nonlinear Darcy Flow\n",
    "\n",
    "Same as DeepONet: Solve the nonlinear Darcy equation with solution-dependent permeability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:43.914054Z",
     "iopub.status.busy": "2025-10-14T11:44:43.913955Z",
     "iopub.status.idle": "2025-10-14T11:44:51.605834Z",
     "shell.execute_reply": "2025-10-14T11:44:51.605607Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_darcy_data(n_funcs=1000, n_points=40):\n",
    "    \"\"\"Generate 1D nonlinear Darcy flow data\"\"\"\n",
    "    \n",
    "    def permeability(s):\n",
    "        return 0.2 + s**2\n",
    "    \n",
    "    # Gaussian process for source function\n",
    "    x = np.linspace(0, 1, n_points)\n",
    "    l, sigma = 0.04, 1.0\n",
    "    K = sigma**2 * np.exp(-0.5 * (x[:, None] - x[None, :])**2 / l**2)\n",
    "    K += 1e-6 * np.eye(n_points)\n",
    "    \n",
    "    def solve_darcy(u_func):\n",
    "        dx = x[1] - x[0]\n",
    "        s = np.zeros(n_points)\n",
    "        \n",
    "        for _ in range(100):  # Fixed point iteration\n",
    "            kappa = permeability(s)\n",
    "            main_diag = (kappa[1:] + kappa[:-1]) / dx**2\n",
    "            upper_diag = -kappa[1:-1] / dx**2\n",
    "            lower_diag = -kappa[1:-1] / dx**2\n",
    "            \n",
    "            A = diags([lower_diag, main_diag, upper_diag], [-1, 0, 1], \n",
    "                     shape=(n_points-2, n_points-2))\n",
    "            \n",
    "            s_interior = spsolve(A, u_func[1:-1])\n",
    "            s_new = np.zeros(n_points)\n",
    "            s_new[1:-1] = s_interior\n",
    "            s = 0.5 * s_new + 0.5 * s\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    # Generate dataset\n",
    "    np.random.seed(42)\n",
    "    U, S = [], []\n",
    "    \n",
    "    print(\"Generating Darcy dataset...\")\n",
    "    for i in tqdm(range(n_funcs), desc=\"Solving PDEs\"):\n",
    "        u = multivariate_normal.rvs(mean=np.zeros(n_points), cov=K)\n",
    "        s = solve_darcy(u)\n",
    "        U.append(u)\n",
    "        S.append(s)\n",
    "    \n",
    "    return np.array(U), np.array(S), x\n",
    "\n",
    "\n",
    "print(\"\\n=== 1D NONLINEAR DARCY EXAMPLE ===\")\n",
    "darcy_sources, darcy_solutions, x_darcy = generate_darcy_data(n_funcs=1000)\n",
    "\n",
    "# Split data\n",
    "n_train_darcy = 800\n",
    "train_sources_darcy = darcy_sources[:n_train_darcy]\n",
    "train_solutions_darcy = darcy_solutions[:n_train_darcy]\n",
    "test_sources_darcy = darcy_sources[n_train_darcy:]\n",
    "test_solutions_darcy = darcy_solutions[n_train_darcy:]\n",
    "\n",
    "print(f\"\\nData: {n_train_darcy} training, {len(test_sources_darcy)} test\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "for i in range(3):\n",
    "    ax = axes[i]\n",
    "    ax.plot(x_darcy, darcy_sources[i], 'g-', linewidth=2, label='f(x)')\n",
    "    ax.plot(x_darcy, darcy_solutions[i], 'b-', linewidth=2, label='u(x)')\n",
    "    ax.set_title(f'Sample {i+1}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "plt.suptitle('1D Nonlinear Darcy Flow')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Encoders for Darcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:51.607082Z",
     "iopub.status.busy": "2025-10-14T11:44:51.607006Z",
     "iopub.status.idle": "2025-10-14T11:44:53.955484Z",
     "shell.execute_reply": "2025-10-14T11:44:53.955256Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nTraining Darcy encoders...\")\n",
    "\n",
    "# Create encoders\n",
    "darcy_source_encoder = FunctionEncoder(sensor_dim=40, n_basis=20, hidden_dim=128).to(device)\n",
    "darcy_solution_encoder = FunctionEncoder(sensor_dim=40, n_basis=20, hidden_dim=128).to(device)\n",
    "\n",
    "# Train\n",
    "print(\"1. Source encoder:\")\n",
    "darcy_source_losses = train_encoder(darcy_source_encoder, train_sources_darcy, x_darcy,\n",
    "                                   n_epochs=500, lr=0.001, name=\"Darcy Source\")\n",
    "\n",
    "print(\"\\n2. Solution encoder:\")\n",
    "darcy_solution_losses = train_encoder(darcy_solution_encoder, train_solutions_darcy, x_darcy,\n",
    "                                     n_epochs=500, lr=0.001, name=\"Darcy Solution\")\n",
    "\n",
    "print(f\"\\nFinal losses - Source: {darcy_source_losses[-1]:.6f}, Solution: {darcy_solution_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn and Apply Darcy Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:53.956731Z",
     "iopub.status.busy": "2025-10-14T11:44:53.956656Z",
     "iopub.status.idle": "2025-10-14T11:44:54.146872Z",
     "shell.execute_reply": "2025-10-14T11:44:54.146660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create B2B operator for Darcy\n",
    "darcy_b2b = B2BOperator(darcy_source_encoder, darcy_solution_encoder)\n",
    "\n",
    "# Learn transformation\n",
    "print(\"Learning Darcy transformation matrix...\")\n",
    "train_source_darcy_tensor = torch.tensor(train_sources_darcy, dtype=torch.float32).to(device)\n",
    "train_solution_darcy_tensor = torch.tensor(train_solutions_darcy, dtype=torch.float32).to(device)\n",
    "\n",
    "A_darcy, fit_error_darcy = darcy_b2b.learn_transformation(\n",
    "    train_source_darcy_tensor, train_solution_darcy_tensor\n",
    ")\n",
    "\n",
    "print(f\"Transformation matrix shape: {A_darcy.shape}\")\n",
    "print(f\"Fitting error: {fit_error_darcy:.6f}\")\n",
    "\n",
    "# Visualize transformation matrix and SVD\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Matrix heatmap\n",
    "ax = axes[0]\n",
    "im = ax.imshow(A_darcy.cpu().numpy(), cmap='RdBu_r', aspect='auto')\n",
    "plt.colorbar(im, ax=ax, label='Weight')\n",
    "ax.set_title('Darcy Operator Transformation Matrix')\n",
    "ax.set_xlabel('Source Basis')\n",
    "ax.set_ylabel('Solution Basis')\n",
    "\n",
    "# Singular values (line plot)\n",
    "ax = axes[1]\n",
    "U, S, Vt = torch.linalg.svd(A_darcy.cpu())\n",
    "ax.plot(range(1, len(S)+1), S.numpy(), 'o-', linewidth=2, markersize=6, color='purple')\n",
    "ax.set_title('Singular Value Decay')\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Singular Value')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative energy\n",
    "ax = axes[2]\n",
    "cumsum = torch.cumsum(S**2, dim=0) / torch.sum(S**2)\n",
    "ax.plot(range(1, len(S)+1), cumsum.numpy(), 'o-', linewidth=2, markersize=6, color='green')\n",
    "ax.axhline(0.99, color='r', linestyle='--', label='99% energy')\n",
    "ax.set_title('Cumulative Energy')\n",
    "ax.set_xlabel('Number of Singular Values')\n",
    "ax.set_ylabel('Fraction of Total Energy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMatrix rank: {torch.linalg.matrix_rank(A_darcy).item()}\")\n",
    "print(f\"Condition number: {torch.linalg.cond(A_darcy).item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Darcy Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T11:44:54.148178Z",
     "iopub.status.busy": "2025-10-14T11:44:54.148074Z",
     "iopub.status.idle": "2025-10-14T11:44:54.432112Z",
     "shell.execute_reply": "2025-10-14T11:44:54.431869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test Darcy operator\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "darcy_test_errors = []\n",
    "x_darcy_tensor = torch.tensor(x_darcy, dtype=torch.float32).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "\n",
    "for i in range(6):\n",
    "    test_idx = i * 5\n",
    "    \n",
    "    # Apply B2B\n",
    "    source_func = torch.tensor(test_sources_darcy[test_idx:test_idx+1], dtype=torch.float32).to(device)\n",
    "    pred_solution = darcy_b2b.apply(source_func, x_darcy_tensor).squeeze().cpu().numpy()\n",
    "    \n",
    "    true_solution = test_solutions_darcy[test_idx]\n",
    "    \n",
    "    # Error\n",
    "    mse = np.mean((pred_solution - true_solution)**2)\n",
    "    rel_error = np.sqrt(mse) / np.sqrt(np.mean(true_solution**2) + 1e-8)\n",
    "    darcy_test_errors.append(rel_error)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i]\n",
    "    ax.plot(x_darcy, test_sources_darcy[test_idx], 'g-', linewidth=2, alpha=0.7, label='f(x)')\n",
    "    ax.plot(x_darcy, true_solution, 'b-', linewidth=2, label='True u(x)')\n",
    "    ax.plot(x_darcy, pred_solution, 'r--', linewidth=2, label='B2B u(x)')\n",
    "    \n",
    "    ax.set_title(f'Test {i+1}: Rel. Error = {rel_error:.4f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "plt.suptitle('B2B Darcy Operator Results', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average relative error: {np.mean(darcy_test_errors):.4f} ± {np.std(darcy_test_errors):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
