%**************************************************************************************
% License:
% CC BY-NC-SA 4.0 (http://creativecommons.org/licenses/by-nc-sa/4.0/)
%**************************************************************************************

\documentclass[notes]{beamer}

\mode<presentation> {

\usetheme{Madrid}

% Burnt orange
\definecolor{burntorange}{rgb}{0.8, 0.33, 0.0}
\colorlet{beamer@blendedblue}{burntorange}
% Pale yellow
\definecolor{paleyellow}{rgb}{1.0, 1.0, 0.953}
\setbeamercolor{background canvas}{bg=paleyellow}
% Secondary and tertiary palette
\setbeamercolor*{palette secondary}{use=structure,fg=white,bg=burntorange!80!black}
\setbeamercolor*{palette tertiary}{use=structure,fg=white,bg=burntorange!60!black}

% To remove the navigation symbols from the bottom of all slides uncomment this line
%\setbeamertemplate{navigation symbols}{}
}

\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{bm}
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{breqn}
\usepackage{cleveref}
\usepackage{graphicx} % for figures
\usepackage[labelsep=space,tableposition=top]{caption}
\renewcommand{\figurename}{Fig.} 
\usepackage{caption,subcaption}% http://ctan.org/pkg/{caption,subcaption}
\usepackage{xcolor}
\usepackage{hyperref}

\AtBeginSection[]{
	\begin{frame}{Outline}
		\tableofcontents[
		currentsection,      % highlight the current section
		hideallsubsections   % show only section titles
		]
	\end{frame}
}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------
\title[DeepONet: Operator Learning]{DeepONet: Learning Operators with Neural Networks} 
\author{Krishna Kumar} % name
\institute[UT Austin] % institution 
{
University of Texas at Austin \\
\medskip
\textit{
  \url{krishnak@utexas.edu}} % Your email address
}
\date{} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % title page as the first slide
\end{frame}

\begin{frame}
 \frametitle{Overview}
 \tableofcontents
\end{frame}

%----------------------------------------------------------------------------------------
% slides
%----------------------------------------------------------------------------------------

\section{From Functions to Operators: The Conceptual Leap}

%------------------------------------------------
\begin{frame}
\frametitle{Learning Objectives}

\begin{itemize}
    \item Understand the leap from function approximation to operator learning
    \item Master the Universal Approximation Theorem for Operators
    \item Learn the DeepONet architecture: Branch and Trunk networks
    \item Implement operator learning for the derivative operator
    \item Apply DeepONet to the 1D nonlinear Darcy problem
    \item Extend to Physics-Informed DeepONet with PDE constraints
\end{itemize}

\vspace{1cm}
\centering
\href{https://colab.research.google.com/github/kks32-courses/sciml-sciml/blob/main/docs/02-deeponet/deeponet.ipynb}{\beamergotobutton{Open Notebook: DeepONet}}

\vspace{0.5cm}
\href{https://colab.research.google.com/github/kks32-courses/sciml-sciml/blob/main/docs/02-deeponet/pideeponet.ipynb}{\beamergotobutton{Open Notebook: PI-DeepONet}}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Fundamental Question}

We've learned how neural networks can approximate \textbf{functions}: $f: \mathbb{R}^d \rightarrow \mathbb{R}^m$.

\vspace{1cm}

\begin{alertblock}{But what if we want to learn mappings between infinite-dimensional function spaces?}
Enter \textbf{operators}: mappings that take functions as input and produce functions as output.
\begin{equation*}
\mathcal{G}: \mathcal{A} \rightarrow \mathcal{U}
\end{equation*}
where $\mathcal{A}$ and $\mathcal{U}$ are function spaces.
\end{alertblock}

\textbf{Examples of operators:}
\begin{itemize}
    \item \textbf{Derivative operator:} $\mathcal{G}u = \frac{du}{dx}$
    \item \textbf{Integration operator:} $\mathcal{G}f = \int_0^x f(t) dt$
    \item \textbf{PDE solution operator:} Given boundary conditions or source terms, map to the PDE solution
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Traditional Function Approximation}

\textbf{What we're familiar with:}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Point-wise mapping:}
        \begin{itemize}
            \item \textbf{Input:} A number $x = 2.5$
            \item \textbf{Output:} A number $f(x) = x^2 = 6.25$
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Characteristics:}
        \begin{itemize}
            \item Input: Single numbers (scalars)
            \item Output: Single numbers (scalars)
            \item Learn: Point-wise mappings
            \item Architecture: Standard feedforward
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\linewidth]{figs/function_approximation.png}
            \caption*{Traditional neural networks learn scalar-to-scalar mappings.}
        \end{figure}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Operator Learning: The Next Level}

\textbf{The paradigm shift:}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Function-to-function mapping:}
        \begin{itemize}
            \item \textbf{Input:} An entire function $u(x) = \sin(x)$
            \item \textbf{Output:} Another entire function $\mathcal{G}[u](x) = \cos(x)$ (the derivative!)
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Examples in science:}
        \begin{itemize}
            \item $\mathcal{D}[u] = \frac{du}{dx}$ (Differentiation)
            \item $\mathcal{I}[f] = \int_0^x f(t) dt$ (Integration)
            \item $\mathcal{S}[f] = u$ where $\nabla^2 u = f$ (PDE solution)
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\linewidth]{figs/operator_concept.png}
            \caption*{Operators map entire functions to entire functions.}
        \end{figure}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Challenge with Traditional Neural Networks}

Standard neural networks learn point-wise mappings: $\mathbb{R}^n \rightarrow \mathbb{R}^m$. But operators map functions to functions.

\begin{alertblock}{Traditional approach limitations:}
\begin{itemize}
    \item \textbf{Fixed discretization:} Networks trained on specific grids can't generalize to different resolutions
    \item \textbf{Curse of dimensionality:} High-dimensional function spaces are computationally intractable
    \item \textbf{No theoretical foundation:} No guarantee that standard networks can approximate operators
\end{itemize}
\end{alertblock}

\textbf{How do we represent infinite-dimensional functions with finite data?}

\vspace{0.5cm}

\textbf{Solution:} We need a fundamentally different architecture that can handle function inputs and outputs while maintaining theoretical guarantees.

\end{frame}

%------------------------------------------------
\section{Universal Approximation Theorem for Operators}

%------------------------------------------------
\begin{frame}
\frametitle{The Breakthrough Theorem}

Just as the Universal Approximation Theorem tells us neural networks can approximate functions, there's a remarkable extension:

\begin{block}{Theorem (Chen \& Chen, 1995)}
Neural networks can approximate \textbf{operators} that map functions to functions!
\end{block}

\textbf{Mathematical Statement:} For any continuous operator $\mathcal{G}: V \subset C(K_1) \rightarrow C(K_2)$ and $\epsilon > 0$, there exist constants such that:

\begin{equation*}
\left|\mathcal{G}(u)(y) - \sum_{k=1}^p \underbrace{\sum_{i=1}^n c_i^k \sigma\left(\sum_{j=1}^m \xi_{ij}^k u(x_j) + \theta_i^k\right)}_{\text{Branch Network}} \underbrace{\sigma(w_k \cdot y + \zeta_k)}_{\text{Trunk Network}}\right| < \epsilon
\end{equation*}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Decoding the Theorem}

This looks complex, but the insight is beautiful:

\begin{enumerate}
    \item \textbf{Branch Network:} Processes function $u$ sampled at sensor points $\{x_j\}$
    \item \textbf{Trunk Network:} Processes output coordinates $y$
    \item \textbf{Combination:} Multiply branch and trunk outputs, then sum
\end{enumerate}

\begin{block}{Key insight}
Any operator can be written as:
\begin{equation*}
\mathcal{G}(u)(y) \approx \sum_{k=1}^p b_k(u) \cdot t_k(y)
\end{equation*}
where $b_k$ depends only on the input function and $t_k$ depends only on the output location!
\end{block}

This decomposition is the theoretical foundation for the DeepONet architecture.

\end{frame}

%------------------------------------------------
\section{DeepONet Architecture}

%------------------------------------------------
\begin{frame}
\frametitle{DeepONet: The Practical Implementation}

\textbf{DeepONet} (Deep Operator Network) is the practical implementation of the Operator Universal Approximation Theorem.

\begin{block}{Core Architecture}
\begin{equation*}
\mathcal{G}_\theta(u)(y) = \sum_{k=1}^p b_k(u) \cdot t_k(y) + b_0
\end{equation*}
where:
\begin{itemize}
    \item \textbf{Branch network:} $b_k(u) = \mathcal{B}_k([u(x_1), u(x_2), \ldots, u(x_m)])$
    \item \textbf{Trunk network:} $t_k(y) = \mathcal{T}_k(y)$
    \item \textbf{$p$:} Number of basis functions (typically 50-200)
    \item \textbf{$b_0$:} Bias term
\end{itemize}
\end{block}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{DeepONet Architecture Diagram}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/deeponet-arch.png}
	\caption*{DeepONet architecture showing branch and trunk networks.}
\end{figure}

\textbf{Data flow:}
\begin{enumerate}
    \item Input function $u$ sampled at sensor points → Branch network → Coefficients $\{b_k\}$
    \item Query points $y$ → Trunk network → Basis functions $\{t_k\}$  
    \item Element-wise multiplication and summation → Output $\mathcal{G}(u)(y)$
\end{enumerate}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Training Data Structure and Loss Function}

\textbf{Input-output pairs:} $(u^{(i)}, y^{(j)}, \mathcal{G}(u^{(i)})(y^{(j)}))$

\begin{itemize}
    \item \textbf{$N$} input functions: $\{u^{(i)}\}_{i=1}^N$
    \item Each function sampled at \textbf{$m$} sensors: $\{u^{(i)}(x_j)\}_{j=1}^m$
    \item Corresponding outputs at \textbf{query points}: $\{\mathcal{G}(u^{(i)})(y_k)\}$
\end{itemize}

\begin{block}{Loss Function}
\begin{equation*}
\mathcal{L}(\theta) = \frac{1}{N \cdot P} \sum_{i=1}^N \sum_{k=1}^P \left|\mathcal{G}_\theta(u^{(i)})(y_k) - \mathcal{G}(u^{(i)})(y_k)\right|^2
\end{equation*}
\end{block}

\textbf{Key Advantages:}
\begin{itemize}
    \item \textbf{Resolution independence:} Train on one grid, evaluate on any grid
    \item \textbf{Fast evaluation:} Once trained, instant prediction (no iterative solving)
    \item \textbf{Generalization:} Works for new functions not seen during training
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Example 1: The Derivative Operator}

%------------------------------------------------
\begin{frame}
\frametitle{Perfect Starting Point: Learning Derivatives}

\textbf{Problem Setup:} Learn the derivative operator $\mathcal{D}[u] = \frac{du}{dx}$

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Why this example:}
        \begin{itemize}
            \item Simple and intuitive
            \item Exact analytical solution for verification
            \item Shows how DeepONet learns basis decompositions
            \item Bridges function approximation → operator learning
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Input functions:} Cubic polynomials
        \begin{equation*}
        u(x) = ax^3 + bx^2 + cx + d
        \end{equation*}
        
        \textbf{Target operator:}
        \begin{equation*}
        \mathcal{D}[u](x) = 3ax^2 + 2bx + c
        \end{equation*}
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\linewidth]{figs/polynomial_samples.png}
            \caption*{Sample cubic polynomials and their derivatives.}
        \end{figure}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The DeepONet Challenge}

\textbf{Key insight:} The derivative of a cubic is always quadratic, so it can be written as:
\begin{equation*}
\frac{du}{dx} = w_1 \cdot 1 + w_2 \cdot x + w_3 \cdot x^2
\end{equation*}
where $w_1 = c$, $w_2 = 2b$, $w_3 = 3a$.

\vspace{1cm}

\begin{alertblock}{The DeepONet challenge}
Can it learn this mapping automatically without being told the explicit form?
\end{alertblock}

\vspace{0.5cm}

\textbf{What we expect:} 
\begin{itemize}
    \item Branch network should learn to extract coefficients $\{a, b, c\}$
    \item Trunk network should learn basis functions $\{1, x, x^2\}$
    \item The combination should reproduce the derivative exactly
\end{itemize}

\end{frame}


%------------------------------------------------
\begin{frame}
\frametitle{Prediction Results}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{figs/prediction_results.png}
	\caption*{DeepONet predictions vs. true derivatives for test polynomials.}
\end{figure}

\textbf{Results:}
\begin{itemize}
    \item Perfect agreement between predicted and true derivatives
    \item Works for polynomial coefficients not seen during training
    \item Demonstrates true operator learning, not just pattern memorization
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Understanding What DeepONet Learned}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{figs/basis_analysis.png}
	\caption*{Analysis of learned basis functions and branch network behavior.}
\end{figure}

\textbf{Mathematical Decomposition:}
\begin{equation*}
f'(x) \approx \sum_{i=1}^3 w_i(a,b,c,d) \times \phi_i(x) + \text{bias}
\end{equation*}

\textbf{Key findings:}
\begin{itemize}
    \item Basis functions $\phi_i(x)$ approximate $\{1, x, x^2\}$
    \item Branch network learns to map coefficients to appropriate weights
    \item The decomposition matches the analytical structure perfectly
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Example 2: The 1D Nonlinear Darcy Problem}

%------------------------------------------------
\begin{frame}
\frametitle{A Real PDE: The 1D Nonlinear Darcy Problem}

\textbf{Now for something more challenging:} A real PDE with nonlinear physics!

\begin{block}{Problem Formulation}
The 1D nonlinear Darcy equation models groundwater flow with solution-dependent permeability:
\begin{equation*}
\frac{d}{dx}\left(-\kappa(u(x))\frac{du}{dx}\right) = f(x), \quad x \in [0,1]
\end{equation*}
where:
\begin{itemize}
    \item $u(x)$ is the \textbf{solution field} (pressure/hydraulic head)
    \item $\kappa(u) = 0.2 + u^2(x)$ is the \textbf{nonlinear permeability}
    \item $f(x) \sim \text{GP}(0, k(x, x'))$ is a \textbf{Gaussian random field source}
    \item Homogeneous Dirichlet BCs: $u(0) = 0$, $u(1) = 0$
\end{itemize}
\end{block}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Operator Learning Challenge}

\textbf{Goal:} Learn the solution operator $\mathcal{G}$ such that:
\begin{equation*}
\mathcal{G}[f] = u
\end{equation*}
where $u$ is the solution to the nonlinear Darcy equation for source $f$.

\vspace{1cm}

\begin{alertblock}{Why this is much harder than the derivative operator:}
\begin{enumerate}
    \item \textbf{Nonlinear PDE:} No analytical solution
    \item \textbf{Random sources:} Infinite variety of input functions  
    \item \textbf{Complex physics:} Solution depends on entire source profile
\end{enumerate}
\end{alertblock}

\vspace{0.5cm}

\textbf{Traditional approach:} For each new source $f$, solve the PDE numerically (expensive!)

\textbf{DeepONet approach:} Learn the operator once, then instant evaluation for any new source

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Darcy Dataset Generation}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{figs/darcy_samples.png}
	\caption*{Sample Gaussian random field sources and their corresponding solutions.}
\end{figure}

\textbf{Dataset characteristics:}
\begin{itemize}
    \item 1000 source functions $f(x)$ generated from a Gaussian process
    \item Each source solved numerically using iterative methods
    \item Solutions exhibit complex, nonlinear relationships to sources
    \item Provides rich training data for operator learning
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Learned Basis Functions for Darcy}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{figs/darcy_basis_functions.png}
	\caption*{First 8 basis functions learned by the trunk network.}
\end{figure}

\textbf{Observations:}
\begin{itemize}
    \item Basis functions capture diverse spatial patterns
    \item Some functions emphasize boundary behavior
    \item Others capture interior oscillations
    \item Together they span the solution space for the nonlinear Darcy operator
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Physics-Informed DeepONet}

%------------------------------------------------
\begin{frame}
\frametitle{Beyond Data-Driven Learning}

\textbf{The next frontier:} What if we could incorporate physics directly into operator learning?

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Standard DeepONet:}
        \begin{itemize}
            \item Learns from data only
            \item Requires many training examples
            \item No physics knowledge
            \item May violate physical laws
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Loss function:}
        \begin{equation*}
        \mathcal{L} = \|u_{pred} - u_{true}\|^2
        \end{equation*}
    \end{column}
    \begin{column}{0.5\textwidth}
        \textbf{Physics-Informed DeepONet:}
        \begin{itemize}
            \item Incorporates governing equations
            \item Enforces boundary conditions
            \item Requires fewer training examples
            \item Guarantees physical consistency
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Enhanced loss function:}
        \begin{equation*}
        \mathcal{L} = \mathcal{L}_{data} + \mathcal{L}_{physics} + \mathcal{L}_{boundary}
        \end{equation*}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Physics-Informed DeepONet for 1D Poisson Equation}

\textbf{Problem:} Learn the solution operator for the 1D Poisson equation:
\begin{equation*}
\frac{d^2u}{dx^2} = -f(x), \quad x \in [0,1]
\end{equation*}
with Dirichlet boundary conditions: $u(0) = 0$, $u(1) = 0$

\vspace{1cm}

\begin{block}{Physics-Informed Loss Components}
\begin{enumerate}
    \item \textbf{Data Loss:} $\mathcal{L}_{data} = \frac{1}{N} \sum_{i=1}^N \|u_{pred}^{(i)} - u_{true}^{(i)}\|^2$
    \item \textbf{Physics Loss:} $\mathcal{L}_{physics} = \frac{1}{N} \sum_{i=1}^N \left\|\frac{d^2u_{pred}^{(i)}}{dx^2} + f^{(i)}\right\|^2$
    \item \textbf{Boundary Loss:} $\mathcal{L}_{boundary} = \frac{1}{N} \sum_{i=1}^N \left[|u_{pred}^{(i)}(0)|^2 + |u_{pred}^{(i)}(1)|^2\right]$
\end{enumerate}
\end{block}

\vspace{0.5cm}

\centering
\href{https://colab.research.google.com/github/kks32-courses/sciml-sciml/blob/main/docs/02-deeponet/pideeponet.ipynb}{\beamergotobutton{Open Notebook: PI-DeepONet}}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Key Modifications from Standard DeepONet}

\begin{block}{1. Automatic Differentiation}
\begin{itemize}
    \item Use PyTorch's autograd to compute $\frac{d^2u}{dx^2}$
    \item Enable gradient computation through the network
    \item Essential for physics loss calculation
\end{itemize}
\end{block}

\begin{block}{2. Multi-Objective Training}
\begin{itemize}
    \item Balance data fidelity and physics consistency
    \item Tunable weights: $\lambda_{physics}$, $\lambda_{boundary}$
    \item Monitor all loss components during training
\end{itemize}
\end{block}

\begin{block}{3. Soft Constraint Enforcement}
\begin{itemize}
    \item Physics and boundary conditions as loss terms
    \item Flexible framework for different PDEs
    \item Automatically satisfied solutions
\end{itemize}
\end{block}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Architecture: Physics-Informed DeepONet}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/pideeponet-arch.png}
    \caption*{Physics-Informed DeepONet architecture with automatic differentiation.}
\end{figure}

\textbf{Key components:}
\begin{itemize}
    \item \textbf{Branch Network:} Processes source functions $f(x)$
    \item \textbf{Trunk Network:} Processes spatial coordinates $x$
    \item \textbf{Automatic Differentiation:} Computes derivatives for physics loss
    \item \textbf{Multi-Component Loss:} Combines data, physics, and boundary terms
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Training Dynamics: Loss Components}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figs/pi_training_losses.png}
    \caption*{Training history showing all loss components for PI-DeepONet.}
\end{figure}

\textbf{Observations:}
\begin{itemize}
    \item All loss components decrease during training
    \item Physics and boundary losses reach near-zero values
    \item Balanced learning across all objectives
    \item Automatic physics constraint satisfaction
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Physics Consistency Verification}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figs/pi_physics_verification.png}
    \caption*{Verification of physics consistency: predictions, physics residuals, and boundary conditions.}
\end{figure}

\textbf{Comprehensive evaluation:}
\begin{itemize}
    \item \textbf{Prediction accuracy:} L2 error between predicted and true solutions
    \item \textbf{Physics residual:} $\left|\frac{d^2u}{dx^2} + f(x)\right|$ approaches zero
    \item \textbf{Boundary conditions:} $|u(0)| + |u(1)|$ near machine precision
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Comparison: Standard vs Physics-Informed DeepONet}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figs/pi_vs_standard_comparison.png}
    \caption*{Comparison of standard DeepONet vs Physics-Informed DeepONet predictions.}
\end{figure}

\textbf{Quantitative improvements:}
\begin{itemize}
    \item \textbf{Prediction accuracy:} 15-30\% reduction in L2 error
    \item \textbf{Physics consistency:} Guaranteed PDE satisfaction
    \item \textbf{Boundary conditions:} Exact enforcement vs approximate
    \item \textbf{Data efficiency:} Better performance with fewer training examples
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Key Advantages of Physics-Informed DeepONet}

\begin{block}{Enhanced Performance}
\begin{itemize}
    \item \textbf{Better generalization:} Physics constraints improve extrapolation
    \item \textbf{Guaranteed consistency:} Solutions automatically satisfy PDEs
    \item \textbf{Reduced data requirements:} Physics knowledge compensates for limited data
\end{itemize}
\end{block}

\begin{block}{Interpretable Learning}
\begin{itemize}
    \item \textbf{Physics loss tracking:} Monitor PDE satisfaction during training
    \item \textbf{Boundary condition enforcement:} Explicit constraint satisfaction
    \item \textbf{Multi-scale learning:} Different components learn at different scales
\end{itemize}
\end{block}

\begin{block}{Flexible Framework}
\begin{itemize}
    \item \textbf{Adaptable to different PDEs:} Change physics loss for new equations
    \item \textbf{Various boundary conditions:} Dirichlet, Neumann, Robin, periodic
    \item \textbf{Multi-dimensional extension:} 2D/3D problems with same principles
\end{itemize}
\end{block}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Implementation Best Practices}

\begin{block}{1. Loss Balancing}
\begin{itemize}
    \item Carefully tune $\lambda_{physics}$ and $\lambda_{boundary}$ weights
    \item Monitor loss ratios during training
    \item Adjust weights based on problem complexity
\end{itemize}
\end{block}

\begin{block}{2. Training Strategy}
\begin{itemize}
    \item Use sufficient collocation points for physics loss
    \item Enable automatic differentiation properly
    \item Track all loss components separately
\end{itemize}
\end{block}

\begin{block}{3. Validation}
\begin{itemize}
    \item Always verify physics satisfaction on test data
    \item Check boundary condition compliance
    \item Compare with standard DeepONet baseline
\end{itemize}
\end{block}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Extensions and Future Directions}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Immediate Extensions:}
        \begin{itemize}
            \item \textbf{Different boundary conditions:} Neumann, Robin, periodic
            \item \textbf{Higher dimensions:} 2D/3D Poisson equations
            \item \textbf{Time-dependent PDEs:} Parabolic and hyperbolic equations
            \item \textbf{Nonlinear PDEs:} Variable coefficients and nonlinear terms
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \textbf{Advanced Applications:}
        \begin{itemize}
            \item \textbf{Inverse problems:} Learn unknown parameters from data
            \item \textbf{Multi-physics:} Coupled PDE systems
            \item \textbf{Uncertainty quantification:} Bayesian neural operators
            \item \textbf{Real-time control:} Fast PDE-constrained optimization
        \end{itemize}
    \end{column}
\end{columns}

\vspace{1cm}

\begin{center}
\textbf{Physics-informed DeepONet represents a powerful paradigm that combines the flexibility of neural networks with the rigor of physical laws.}
\end{center}

\end{frame}

%------------------------------------------------
\section{Summary and Implications}

%------------------------------------------------
\begin{frame}
\frametitle{What We've Learned: The Paradigm Shift}

\begin{block}{1. Conceptual Leap}
\begin{itemize}
    \item \textbf{Functions:} $\mathbb{R}^d \rightarrow \mathbb{R}^m$ (point to point)
    \item \textbf{Operators:} $\mathcal{F}_1 \rightarrow \mathcal{F}_2$ (function to function)
\end{itemize}
\end{block}

\begin{block}{2. Theoretical Foundation}
\begin{itemize}
    \item Universal Approximation Theorem for Operators
    \item Branch-trunk architecture emerges naturally
    \item Basis function decomposition: $\mathcal{G}(u)(y) = \sum_k b_k(u) \cdot t_k(y)$
\end{itemize}
\end{block}

\begin{block}{3. Practical Implementation}
\begin{itemize}
    \item \textbf{Branch network:} Encodes input functions into coefficients
    \item \textbf{Trunk network:} Generates basis functions at query points
    \item \textbf{Training:} Learn from input-output function pairs
\end{itemize}
\end{block}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Key Advantages of DeepONet}

\begin{itemize}
	\item \textbf{Resolution independence:} Train on one grid, evaluate on any grid
    \item \textbf{Fast evaluation:} Once trained, instant prediction
    \item \textbf{Generalization:} Works for new functions not seen during training  
    \item \textbf{Physical consistency:} Learns the underlying operator, not just patterns
\end{itemize}

\vspace{1cm}

\begin{block}{DeepONet represents a paradigm shift:}
\begin{itemize}
    \item \textbf{Traditional numerical methods:} Solve each problem instance
    \item \textbf{Operator learning:} Learn the solution pattern once, apply everywhere
\end{itemize}
\end{block}
\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{When to Use DeepONet}

\begin{block}{Ideal scenarios:}
\begin{itemize}
    \item \textbf{Parametric PDEs:} Need solutions for many different source terms/boundary conditions
    \item \textbf{Real-time applications:} Require instant evaluation
    \item \textbf{Complex geometries:} Traditional methods struggle
    \item \textbf{Multi-query problems:} Same operator, many evaluations
\end{itemize}
\end{block}

\begin{alertblock}{Limitations:}
\begin{itemize}
    \item \textbf{Training data:} Need many solved examples
    \item \textbf{Complex operators:} Very nonlinear mappings may be challenging
    \item \textbf{High dimensions:} Curse of dimensionality still applies
\end{itemize}
\end{alertblock}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Bigger Picture}

This opens new possibilities for:

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Scientific Applications:}
        \begin{itemize}
            \item \textbf{Inverse problems:} Learn parameter-to-solution mappings
            \item \textbf{Control applications:} Real-time system response
            \item \textbf{Multi-physics:} Coupled operator learning
            \item \textbf{Scientific discovery:} Understanding operator structure
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \textbf{Future Directions:}
        \begin{itemize}
            \item \textbf{Multi-output operators:} Vector-valued mappings
            \item \textbf{Higher dimensions:} 2D/3D PDEs
            \item \textbf{Physics-informed training:} Incorporate governing equations
            \item \textbf{Fourier Neural Operators:} Alternative architectures
        \end{itemize}
    \end{column}
\end{columns}

\vspace{1cm}

\begin{center}
\textbf{We've seen how physics-informed DeepONet combines PINNs with operator learning!}
\end{center}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Questions?}

\centering
\Large Thank you!

\vspace{2cm}

\textbf{Contact:} \\
Krishna Kumar \\
\textit{krishnak@utexas.edu} \\
University of Texas at Austin

\vspace{1cm}

\textbf{Interactive Demo:} \\
\href{https://colab.research.google.com/github/kks32-courses/sciml-sciml/blob/main/docs/02-deeponet/deeponet.ipynb}{\beamergotobutton{DeepONet Notebook}}

\end{frame}

\end{document}