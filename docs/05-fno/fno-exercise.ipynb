{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourier Neural Operator - Exercise\n",
    "\n",
    "This notebook contains exercises to implement and experiment with FNO.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Understanding Fourier Transform\n",
    "\n",
    "### Task\n",
    "\n",
    "Implement a function that computes the derivative using Fourier transform.\n",
    "\n",
    "**Theory**: The derivative in Fourier space is:\n",
    "\n",
    "$$\n",
    "\\widehat{\\frac{du}{dx}}(k) = 2\\pi i k \\cdot \\hat{u}(k)\n",
    "$$\n",
    "\n",
    "**Steps**:\n",
    "1. Apply FFT to $u(x)$\n",
    "2. Multiply by $2\\pi i k$\n",
    "3. Apply inverse FFT\n",
    "\n",
    "### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_derivative(u, dx):\n",
    "    \"\"\"\n",
    "    Compute derivative using Fourier transform\n",
    "    \n",
    "    Args:\n",
    "        u: Function values (n_points,)\n",
    "        dx: Grid spacing\n",
    "    \n",
    "    Returns:\n",
    "        du_dx: Derivative (n_points,)\n",
    "    \"\"\"\n",
    "    n = len(u)\n",
    "    \n",
    "    # TODO: Implement this function\n",
    "    # Hint 1: Use torch.fft.fft and torch.fft.ifft\n",
    "    # Hint 2: Frequencies are torch.fft.fftfreq(n, d=dx)\n",
    "    # Hint 3: Multiply by 2*pi*i*k in frequency space\n",
    "    \n",
    "    raise NotImplementedError(\"Implement fourier_derivative\")\n",
    "    \n",
    "    return du_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "x = torch.linspace(0, 2*np.pi, 100)\n",
    "u = torch.sin(x)  # u(x) = sin(x)\n",
    "du_true = torch.cos(x)  # du/dx = cos(x)\n",
    "\n",
    "dx = x[1] - x[0]\n",
    "du_computed = fourier_derivative(u, dx)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x, du_true, 'k-', label='True derivative', linewidth=2)\n",
    "plt.plot(x, du_computed.real, 'r--', label='Fourier derivative', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('du/dx')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('Derivative of sin(x)')\n",
    "plt.show()\n",
    "\n",
    "error = torch.abs(du_true - du_computed.real).mean()\n",
    "print(f\"Mean absolute error: {error:.6f}\")\n",
    "print(f\"Test passed: {error < 1e-5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement Spectral Convolution Layer\n",
    "\n",
    "### Task\n",
    "\n",
    "Complete the `SpectralConv1d` class. The forward pass should:\n",
    "1. Apply real FFT to input\n",
    "2. Multiply by learned complex weights (only for low frequencies)\n",
    "3. Apply inverse FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes = modes\n",
    "        \n",
    "        # Initialize weights\n",
    "        scale = 1 / (in_channels * out_channels)\n",
    "        self.weights = nn.Parameter(\n",
    "            scale * torch.rand(in_channels, out_channels, modes, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, n_points)\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # TODO: Implement the forward pass\n",
    "        # Step 1: Apply FFT using torch.fft.rfft\n",
    "        # Step 2: Convert weights to complex using torch.view_as_complex\n",
    "        # Step 3: Multiply x_ft by weights (use torch.einsum)\n",
    "        # Step 4: Apply inverse FFT using torch.fft.irfft\n",
    "        \n",
    "        raise NotImplementedError(\"Implement SpectralConv1d.forward\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test your implementation\n",
    "layer = SpectralConv1d(in_channels=4, out_channels=4, modes=8)\n",
    "x = torch.randn(2, 4, 32)\n",
    "y = layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Test passed: {y.shape == x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Build a Simple FNO\n",
    "\n",
    "### Task\n",
    "\n",
    "Construct a simple FNO with 2 Fourier layers to learn the antiderivative operator:\n",
    "\n",
    "$$\n",
    "G: f \\mapsto u \\text{ where } \\frac{du}{dx} = f\n",
    "$$\n",
    "\n",
    "Given $f(x)$, predict $u(x) = \\int_0^x f(s) ds$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "def generate_antiderivative_data(n_samples, n_points=128):\n",
    "    x = np.linspace(0, 2*np.pi, n_points)\n",
    "    \n",
    "    f_all = []\n",
    "    u_all = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Random function: sum of sines\n",
    "        n_modes = np.random.randint(2, 6)\n",
    "        f = np.zeros(n_points)\n",
    "        u = np.zeros(n_points)\n",
    "        \n",
    "        for _ in range(n_modes):\n",
    "            k = np.random.randint(1, 5)\n",
    "            amp = np.random.randn()\n",
    "            phase = np.random.uniform(0, 2*np.pi)\n",
    "            \n",
    "            f += amp * np.cos(k*x + phase)\n",
    "            u += (amp/k) * np.sin(k*x + phase)\n",
    "        \n",
    "        f_all.append(f)\n",
    "        u_all.append(u)\n",
    "    \n",
    "    return np.array(f_all), np.array(u_all), x\n",
    "\n",
    "n_train = 500\n",
    "n_test = 100\n",
    "n_points = 128\n",
    "\n",
    "f_train, u_train, x = generate_antiderivative_data(n_train, n_points)\n",
    "f_test, u_test, _ = generate_antiderivative_data(n_test, n_points)\n",
    "\n",
    "print(f\"Training: {f_train.shape}\")\n",
    "print(f\"Test: {f_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize examples\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for i in range(3):\n",
    "    axes[0, i].plot(x, f_train[i])\n",
    "    axes[0, i].set_title(f'f(x) - Example {i+1}')\n",
    "    axes[0, i].set_xlabel('x')\n",
    "    axes[0, i].set_ylabel('f(x)')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, i].plot(x, u_train[i])\n",
    "    axes[1, i].set_title(r'$u(x) = \\int f(x)dx$')\n",
    "    axes[1, i].set_xlabel('x')\n",
    "    axes[1, i].set_ylabel('u(x)')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFNO(nn.Module):\n",
    "    \"\"\"Simple FNO for learning antiderivative\"\"\"\n",
    "    def __init__(self, modes, width):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Build your FNO\n",
    "        # Components needed:\n",
    "        # 1. Lifting layer: nn.Linear(2, width) for (x, f(x))\n",
    "        # 2. Two Fourier layers: SpectralConv1d(width, width, modes)\n",
    "        # 3. Two local layers: nn.Conv1d(width, width, 1)\n",
    "        # 4. Projection: nn.Linear(width, 1)\n",
    "        \n",
    "        raise NotImplementedError(\"Implement SimpleFNO\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        # 1. Lift input\n",
    "        # 2. For each Fourier layer: x = gelu(fourier(x) + local(x))\n",
    "        # 3. Project to output\n",
    "        \n",
    "        raise NotImplementedError(\"Implement SimpleFNO.forward\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleFNO(modes=16, width=32).to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def prepare_data(f, x):\n",
    "    n_samples = f.shape[0]\n",
    "    n_points = len(x)\n",
    "    grid = np.tile(x.reshape(1, -1, 1), (n_samples, 1, 1))\n",
    "    values = f.reshape(n_samples, n_points, 1)\n",
    "    data = np.concatenate([grid, values], axis=-1)\n",
    "    return torch.FloatTensor(data)\n",
    "\n",
    "X_train = prepare_data(f_train, x).to(device)\n",
    "y_train = torch.FloatTensor(u_train).to(device)\n",
    "\n",
    "X_test = prepare_data(f_test, x).to(device)\n",
    "y_test = torch.FloatTensor(u_test).to(device)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "n_epochs = 500\n",
    "batch_size = 20\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    model.train()\n",
    "    perm = torch.randperm(len(X_train))\n",
    "    train_loss = 0\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        X_batch = X_train[idx]\n",
    "        y_batch = y_train[idx]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = F.mse_loss(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * len(X_batch)\n",
    "    \n",
    "    train_loss /= len(X_train)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        test_loss = F.mse_loss(y_pred, y_test).item()\n",
    "        test_losses.append(test_loss)\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}: train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.semilogy(train_losses, label='Train')\n",
    "plt.semilogy(test_losses, label='Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test).cpu().numpy()\n",
    "\n",
    "y_test_np = y_test.cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "\n",
    "for i in range(3):\n",
    "    # Input f(x)\n",
    "    axes[0, i].plot(x, f_test[i])\n",
    "    axes[0, i].set_title(f'Input f(x) - {i+1}')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Prediction\n",
    "    axes[1, i].plot(x, y_test_np[i], 'k-', label='True', linewidth=2)\n",
    "    axes[1, i].plot(x, y_pred_test[i], 'r--', label='FNO', linewidth=2)\n",
    "    axes[1, i].set_title(r'Antiderivative $\\int f dx$')\n",
    "    axes[1, i].legend()\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error\n",
    "    error = np.abs(y_test_np[i] - y_pred_test[i])\n",
    "    axes[2, i].plot(x, error)\n",
    "    axes[2, i].set_title(f'Error (MAE={np.mean(error):.4f})')\n",
    "    axes[2, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Resolution Invariance Test\n",
    "\n",
    "### Task\n",
    "\n",
    "Test if your trained FNO can handle different resolutions:\n",
    "1. Train on $n=128$ points\n",
    "2. Evaluate on $n=256$ points\n",
    "3. Evaluate on $n=64$ points\n",
    "\n",
    "Compare the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement resolution test\n",
    "# Steps:\n",
    "# 1. Generate test data at n=256 and n=64\n",
    "# 2. Interpolate f(x) to these resolutions\n",
    "# 3. Compute true antiderivative\n",
    "# 4. Run model predictions\n",
    "# 5. Compare MAE for different resolutions\n",
    "\n",
    "def test_resolution(model, f, x_original, n_points_new):\n",
    "    \"\"\"Test model at different resolution\"\"\"\n",
    "    # TODO: Implement this\n",
    "    raise NotImplementedError(\"Implement test_resolution\")\n",
    "\n",
    "# Test at different resolutions\n",
    "resolutions = [64, 128, 256]\n",
    "errors = []\n",
    "\n",
    "for n in resolutions:\n",
    "    error = test_resolution(model, f_test[0], x, n)\n",
    "    errors.append(error)\n",
    "    print(f\"Resolution {n}: MAE = {error:.6f}\")\n",
    "\n",
    "# Plot errors vs resolution\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(resolutions, errors, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Resolution (n_points)')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Resolution Invariance Test')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Effect of Fourier Modes\n",
    "\n",
    "### Task\n",
    "\n",
    "Study how the number of Fourier modes affects performance:\n",
    "1. Train FNOs with `modes = [4, 8, 16, 32]`\n",
    "2. Compare test error and number of parameters\n",
    "3. Plot error vs modes\n",
    "\n",
    "**Question**: What happens when `modes` is too small? Too large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement mode comparison\n",
    "# Train multiple models with different modes\n",
    "# Compare:\n",
    "# - Test error\n",
    "# - Number of parameters\n",
    "# - Training time\n",
    "\n",
    "mode_values = [4, 8, 16, 32]\n",
    "results = []\n",
    "\n",
    "for modes in mode_values:\n",
    "    # TODO: Train and evaluate model\n",
    "    pass\n",
    "\n",
    "# Plot results\n",
    "# TODO: Create comparison plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Compare FNO with MLP\n",
    "\n",
    "### Task\n",
    "\n",
    "Build an MLP baseline that maps $(x, f(x)) \\to u(x)$ point-wise.\n",
    "\n",
    "Compare:\n",
    "1. Test error\n",
    "2. Number of parameters\n",
    "3. Resolution invariance\n",
    "\n",
    "**Question**: Why does FNO have better resolution invariance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBaseline(nn.Module):\n",
    "    \"\"\"Point-wise MLP baseline\"\"\"\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super().__init__()\n",
    "        # TODO: Build MLP\n",
    "        # Input: (x, f(x)) - 2 dimensions\n",
    "        # Output: u(x) - 1 dimension\n",
    "        # Architecture: 2 -> hidden -> hidden -> 1\n",
    "        raise NotImplementedError(\"Implement MLPBaseline\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward\n",
    "        # Note: MLP processes each point independently\n",
    "        raise NotImplementedError(\"Implement MLPBaseline.forward\")\n",
    "        return x\n",
    "\n",
    "# TODO: Train and compare with FNO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise: 2D Problem\n",
    "\n",
    "### Task\n",
    "\n",
    "Implement a 2D Poisson solver:\n",
    "\n",
    "$$\n",
    "-\\Delta u = f, \\quad u|_{\\partial D} = 0\n",
    "$$\n",
    "\n",
    "Use a 2D FNO to learn the mapping $f \\to u$.\n",
    "\n",
    "**Hint**: You'll need `SpectralConv2d` and `torch.fft.rfft2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement 2D Poisson solver with FNO\n",
    "# 1. Generate data: random source f, solve for u\n",
    "# 2. Build 2D FNO\n",
    "# 3. Train and evaluate\n",
    "# 4. Test resolution invariance in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Questions\n",
    "\n",
    "Answer these after completing the exercises:\n",
    "\n",
    "1. **Why does FNO use FFT instead of learning convolution kernels directly?**\n",
    "\n",
    "2. **What is the computational complexity of one FNO layer?**\n",
    "   - With FFT: ?\n",
    "   - Without FFT (direct convolution): ?\n",
    "\n",
    "3. **Why is FNO resolution-invariant but CNN is not?**\n",
    "\n",
    "4. **What happens if you set `modes` = `n_points`? Why is truncation useful?**\n",
    "\n",
    "5. **Compare FNO with DeepONet**:\n",
    "   - When would you use FNO?\n",
    "   - When would you use DeepONet?\n",
    "\n",
    "6. **Can FNO handle non-uniform grids? Why or why not?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
