# Neural ODE Course Materials

This directory contains comprehensive materials on Neural Ordinary Differential Equations.

## Files

### Slides
- **`neural-ode-slides.tex`** - LaTeX beamer slides (slides only, no notes)
- **`neural-ode-slides.pdf`** - Compiled slides PDF (38 pages)

### Technical Notes
- **`neural-ode-notes.tex`** - Comprehensive technical document with detailed explanations
- **`neural-ode-notes.pdf`** - Compiled notes PDF (17 pages)

### Notebooks
- **`latent-ode.ipynb`** - Jupyter notebook implementing Latent ODEs for irregular time series
  - Includes comparison with ResNet baseline
  - Extended time horizon (12Ï€) for better visualization
  - Forward and backward extrapolation demonstrations

### Papers
- **`neuralode.pdf`** - Original Neural ODE paper (Chen et al., NeurIPS 2018)
- **`latentode.pdf`** - Latent ODEs for Irregularly-Sampled Time Series
- **`zeroshot-neural-ode.pdf`** - Zero-Shot Transfer with Function Encoders

### Figures
- **`figs/`** - Directory containing all figures used in slides and notebooks
  - `adjoint.png` - Adjoint method visualization
  - `neuralode.png` - Neural ODE architecture
  - `resent-neuralode-arch.png` - ResNet vs Neural ODE comparison
  - Additional figures generated by notebooks

## Content Overview

### Slides Topics
1. From ResNets to Continuous Dynamics
2. Neural ODE Architecture
3. Training Neural ODEs
4. Detailed Adjoint Method
5. Applications (Latent ODEs, Function Encoders)

### Technical Notes Topics
1. **From ResNets to Continuous Dynamics**
   - The core insight connecting deep learning and ODEs
   - Why Euler is the worst integration method
   - Compound interest analogy
   - ResNets as discrete approximations

2. **The Training Challenge**
   - Memory cost of standard backpropagation
   - Why $\mathcal{O}(N)$ memory is prohibitive
   - The need for $\mathcal{O}(1)$ memory solution

3. **The Adjoint Method**
   - Optimal control perspective
   - Lagrange multipliers formulation
   - Complete derivation from first principles
   - Integration by parts technique
   - How autodiff makes it practical

4. **The Adjoint Algorithm**
   - Concrete forward and backward pass descriptions
   - Augmented ODE system
   - Understanding each component's dynamics
   - Vector-Jacobian products (VJPs)

5. **Detailed Adjoint Derivation**
   - The flow map concept
   - Why direct differentiation fails (circular dependency)
   - Lagrange multiplier setup
   - Step-by-step derivation with integration by parts
   - Making "bad" terms vanish
   - Final gradient formula

6. **Latent ODEs**
   - Motivation for irregular time series
   - ODE-RNN encoder architecture
   - VAE framework
   - ELBO loss function explained

7. **The ELBO Loss**
   - What is ELBO and why we need it
   - Reconstruction vs regularization terms
   - Closed-form KL divergence for Gaussians
   - Key insight: KL only on initial state
   - Training algorithm

8. **Function Encoders**
   - Zero-shot transfer problem
   - Hilbert space formulation
   - Inner products for dynamical systems
   - Monte Carlo computation of coefficients

## Usage

### Compiling LaTeX Files

```bash
# Slides
pdflatex neural-ode-slides.tex

# Notes (run twice for table of contents)
pdflatex neural-ode-notes.tex
pdflatex neural-ode-notes.tex
```

### Running Notebooks

```bash
# Activate virtual environment (if available)
source ../../env/bin/activate

# Launch Jupyter
jupyter notebook latent-ode.ipynb
```

## Key Pedagogical Approach

The materials follow a clear pedagogical style inspired by:
- **3Blue1Brown**: Visual intuition and clear motivation
- **Gilbert Strang**: Direct, concise explanations
- **Andrej Karpathy**: Implementation details and practical insights
- **Steven Brunton**: Connection to classical numerical methods

### Writing Principles
- **Clarity First**: Every concept explained intuitively before formalism
- **No Fluff**: Technical and precise, avoid unnecessary praise or filler
- **Conciseness**: Omit needless words (Strunk & White)
- **Show, Don't Tell**: Examples and derivations instead of vague statements

## References

1. Chen, R. T., Rubanova, Y., Bettencourt, J., & Duvenaud, D. (2018). Neural Ordinary Differential Equations. NeurIPS 2018.

2. Rubanova, Y., Chen, R. T., & Duvenaud, D. (2019). Latent ODEs for Irregularly-Sampled Time Series. NeurIPS 2019.

3. Brunton, S. Neural Ordinary Differential Equations [YouTube Lecture]

## License

CC BY-NC-SA 4.0 (http://creativecommons.org/licenses/by-nc-sa/4.0/)
