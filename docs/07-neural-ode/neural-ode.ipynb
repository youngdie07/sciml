{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Neural Ordinary Differential Equations\n\n**Learning Objectives:**\n- Understand the connection between ResNets and continuous dynamics\n- Master the adjoint method for memory-efficient backpropagation\n- Apply Neural ODEs to learn dynamical systems\n\n**Notebook:** [![Open in Colab](https://img.shields.io/badge/Open%20in-Colab-F9AB00?style=flat-square&logo=googlecolab)](https://colab.research.google.com/github/kks32-courses/sciml/blob/main/docs/07-neural-ode/neural-ode.ipynb)\n\n**Paper:** [![Neural ODE Paper](https://img.shields.io/badge/View-Paper-red?style=flat-square&logo=adobeacrobatreader&logoColor=white)](https://arxiv.org/abs/1806.07366)\n\n---\n\n**Reference:** Chen et al., \"Neural Ordinary Differential Equations,\" NeurIPS 2018 (Best Paper)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From ResNets to Continuous Dynamics\n",
    "\n",
    "### The ResNet Formula\n",
    "\n",
    "A residual network transforms the hidden state at each layer:\n",
    "\n",
    "$$h_{t+1} = h_t + f(h_t, \\theta_t)$$\n",
    "\n",
    "where $t \\in \\{0, 1, \\ldots, T\\}$ indexes discrete layers.\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "This is **Euler's method** for solving an ODE. As we add more layers and take smaller steps:\n",
    "\n",
    "$$\\frac{dh(t)}{dt} = f(h(t), t, \\theta)$$\n",
    "\n",
    "Instead of discrete layers, we parameterize the **derivative** of the hidden state.\n",
    "\n",
    "### Three Key Advantages\n",
    "\n",
    "1. **Memory Efficiency**: $\\mathcal{O}(1)$ vs $\\mathcal{O}(L)$ via adjoint method\n",
    "2. **Adaptive Computation**: ODE solvers adjust evaluations automatically  \n",
    "3. **Continuous Time**: Natural for irregular data and physical systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Add torchdiffeq to path\n",
    "sys.path.insert(0, './torchdiffeq')\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Adjoint Method\n",
    "\n",
    "### The Memory Problem\n",
    "\n",
    "Standard backpropagation through an ODE solver:\n",
    "- Must store all intermediate states\n",
    "- Memory: $\\mathcal{O}(\\text{NFE})$ where NFE = number of function evaluations\n",
    "\n",
    "### The Solution\n",
    "\n",
    "Define the **adjoint state** $a(t) = \\frac{\\partial L}{\\partial h(t)}$, which evolves backward:\n",
    "\n",
    "$$\\frac{da(t)}{dt} = -a(t)^T \\frac{\\partial f(h(t), t, \\theta)}{\\partial h}$$\n",
    "\n",
    "Gradients for parameters:\n",
    "\n",
    "$$\\frac{dL}{d\\theta} = -\\int_{t_1}^{t_0} a(t)^T \\frac{\\partial f(h(t), t, \\theta)}{\\partial \\theta} dt$$\n",
    "\n",
    "**Key property**: Memory cost is $\\mathcal{O}(1)$ - independent of number of evaluations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Learning Spiral Dynamics\n",
    "\n",
    "This is the main example from the Neural ODE paper (Section 5.1), implemented in `torchdiffeq/examples/ode_demo.py`.\n",
    "\n",
    "**True dynamics:**\n",
    "$$\\frac{dz}{dt} = A z^3$$\n",
    "\n",
    "where $A = \\begin{bmatrix} -0.1 & 2.0 \\\\ -2.0 & -0.1 \\end{bmatrix}$\n",
    "\n",
    "The network learns to approximate these dynamics from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate true spiral data (from ode_demo.py)\n",
    "data_size = 1000\n",
    "true_y0 = torch.tensor([[2., 0.]]).to(device)\n",
    "t = torch.linspace(0., 25., data_size).to(device)\n",
    "true_A = torch.tensor([[-0.1, 2.0], [-2.0, -0.1]]).to(device)\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def forward(self, t, y):\n",
    "        return torch.mm(y**3, true_A)\n",
    "\n",
    "with torch.no_grad():\n",
    "    true_y = odeint(Lambda(), true_y0, t, method='dopri5')\n",
    "\n",
    "print(f\"Data shape: {true_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize true spiral\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "t_np = t.cpu().numpy()\n",
    "y_np = true_y.cpu().numpy()\n",
    "\n",
    "ax1.plot(y_np[:, 0, 0], y_np[:, 0, 1], 'b-', linewidth=2)\n",
    "ax1.plot(y_np[0, 0, 0], y_np[0, 0, 1], 'go', markersize=10, label='Start')\n",
    "ax1.plot(y_np[-1, 0, 0], y_np[-1, 0, 1], 'ro', markersize=10, label='End')\n",
    "ax1.set_xlabel('$z_1$')\n",
    "ax1.set_ylabel('$z_2$')\n",
    "ax1.set_title('True Spiral Dynamics')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axis('equal')\n",
    "\n",
    "ax2.plot(t_np, y_np[:, 0, 0], 'b-', label='$z_1(t)$', linewidth=2)\n",
    "ax2.plot(t_np, y_np[:, 0, 1], 'r-', label='$z_2(t)$', linewidth=2)\n",
    "ax2.set_xlabel('Time $t$')\n",
    "ax2.set_ylabel('State')\n",
    "ax2.set_title('Time Series')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/spiral_data.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural ODE (exact implementation from torchdiffeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact ODEFunc from ode_demo.py\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 2),\n",
    "        )\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "    \n",
    "    def forward(self, t, y):\n",
    "        return self.net(y**3)\n",
    "\n",
    "# Exact batch function from ode_demo.py\n",
    "batch_time = 10\n",
    "batch_size = 20\n",
    "\n",
    "def get_batch():\n",
    "    s = torch.from_numpy(np.random.choice(np.arange(data_size - batch_time, dtype=np.int64), \n",
    "                                          batch_size, replace=False))\n",
    "    batch_y0 = true_y[s]  # (M, D)\n",
    "    batch_t = t[:batch_time]  # (T)\n",
    "    batch_y = torch.stack([true_y[s + i] for i in range(batch_time)], dim=0)  # (T, M, D)\n",
    "    return batch_y0.to(device), batch_t.to(device), batch_y.to(device)\n",
    "\n",
    "# Train exactly as in ode_demo.py\n",
    "func = ODEFunc().to(device)\n",
    "optimizer = optim.RMSprop(func.parameters(), lr=1e-3)\n",
    "\n",
    "niters = 2000\n",
    "test_freq = 100\n",
    "losses = []\n",
    "\n",
    "print(\"Training Neural ODE...\")\n",
    "for itr in range(1, niters + 1):\n",
    "    optimizer.zero_grad()\n",
    "    batch_y0, batch_t, batch_y = get_batch()\n",
    "    pred_y = odeint(func, batch_y0, batch_t).to(device)\n",
    "    loss = torch.mean(torch.abs(pred_y - batch_y))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if itr % test_freq == 0:\n",
    "        with torch.no_grad():\n",
    "            pred_y = odeint(func, true_y0, t)\n",
    "            test_loss = torch.mean(torch.abs(pred_y - true_y))\n",
    "            print(f'Iter {itr:04d} | Loss {test_loss:.6f}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final model\n",
    "with torch.no_grad():\n",
    "    pred_y = odeint(func, true_y0, t)\n",
    "\n",
    "pred_y_np = pred_y.cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curve\n",
    "axes[0, 0].plot(losses, linewidth=1, alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Training Loss (MAE)')\n",
    "axes[0, 0].set_title('Training Progress')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Phase portrait\n",
    "axes[0, 1].plot(y_np[:, 0, 0], y_np[:, 0, 1], 'g-', linewidth=2, alpha=0.7, label='True')\n",
    "axes[0, 1].plot(pred_y_np[:, 0, 0], pred_y_np[:, 0, 1], 'b--', linewidth=2, label='Predicted')\n",
    "axes[0, 1].set_xlabel('$z_1$')\n",
    "axes[0, 1].set_ylabel('$z_2$')\n",
    "axes[0, 1].set_title('Phase Portrait')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axis('equal')\n",
    "\n",
    "# Time series z1\n",
    "axes[1, 0].plot(t_np, y_np[:, 0, 0], 'g-', linewidth=2, alpha=0.7, label='True')\n",
    "axes[1, 0].plot(t_np, pred_y_np[:, 0, 0], 'b--', linewidth=2, label='Predicted')\n",
    "axes[1, 0].set_xlabel('Time $t$')\n",
    "axes[1, 0].set_ylabel('$z_1$')\n",
    "axes[1, 0].set_title('Component $z_1(t)$')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Time series z2\n",
    "axes[1, 1].plot(t_np, y_np[:, 0, 1], 'g-', linewidth=2, alpha=0.7, label='True')\n",
    "axes[1, 1].plot(t_np, pred_y_np[:, 0, 1], 'b--', linewidth=2, label='Predicted')\n",
    "axes[1, 1].set_xlabel('Time $t$')\n",
    "axes[1, 1].set_ylabel('$z_2$')\n",
    "axes[1, 1].set_title('Component $z_2(t)$')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/spiral_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "final_error = torch.mean(torch.abs(pred_y - true_y))\n",
    "print(f\"Final MAE: {final_error.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned vector field\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Create grid\n",
    "y_grid, x_grid = np.mgrid[-2:2:21j, -2:2:21j]\n",
    "grid_points = torch.Tensor(np.stack([x_grid, y_grid], -1).reshape(21 * 21, 2)).to(device)\n",
    "\n",
    "# Compute learned dynamics\n",
    "with torch.no_grad():\n",
    "    dydt = func(0, grid_points).cpu().numpy()\n",
    "\n",
    "mag = np.sqrt(dydt[:, 0]**2 + dydt[:, 1]**2).reshape(-1, 1)\n",
    "dydt_norm = (dydt / mag).reshape(21, 21, 2)\n",
    "\n",
    "# Learned field\n",
    "ax1.streamplot(x_grid, y_grid, dydt_norm[:, :, 0], dydt_norm[:, :, 1], color='black', density=1.5)\n",
    "ax1.plot(pred_y_np[:, 0, 0], pred_y_np[:, 0, 1], 'b-', linewidth=2, alpha=0.7, label='Predicted trajectory')\n",
    "ax1.set_xlim(-2, 2)\n",
    "ax1.set_ylim(-2, 2)\n",
    "ax1.set_xlabel('$z_1$')\n",
    "ax1.set_ylabel('$z_2$')\n",
    "ax1.set_title('Learned Vector Field')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# True field\n",
    "with torch.no_grad():\n",
    "    true_dydt = Lambda()(0, grid_points).cpu().numpy()\n",
    "\n",
    "true_mag = np.sqrt(true_dydt[:, 0]**2 + true_dydt[:, 1]**2).reshape(-1, 1)\n",
    "true_dydt_norm = (true_dydt / true_mag).reshape(21, 21, 2)\n",
    "\n",
    "ax2.streamplot(x_grid, y_grid, true_dydt_norm[:, :, 0], true_dydt_norm[:, :, 1], color='black', density=1.5)\n",
    "ax2.plot(y_np[:, 0, 0], y_np[:, 0, 1], 'g-', linewidth=2, alpha=0.7, label='True trajectory')\n",
    "ax2.set_xlim(-2, 2)\n",
    "ax2.set_ylim(-2, 2)\n",
    "ax2.set_xlabel('$z_1$')\n",
    "ax2.set_ylabel('$z_2$')\n",
    "ax2.set_title('True Vector Field')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/vector_fields.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Conceptual Foundation**\n",
    "   - ResNets are Euler discretizations: $h_{t+1} = h_t + f(h_t) \\to \\frac{dh}{dt} = f(h, t)$\n",
    "   - Neural ODEs parameterize continuous dynamics\n",
    "\n",
    "2. **Key Advantages**\n",
    "   - **Memory**: $\\mathcal{O}(1)$ via adjoint method\n",
    "   - **Adaptive**: ODE solvers adjust computation automatically\n",
    "   - **Continuous**: Natural for physical systems and irregular data\n",
    "\n",
    "3. **Spiral Example**\n",
    "   - Learned complex nonlinear dynamics: $\\frac{dz}{dt} = A z^3$\n",
    "   - Network approximates the true vector field\n",
    "   - Training uses mini-batches of trajectory segments\n",
    "\n",
    "### When to Use Neural ODEs\n",
    "\n",
    "**Good for:**\n",
    "- Modeling physical systems with continuous dynamics\n",
    "- Very deep networks (memory constrained)\n",
    "- Irregular time series\n",
    "- Learning dynamical systems from data\n",
    "\n",
    "**Challenges:**\n",
    "- Training can be slower than ResNets\n",
    "- Requires tuning ODE solver tolerances\n",
    "- Numerical stability considerations\n",
    "\n",
    "### Extensions\n",
    "\n",
    "- **Augmented Neural ODEs**: Add extra dimensions for capacity\n",
    "- **Hamiltonian Neural Networks**: Preserve physical structure\n",
    "- **Latent ODEs**: For irregular time series\n",
    "- **Continuous Normalizing Flows**: Generative modeling\n",
    "\n",
    "### References\n",
    "\n",
    "1. Chen et al., \"Neural Ordinary Differential Equations,\" NeurIPS 2018\n",
    "2. Grathwohl et al., \"FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models,\" ICLR 2019\n",
    "3. Rubanova et al., \"Latent ODEs for Irregularly-Sampled Time Series,\" NeurIPS 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}