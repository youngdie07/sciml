%**************************************************************************************
% License:
% CC BY-NC-SA 4.0 (http://creativecommons.org/licenses/by-nc-sa/4.0/)
%**************************************************************************************

\documentclass[notes]{beamer}

\mode<presentation> {

\usetheme{Madrid}

% Burnt orange
\definecolor{burntorange}{rgb}{0.8, 0.33, 0.0}
\colorlet{beamer@blendedblue}{burntorange}
% Pale yellow
\definecolor{paleyellow}{rgb}{1.0, 1.0, 0.953}
\setbeamercolor{background canvas}{bg=paleyellow}
% Secondary and tertiary palette
\setbeamercolor*{palette secondary}{use=structure,fg=white,bg=burntorange!80!black}
\setbeamercolor*{palette tertiary}{use=structure,fg=white,bg=burntorange!60!black}

% To remove the navigation symbols from the bottom of all slides uncomment this line
%\setbeamertemplate{navigation symbols}{}
}

\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{breqn}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage[labelsep=space,tableposition=top]{caption}
\renewcommand{\figurename}{Fig.}
\usepackage{caption,subcaption}
\usepackage{xcolor}
\usepackage{hyperref}

\AtBeginSection[]{
	\begin{frame}{Outline}
		\tableofcontents[
		currentsection,
		hideallsubsections
		]
	\end{frame}
}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------
\title[Neural ODEs]{Neural Ordinary Differential Equations}
\author{Krishna Kumar}
\institute[UT Austin]
{
University of Texas at Austin \\
\medskip
\textit{
  \url{krishnak@utexas.edu}}
}
\date{}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
 \frametitle{Overview}
 \tableofcontents
\end{frame}

%----------------------------------------------------------------------------------------
% SLIDES
%----------------------------------------------------------------------------------------

\section{From ResNets to Continuous Dynamics}

%------------------------------------------------
\begin{frame}
\frametitle{Learning Objectives}

\begin{itemize}
    \item Understand the connection between ResNets and continuous dynamics
    \item Master the Neural ODE framework and adjoint method
    \item Implement ODENets for image classification
    \item Apply continuous normalizing flows for generative modeling
    \item Build latent ODE models for irregular time series
\end{itemize}

\vspace{1cm}
\centering
\href{https://colab.research.google.com/github/kks32-courses/sciml/blob/main/docs/07-neural-ode/neural-ode.ipynb}{\beamergotobutton{Open Notebook}}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The ResNet Formula}

A residual network transforms hidden states layer by layer:

\begin{equation}
h_{t+1} = h_t + f(h_t, \theta_t)
\end{equation}

where $t \in \{0, 1, \ldots, T\}$ indexes the layers.

\vspace{0.5cm}

\begin{block}{The Key Question}
What happens as we add more layers ($T \to \infty$) and take smaller steps?
\end{block}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Euler Connection}

The ResNet update is the \textbf{Euler discretization} of an ODE:

\begin{equation}
\frac{dh(t)}{dt} = f(h(t), t, \theta)
\end{equation}

\vspace{0.5cm}

\begin{alertblock}{Key Insight}
A ResNet with infinitely many infinitesimal layers $\equiv$ solving an ODE
\end{alertblock}

\vspace{0.5cm}

Instead of specifying discrete layers, we parameterize the \textbf{derivative} of the hidden state using a neural network.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{ResNet vs Neural ODE}

\begin{columns}[c]

\column{.5\textwidth}
\textbf{Residual Network}
\begin{itemize}
    \item Discrete transformations
    \item Fixed number of layers $T$
    \item $h_{t+1} = h_t + f(h_t)$
    \item Memory: $\mathcal{O}(T)$
\end{itemize}

\column{.5\textwidth}
\textbf{Neural ODE}
\begin{itemize}
    \item Continuous dynamics
    \item Adaptive depth
    \item $\frac{dh}{dt} = f(h(t), t)$
    \item Memory: $\mathcal{O}(1)$
\end{itemize}

\end{columns}

\vspace{0.5cm}

% Figure will be generated by running the notebook
% \begin{center}
% \includegraphics[width=0.8\textwidth]{figs/resnet_vs_ode.png}
% \end{center}

\end{frame}

%------------------------------------------------
\section{Why Neural ODEs?}

%------------------------------------------------
\begin{frame}
\frametitle{Three Key Advantages}

\begin{enumerate}
    \item \textbf{Memory Efficiency}: $\mathcal{O}(1)$ vs $\mathcal{O}(L)$
    \begin{itemize}
        \item Adjoint method recomputes forward pass during backprop
        \item Train arbitrarily deep networks with constant memory
    \end{itemize}

    \vspace{0.3cm}

    \item \textbf{Adaptive Computation}
    \begin{itemize}
        \item ODE solver adjusts \# function evaluations automatically
        \item Trade speed for accuracy via tolerance
    \end{itemize}

    \vspace{0.3cm}

    \item \textbf{Continuous Time}
    \begin{itemize}
        \item Natural for irregular time series
        \item No discretization artifacts
    \end{itemize}
\end{enumerate}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Memory Efficiency: The Adjoint Method}

\textbf{Standard Backpropagation:}
\begin{itemize}
    \item Store all intermediate layer activations
    \item Memory: $\mathcal{O}(L)$ where $L$ = number of layers
\end{itemize}

\vspace{0.5cm}

\textbf{Adjoint Method:}
\begin{itemize}
    \item Solve a second ODE backwards in time
    \item Recompute forward states during backward pass
    \item Memory: $\mathcal{O}(1)$ independent of depth
\end{itemize}

\vspace{0.3cm}

The adjoint state $a(t) = \frac{\partial L}{\partial h(t)}$ evolves as:

\begin{equation}
\frac{da(t)}{dt} = -a(t)^T \frac{\partial f(h(t), t, \theta)}{\partial h}
\end{equation}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Adjoint Method Visualized}

\begin{center}
\includegraphics[width=0.7\textwidth]{figs/neuralode.png}
\end{center}

\begin{itemize}
    \item \textbf{Forward}: Solve ODE from $t_0$ to $t_1$
    \item \textbf{Backward}: Solve augmented ODE from $t_1$ to $t_0$
    \item Automatically handled by \texttt{odeint\_adjoint}
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Neural ODE Architecture}

%------------------------------------------------
\begin{frame}
\frametitle{Basic Neural ODE Components}

\begin{block}{1. ODE Function $f(h, t, \theta)$}
Neural network that computes the derivative $\frac{dh}{dt}$
\end{block}

\begin{block}{2. ODE Solver}
Integrates $h(t)$ from $t_0$ to $t_1$:
\begin{equation}
h(t_1) = h(t_0) + \int_{t_0}^{t_1} f(h(t), t, \theta) dt
\end{equation}
\end{block}

\begin{block}{3. Adjoint Method}
Computes gradients $\frac{\partial L}{\partial \theta}$ efficiently
\end{block}

\end{frame}

%------------------------------------------------
\begin{frame}[fragile]
\frametitle{PyTorch Implementation}

\begin{block}{Using torchdiffeq}
\begin{verbatim}
from torchdiffeq import odeint_adjoint as odeint

class ODEFunc(nn.Module):
    def forward(self, t, h):
        return self.net(h)

class NeuralODE(nn.Module):
    def forward(self, h0, t):
        return odeint(self.odefunc, h0, t)
\end{verbatim}
\end{block}

Key parameters:
\begin{itemize}
    \item \texttt{method}: 'dopri5' (adaptive), 'euler', 'rk4', etc.
    \item \texttt{rtol}, \texttt{atol}: Tolerance for accuracy
\end{itemize}

\end{frame}



%------------------------------------------------
\begin{frame}
\frametitle{Hyperparameter Selection}

\begin{block}{ODE Solver Tolerance}
\begin{itemize}
    \item \texttt{rtol}, \texttt{atol}: Control accuracy
    \item Higher tolerance $\to$ faster but less accurate
    \item Typical: \texttt{rtol=1e-3}, \texttt{atol=1e-4}
\end{itemize}
\end{block}

\begin{block}{Solver Method}
\begin{itemize}
    \item \textbf{Adaptive}: 'dopri5', 'adams' (recommended)
    \item \textbf{Fixed-step}: 'euler', 'rk4' (for debugging)
\end{itemize}
\end{block}

\begin{block}{Integration Time}
\begin{itemize}
    \item Usually $T = 1.0$ (can be learned)
    \item Longer $T$ $\to$ more expressive but slower
\end{itemize}
\end{block}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Extensions}

\begin{block}{Augmented Neural ODEs}
Add extra dimensions to avoid topological constraints
\end{block}

\begin{block}{Second-order Neural ODEs}
Include acceleration: $\frac{d^2h}{dt^2} = f(h, \frac{dh}{dt}, t)$
\end{block}

\begin{block}{Stochastic Differential Equations (SDEs)}
Add noise for uncertainty: $dh = f(h, t)dt + g(h, t)dW$
\end{block}

\begin{block}{Hamiltonian Neural Networks}
Preserve energy and symplectic structure
\end{block}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Summary}

\begin{block}{Key Takeaways}
\begin{enumerate}
    \item Neural ODEs = continuous-depth neural networks
    \item ResNets $\to$ ODEs via Euler discretization
    \item Adjoint method enables $\mathcal{O}(1)$ memory training
    \item Applications: classification, generative models, time series
\end{enumerate}
\end{block}

\vspace{0.5cm}

\begin{block}{The Big Idea}
Parameterize the \textbf{derivative} of hidden states, not the states themselves
\end{block}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{References}

\begin{thebibliography}{99}

\bibitem{chen2018} Chen, R. T. Q., Rubanova, Y., Bettencourt, J., \& Duvenaud, D. (2018).
\newblock Neural Ordinary Differential Equations.
\newblock \textit{NeurIPS 2018} (Best Paper Award).

\bibitem{grathwohl2019} Grathwohl, W., Chen, R. T. Q., Bettencourt, J., Sutskever, I., \& Duvenaud, D. (2019).
\newblock FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models.
\newblock \textit{ICLR 2019}.

\bibitem{rubanova2019} Rubanova, Y., Chen, R. T. Q., \& Duvenaud, D. (2019).
\newblock Latent ODEs for Irregularly-Sampled Time Series.
\newblock \textit{NeurIPS 2019}.

\end{thebibliography}

\end{frame}


%----------------------------------------------------------------------------------------

\end{document}
