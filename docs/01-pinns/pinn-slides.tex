%**************************************************************************************
% License:
% CC BY-NC-SA 4.0 (http://creativecommons.org/licenses/by-nc-sa/4.0/)
%**************************************************************************************

\documentclass[aspectratio=169]{beamer}

\mode<presentation> {
	\usetheme{Madrid}
	
	% Burnt orange
	\definecolor{burntorange}{rgb}{0.8, 0.33, 0.0}
	\colorlet{beamer@blendedblue}{burntorange}
	% Pale yellow
	\definecolor{paleyellow}{rgb}{1.0, 1.0, 0.953}
	\setbeamercolor{background canvas}{bg=paleyellow}
	% Secondary and tertiary palette
	\setbeamercolor*{palette secondary}{use=structure,fg=white,bg=burntorange!80!black}
	\setbeamercolor*{palette tertiary}{use=structure,fg=white,bg=burntorange!60!black}
	
	% Remove navigation symbols
	\setbeamertemplate{navigation symbols}{}
}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[labelsep=space,tableposition=top]{caption}
\renewcommand{\figurename}{Fig.} 
\usepackage{caption,subcaption}
\usepackage{xcolor}
\usepackage{transparent}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usetikzlibrary{shapes,arrows,positioning,calc}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}

% Define math operators
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\ReLU}{ReLU}
\DeclareMathOperator{\sign}{sign}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\tiny\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{green!70!black},
    showstringspaces=false,
    frame=single,
    breaklines=true,
    numbers=left,
    numberstyle=\tiny
}

\AtBeginSection[]{
	\begin{frame}{Outline}
		\tableofcontents[
		currentsection,      % highlight the current section
		hideallsubsections   % show only section titles
		]
	\end{frame}
}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------
\title[Physics-Informed Neural Networks]{Physics-Informed Neural Networks (PINNs)} 
\author{Krishna Kumar} % name
\institute[UT Austin] % institution 
{
University of Texas at Austin \\
\medskip
\textit{
  \url{krishnak@utexas.edu}} % Your email address
}
\date{} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % title page as the first slide
\end{frame}

\begin{frame}
 \frametitle{Overview}
 \tableofcontents
\end{frame}

%----------------------------------------------------------------------------------------
% slides
%----------------------------------------------------------------------------------------

\section{The PINN Concept: Beyond Data-Only}

%------------------------------------------------
\begin{frame}
\frametitle{Learning Objectives}

\begin{itemize}
    \item Understand why standard neural networks fail for physics problems
    \item Learn how to incorporate physics into neural network training
    \item Master automatic differentiation for computing derivatives
    \item Compare data-driven vs physics-informed approaches
\end{itemize}

\vspace{1cm}
\centering
\href{https://colab.research.google.com/github/kks32-courses/sciml/blob/main/docs/01-pinns/pinn.ipynb}{\beamergotobutton{Open Notebook: PINN}}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Problem: A Damped Harmonic Oscillator}

A mass $m$ on a spring (constant $k$) with damping (coefficient $c$). The displacement $u(t)$ satisfies:
\begin{equation*}
m \frac{d^2 u}{dt^2} + c \frac{du}{dt} + ku = 0
\end{equation*}
with initial conditions: $u(0) = 1$, $\frac{du}{dt}(0) = 0$.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{figs/harmonic-oscillator.png}
	\caption*{A classic physics problem to illustrate PINNs.}
\end{figure}

\textbf{The Challenge:} Reconstruct the full solution $u(t)$ from a few sparse, noisy data points.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Stage 1: The Data-Only Approach}

\textbf{Idea:} Train a standard neural network to fit the sparse data.

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Loss Function:} Mean Squared Error
        \begin{equation*}
        \mathcal{L}_{\text{data}}(\theta) = \frac{1}{N} \sum_{i=1}^N |\hat{u}_\theta(t_i) - u_i|^2
        \end{equation*}
        
        \textbf{Architecture:}
        \begin{itemize}
            \item Input: Time $t$
            \item Hidden Layers: Tanh activations
            \item Output: Displacement $\hat{u}_\theta(t)$
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\linewidth]{figs/oscillator-nn.png}
            \caption*{Standard NN for function fitting.}
        \end{figure}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Failure of the Data-Only Approach}

\textbf{Result:} The network fits the training points but fails catastrophically between them.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/oscillator-result-nn.png}
	\caption*{The network overfits to the sparse data and does not respect the underlying physics.}
\end{figure}

\textbf{Conclusion:} The network has no knowledge of the governing laws of physics, leading to unphysical oscillations and poor generalization.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Stage 2: Enter Physics-Informed Neural Networks}

\textbf{The Key Insight:} Don't just fit data. Enforce the differential equation itself!

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Physics Residual:}
        We define a residual based on the ODE:
        \begin{equation*}
        \mathcal{R}_\theta(t) = m\frac{d^2\hat{u}_\theta}{dt^2} + c\frac{d\hat{u}_\theta}{dt} + k\hat{u}_\theta
        \end{equation*}
        If the solution is correct, $\mathcal{R}_\theta(t)$ should be zero.
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\linewidth]{figs/oscillator-pinn-nn.png}
            \caption*{PINN architecture with physics loss.}
        \end{figure}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Complete PINN Loss Function}

The total loss is a combination of data fit and physics enforcement.
\begin{equation*}
\mathcal{L}_{\text{total}}(\theta) = \mathcal{L}_{\text{data}}(\theta) + \lambda \mathcal{L}_{\text{physics}}(\theta)
\end{equation*}

\begin{block}{Data Loss}
Ensures the solution passes through the measurements.
\begin{equation*}
\mathcal{L}_{\text{data}}(\theta) = \frac{1}{N_{\text{data}}} \sum_{i=1}^{N_{\text{data}}} |\hat{u}_\theta(t_i) - u_i|^2
\end{equation*}
\end{block}

\begin{block}{Physics Loss}
Ensures the solution obeys the ODE at random "collocation" points.
\begin{equation*}
\mathcal{L}_{\text{physics}}(\theta) = \frac{1}{N_{\text{colloc}}} \sum_{j=1}^{N_{\text{colloc}}} |\mathcal{R}_\theta(t_j)|^2
\end{equation*}
\end{block}

The hyperparameter $\lambda$ balances the two terms.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Secret Weapon: Automatic Differentiation (AD)}

\textbf{Critical Question:} How do we compute $\frac{d\hat{u}_\theta}{dt}$ and $\frac{d^2\hat{u}_\theta}{dt^2}$?

\begin{alertblock}{Answer: Automatic Differentiation}
AD provides \textbf{exact} derivatives of the neural network output with respect to its input, by applying the chain rule through the computational graph.
\begin{itemize}
    \item No finite difference errors.
    \item Computationally efficient (especially reverse-mode AD).
    \item Built into modern frameworks (PyTorch, TensorFlow, JAX).
\end{itemize}
\end{alertblock}

\textbf{Example:} For $u(t) = \sin(t)$, AD can compute $u'(t)=\cos(t)$ and $u''(t)=-\sin(t)$ to machine precision.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Theoretical Foundation: UAT for Sobolev Spaces}

\textbf{Classical UAT:} NNs can approximate any \textit{continuous function}.

\textbf{Problem:} For PDEs, we need to approximate functions \textbf{and their derivatives}.

\begin{block}{Extended Universal Approximation Theorem}
Neural networks with sufficiently smooth activation functions (e.g., $\tanh$, not ReLU) can approximate functions in \textbf{Sobolev spaces} $H^k(\Omega)$.
\begin{equation*}
\|u - \hat{u}_\theta\|_{H^k} < \epsilon
\end{equation*}
The Sobolev norm $\|u\|_{H^k}^2 = \sum_{|\alpha| \leq k} \|D^\alpha u\|_{L^2}^2$ measures the error in the function and all its derivatives up to order $k$.
\end{block}

\textbf{Why this matters:} For a $k^{th}$-order ODE/PDE, we need an activation function that is at least $k$ times differentiable ($C^k$). For our 2nd-order oscillator, we need a $C^2$ activation like $\tanh$.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Moment of Truth: Standard NN vs. PINN}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{figs/pinn-nn-comparison.png}
	\caption*{Placeholder for the comparison plot from the notebook.}
\end{figure}

\textbf{Observation:}
\begin{itemize}
    \item \textbf{Standard NN:} Fits data points, but fails to generalize.
    \item \textbf{PINN:} Fits data points AND follows the physics, resulting in a globally accurate solution.
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Deep Dive: Derivative and Phase Portrait Analysis}

The ultimate test: Does the PINN learn physically consistent derivatives?

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{figs/derivative-comparison.png}
	\caption*{Derivative  plots.}
\end{figure}

\textbf{Result:} The PINN learns the correct velocity ($du/dt$) and acceleration ($d^2u/dt^2$).

\end{frame}

%------------------------------------------------
\begin{frame}
	\frametitle{Deep Dive: Derivative and Phase Portrait Analysis}
	
	The ultimate test: Does the PINN learn physically consistent derivatives?
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=\textwidth]{figs/oscillator-phase-portrait.png}
		\caption*{Phase portrait plots.}
	\end{figure}
	
	\textbf{Result:} The phase portrait (velocity vs. displacement) traces the correct physical trajectory (a spiral for a damped oscillator).
	
\end{frame}

%------------------------------------------------
\section{Enforcing Boundary Conditions}

%------------------------------------------------
\begin{frame}
\frametitle{The 1D Poisson Problem}

We now tackle a boundary value problem, the 1D Poisson equation:
\begin{equation*}
\frac{d^2u}{dx^2} + \pi \sin(\pi x) = 0, \quad \text{for } x \in [0, 1]
\end{equation*}
with Dirichlet boundary conditions (BCs):
\begin{equation*}
u(0) = 0 \quad \text{and} \quad u(1) = 0
\end{equation*}

\textbf{Goal:} Train a PINN to find the solution using only the governing equation and its BCs.

\vspace{1cm}
\centering
\href{https://colab.research.google.com/github/kks32-courses/sciml/blob/main/docs/01-pinns/poisson.ipynb}{\beamergotobutton{Open Notebook: 1D Poisson}}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Method 1: Soft Constraints}

Treat the boundary conditions as another component of the loss function.

\textbf{Total Loss:}
\begin{equation*}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{PDE}} + \lambda_{BC} \mathcal{L}_{BC}
\end{equation*}

\begin{block}{Boundary Loss}
A mean squared error term that penalizes violations of the BCs.
\begin{equation*}
\mathcal{L}_{BC} = \frac{1}{N_{BC}} \sum_{i=1}^{N_{BC}} |\hat{u}_\theta(x_i) - u_{BC}|^2
\end{equation*}
\end{block}

\begin{alertblock}{Pros \& Cons}
\begin{itemize}
    \item[+]\textbf{Flexible:} Easy to implement for any type of BC (Dirichlet, Neumann, etc.).
    \item[-]\textbf{Approximate:} Satisfaction is not guaranteed, only encouraged.
    \item[-]\textbf{Tuning:} Requires careful tuning of the weight $\lambda_{BC}$.
\end{itemize}
\end{alertblock}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Method 2: Hard Constraints}

Modify the network architecture to satisfy the BCs \textit{by construction}.

For our problem with $u(0)=0$ and $u(1)=0$, we can define a trial solution $\tilde{u}(x)$:
\begin{equation*}
\tilde{u}(x) = \underbrace{x(1-x)}_{D(x)} \cdot \underbrace{\text{NN}(x; \theta)}_{\text{Network Output}}
\end{equation*}
The distance function $D(x)$ is zero at the boundaries, forcing $\tilde{u}(x)$ to be zero there, regardless of the network's output.

\begin{alertblock}{Pros \& Cons}
\begin{itemize}
    \item[+]\textbf{Exact:} BCs are satisfied perfectly.
    \item[+]\textbf{Simpler Loss:} No need for $\mathcal{L}_{BC}$ or $\lambda_{BC}$, leading to more stable training.
    \item[-]\textbf{Inflexible:} Requires designing a specific trial function for the problem's geometry and BCs, which can be difficult for complex cases.
\end{itemize}
\end{alertblock}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Comparison: Soft vs. Hard Constraints}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/poisson-soft-vs-hard.png}
\end{figure}

\textbf{Conclusion:}
\begin{itemize}
    \item Both methods achieve high accuracy.
    \item The hard constraint method shows slightly lower error and, by design, has zero error at the boundaries.
    \item For simple geometries and Dirichlet BCs, \textbf{hard constraints are often superior}.
    \item For complex problems, \textbf{soft constraints offer greater versatility}.
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Inverse Problems: Discovering Physics}

%------------------------------------------------
\begin{frame}
\frametitle{Forward vs. Inverse Problems}

\begin{block}{Forward Problem}
\begin{itemize}
    \item \textbf{Given:} Full physical model (equations + parameters).
    \item \textbf{Find:} The solution $u(x,t)$.
    \item \textit{Example: Simulate temperature given thermal conductivity.}
\end{itemize}
\end{block}

\begin{block}{Inverse Problem}
\begin{itemize}
    \item \textbf{Given:} Sparse measurements of the solution $u(x,t)$.
    \item \textbf{Find:} Unknown physical parameters in the model.
    \item \textit{Example: Infer thermal conductivity from temperature measurements.}
\end{itemize}
\end{block}

\vspace{1em}
\centering
\href{https://colab.research.google.com/github/kks32-courses/sciml/blob/main/docs/01-pinns/inverse-heat.ipynb}{\beamergotobutton{Open Notebook: Inverse Heat}}


\end{frame}

%------------------------------------------------
\begin{frame}
	\frametitle{Forward vs. Inverse Problems}
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.55\textwidth]{figs/inverse-heat-diffusivity.png}
	\end{figure}
	
\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The PINN Approach to Inverse Problems}

\textbf{The Key Insight:} Treat the unknown physical parameters as additional trainable variables in the network.

\textbf{Problem:} 1D steady-state heat conduction.
\begin{equation*}
-k \frac{d^2T}{dx^2} = f(x)
\end{equation*}
Here, the thermal diffusivity $k$ is \textbf{unknown}.

\textbf{PINN Framework:}
\begin{itemize}
    \item The neural network learns the temperature field: $\hat{T}_\theta(x)$.
    \item A new trainable parameter is introduced: $\hat{k}$.
    \item The optimizer updates both the network weights $\theta$ and the parameter $\hat{k}$ simultaneously.
\end{itemize}

\textbf{Loss Function:} $\mathcal{L}(\theta, \hat{k}) = \mathcal{L}_{\text{data}} + \mathcal{L}_{\text{PDE}} + \mathcal{L}_{\text{BC}}$
where the physics loss now includes the trainable parameter $\hat{k}$:
\begin{equation*}
\mathcal{L}_{\text{PDE}} = \frac{1}{N_f}\sum \left|-\hat{k}\frac{d^2\hat{T}_\theta}{dx^2}(x_j) - f(x_j)\right|^2
\end{equation*}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Implementation: Parameter Estimation}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/inverse-problem-setup.png}
	\caption*{"Experimental Setup" plot, showing the true solution and sparse, noisy data points.}
\end{figure}

\textbf{Challenge:} Can the PINN recover the true value of $k$ from only 10 noisy measurements?

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Results: Parameter Recovery}

\begin{minipage}[t]{0.48\textwidth}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{figs/inverse-heat-diffusivity.png}
	\caption*{Parameter convergence and temperature field reconstruction.}
\end{figure}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\begin{itemize}
    \item The estimated parameter $\hat{k}$ converges to the true value.
    \item The PINN simultaneously reconstructs the full, continuous temperature field accurately.
    \item This is achieved from very sparse and noisy data, showcasing the regularizing effect of the physics loss.
\end{itemize}
\end{minipage}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Summary: Why PINNs are Powerful}

\textbf{1. Regularization Effect:}
\begin{itemize}
    \item Physics constraints prevent overfitting and guide the solution in data-sparse regions.
\end{itemize}

\textbf{2. Data Efficiency:}
\begin{itemize}
    \item Physics provides a strong inductive bias, allowing PINNs to learn from very few measurements.
\end{itemize}

\textbf{3. Accurate Derivatives:}
\begin{itemize}
    \item Automatic differentiation provides exact derivatives, which are learned correctly as a consequence of enforcing the physics.
\end{itemize}

\textbf{4. Versatility:}
\begin{itemize}
    \item The same framework can solve forward problems, inverse problems, and handle various boundary conditions.
\end{itemize}

\begin{center}
\textbf{PINNs = Universal Function Approx. + Physics Constraints + Auto. Diff.}
\end{center}

\end{frame}

%------------------------------------------------
\section{Collocation Point Strategies}

%------------------------------------------------
\begin{frame}
\frametitle{Collocation Points: Where to Enforce Physics}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{What are collocation points?}
        \begin{itemize}
            \item Points where we evaluate PDE residual
            \item Do not need measurement data
            \item Distributed throughout domain
            \item More points → better physics enforcement
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Key question:}
        How should we distribute these points for optimal performance?
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figs/sampling.png}
            \caption*{Different collocation sampling strategies}
        \end{figure}
    \end{column}
\end{columns}

\end{frame}



%------------------------------------------------
\begin{frame}
	\frametitle{Collocation Sampling Strategies}
\begin{columns}[T]
    \begin{column}{0.33\textwidth}
        \textbf{Uniform Grid:}
        \begin{itemize}
            \item Simple to implement
            \item Good coverage
            \item Curse of dimensionality
        \end{itemize}
    \end{column}
    \begin{column}{0.33\textwidth}
        \textbf{Random (Monte Carlo):}
        \begin{itemize}
            \item Dimension-independent
            \item May cluster/leave gaps
            \item Easy to add points
        \end{itemize}
    \end{column}
    \begin{column}{0.34\textwidth}
        \textbf{Quasi-Random:}
        \begin{itemize}
            \item Better coverage than random
            \item Low discrepancy sequences
            \item Hammersley, Sobol, Halton
        \end{itemize}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
	\frametitle{Collocation Sampling Strategies}
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.85\textwidth]{figs/collocation-sampling-comparison.png}
		\caption*{Comparison of different sampling strategies for collocation points}
	\end{figure}
\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Impact of Collocation Strategy}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figs/collocation-error-grid.png}
            \caption*{Error with uniform grid sampling}
        \end{figure}
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figs/collocation-error-hammersley.png}
            \caption*{Error with Hammersley sampling}
        \end{figure}
    \end{column}
\end{columns}

\vspace{0.3cm}

\textbf{Observations:}
\begin{itemize}
    \item Quasi-random sampling often outperforms uniform/random
    \item Better space-filling properties lead to lower errors
    \item Particularly important in higher dimensions
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Adaptive Weights and Loss Balancing}

%------------------------------------------------
\begin{frame}
\frametitle{The Loss Balancing Challenge}

\begin{block}{The Problem}
Different loss terms can have vastly different magnitudes and gradients
\end{block}

\textbf{Total PINN loss:}
\begin{equation*}
\mathcal{L} = \mathcal{L}_{\text{data}} + \lambda_{\text{PDE}} \mathcal{L}_{\text{PDE}} + \lambda_{\text{BC}} \mathcal{L}_{\text{BC}} + \lambda_{\text{IC}} \mathcal{L}_{\text{IC}}
\end{equation*}

\vspace{0.5cm}

\textbf{Challenges:}
\begin{itemize}
    \item Data loss: Often $O(10^{-3})$ to $O(10^{-1})$
    \item PDE residual: Can be $O(10^2)$ to $O(10^4)$ initially
    \item Boundary conditions: Variable scale
    \item Poor balance → training instability or failure
\end{itemize}

\vspace{0.5cm}

\textbf{Question:} How to choose $\lambda$ values optimally?

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Gradient-Based Adaptive Weighting}

\textbf{Key idea:} Balance the gradients of different loss terms

\vspace{0.5cm}

\textbf{Algorithm (Wang et al., 2021):}
\begin{enumerate}
    \item Compute gradient statistics:
    $$\bar{g}_i = \frac{1}{|\theta|}\sum_{\theta} |\nabla_\theta \mathcal{L}_i|$$
    
    \item Update weights to balance gradients:
    $$\lambda_i^{(k+1)} = \lambda_i^{(k)} \cdot \left(\frac{\max_j \bar{g}_j}{\bar{g}_i}\right)^\alpha$$
    
    \item Apply exponential moving average for stability
\end{enumerate}

\vspace{0.5cm}

\textbf{Benefits:}
\begin{itemize}
    \item Prevents gradient imbalance
    \item Improves convergence speed
    \item Reduces manual tuning
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Adaptive Weights in Action}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/adaptive-weights-results.png}
    \caption*{Comparison of fixed vs adaptive weighting on a 2D Poisson problem}
\end{figure}

\textbf{Results:}
\begin{itemize}
    \item Adaptive weights: 10× faster convergence
    \item Better final accuracy
    \item More stable training
    \item Automatic adjustment to problem characteristics
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Advanced Application: Burgers Equation}

%------------------------------------------------
\begin{frame}
\frametitle{The Burgers Equation: A Nonlinear PDE Challenge}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Viscous Burgers equation:}
        $$\frac{\partial u}{\partial t} + u\frac{\partial u}{\partial x} = \nu\frac{\partial^2 u}{\partial x^2}$$
        
        \textbf{Domain:} $(x,t) \in [-1,1] \times [0,1]$
        
        \textbf{Initial condition:}
        $$u(x,0) = -\sin(\pi x)$$
        
        \textbf{Boundary conditions:}
        $$u(-1,t) = u(1,t) = 0$$
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figs/burgers-solution.png}
            \caption*{Burgers equation solution showing shock formation}
        \end{figure}
    \end{column}
\end{columns}

\vspace{0.3cm}
\textbf{Challenges:} Nonlinearity, shock formation, multi-scale features

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{PINN for Burgers Equation}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Network architecture:}
        \begin{itemize}
            \item Input: $(x, t)$
            \item Output: $u(x, t)$
            \item Hidden layers: 8 × 20 neurons
            \item Activation: Tanh
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Loss function:}
        \begin{align*}
        \mathcal{L} = &\mathcal{L}_{\text{PDE}} + \mathcal{L}_{\text{IC}} + \mathcal{L}_{\text{BC}} \\
        \mathcal{L}_{\text{PDE}} = &\frac{1}{N_f}\sum \left|\frac{\partial u}{\partial t} + u\frac{\partial u}{\partial x} - \nu\frac{\partial^2 u}{\partial x^2}\right|^2
        \end{align*}
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figs/collocation-burgers.png}
            \caption*{Collocation points for Burgers equation}
        \end{figure}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Burgers Equation: Forward Problem Results}

\begin{columns}[t]
	\column{0.65\textwidth}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/burgers-prediction.png}
    \caption*{PINN prediction vs exact solution for Burgers equation}
\end{figure}

	\column{0.35\textwidth}
\textbf{Performance metrics:}
\begin{itemize}
    \item Relative $L^2$ error: $< 1\%$
    \item Training time: ~5 minutes on GPU
    \item No mesh required
    \item Captures shock accurately
\end{itemize}
	
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Inverse Problem: Parameter Discovery in Burgers}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Problem setup:}
        \begin{itemize}
            \item Unknown viscosity $\nu$
            \item Sparse measurements of $u(x,t)$
            \item Goal: Recover $\nu$ and full solution
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Modified loss:}
        $$\mathcal{L} = \mathcal{L}_{\text{data}} + \lambda\mathcal{L}_{\text{PDE}}(\nu)$$
        
        where $\nu$ is a trainable parameter
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figs/burgers-inverse-prediction.png}
            \caption*{Inverse problem: recovering solution from sparse data}
        \end{figure}
    \end{column}
\end{columns}

\vspace{0.5cm}

\textbf{Results:}
\begin{itemize}
    \item True $\nu = 0.01$, Predicted $\nu = 0.00998$
    \item Error in parameter: $< 0.2\%$
    \item From only 2000 scattered measurements!
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Discrete-Time PINNs}

%------------------------------------------------
\begin{frame}
\frametitle{Why Discrete-Time PINNs?}

\textbf{Limitations of Continuous-Time Approach:}
\begin{itemize}
    \item \textbf{Computational Cost:} $O(N_x \times N_t)$ collocation points
    \item \textbf{Memory Issues:} Storing entire space-time solution
    \item \textbf{Training Difficulty:} Long-time behavior hard to capture
    \item \textbf{Stiff Problems:} Need very dense time sampling
\end{itemize}

\vspace{0.5cm}

\textbf{Example:} For Burgers equation on $[-1,1] \times [0,10]$:
\begin{itemize}
    \item Continuous: 100 × 1000 = 100,000 collocation points
    \item Discrete: 100 spatial points per time step
\end{itemize}

\vspace{0.5cm}

\begin{alertblock}{Solution}
Use time-stepping methods within PINN framework to evolve solution sequentially
\end{alertblock}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Continuous vs Discrete Time Approaches}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Continuous-Time PINNs:}
        \begin{itemize}
            \item Treat time as another input: $(x,t) \rightarrow u(x,t)$
            \item Learn entire space-time solution
            \item Many collocation points in time
            \item Can become expensive for long simulations
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \textbf{Discrete-Time PINNs:}
        \begin{itemize}
            \item Leverage classical time-stepping schemes
            \item Learn mapping from $t^n \rightarrow t^{n+1}$
            \item Only spatial collocation needed
            \item More efficient for long time simulations
        \end{itemize}
    \end{column}
\end{columns}

\vspace{0.5cm}
\textbf{Key Idea:} Integrate Runge-Kutta methods within the PINN framework

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Runge-Kutta Time-Stepping Foundation}

For time-dependent PDEs in semi-discrete form:
$$\frac{du}{dt} = \mathcal{N}[u](t)$$

A $q$-stage Runge-Kutta method computes:
\begin{align*}
U^i &= u^n + \Delta t \sum_{j=1}^q a_{ij} \mathcal{N}[U^j], \quad i = 1, \ldots, q \\
u^{n+1} &= u^n + \Delta t \sum_{j=1}^q b_j \mathcal{N}[U^j]
\end{align*}

Where:
\begin{itemize}
    \item $U^i$: intermediate stage values
    \item $\{a_{ij}\}, \{b_j\}, \{c_j\}$: RK coefficients (Butcher tableau)
    \item Implicit if $a_{ij} \neq 0$ for $j \geq i$
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Discrete-Time PINN Architecture}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/burgers-discrete-time.png}
    \caption*{Network outputs $q$ intermediate stages and final solution at $t^{n+1}$}
\end{figure}

Network output: $\mathcal{NN}(x; \theta) = [U^1_{NN}, \ldots, U^q_{NN}, u^{n+1}_{NN}]^T$

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Training Discrete-Time PINNs}

\textbf{Loss Function:} Enforce consistency with RK scheme

The RK equations provide multiple estimates of $u^n(x)$:
\begin{align*}
u^n(x) &\approx U^i_{NN}(x) - \Delta t \sum_{j=1}^q a_{ij} \mathcal{N}[U^j_{NN}](x), \quad i = 1, \ldots, q \\
u^n(x) &\approx u^{n+1}_{NN}(x) - \Delta t \sum_{j=1}^q b_j \mathcal{N}[U^j_{NN}](x)
\end{align*}

\textbf{Total Loss:}
$$\mathcal{L}(\theta) = \frac{1}{N_n(q+1)} \sum_{i=1}^{q+1} \sum_{k=1}^{N_n} |u^n_{i,NN}(x_k) - u^n(x_k)|^2$$

\begin{itemize}
    \item All estimates should equal the known $u^n(x)$
    \item Minimizing discrepancy enforces the physics
    \item Spatial derivatives computed via AD
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Algorithm: Single-Step Discrete-Time PINN}

\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE \textbf{Input:} Solution $u^n(x)$ at time $t^n$, time step $\Delta t$
\STATE \textbf{Input:} RK coefficients $\{a_{ij}\}, \{b_j\}$ for $q$ stages
\STATE Initialize network $\mathcal{NN}(x;\theta): x \rightarrow [U^1, \ldots, U^q, u^{n+1}]$
\WHILE{not converged}
    \STATE Sample spatial points $\{x_k\}_{k=1}^{N_n}$ and- $[U^1_{NN}, \ldots, U^q_{NN}, u^{n+1}_{NN}] \leftarrow \mathcal{NN}(x_k;\theta)$
    \FOR{$j = 1$ to $q$}
        \STATE Compute $\mathcal{N}[U^j_{NN}] = -U^j_{NN}\frac{\partial U^j_{NN}}{\partial x} + \nu\frac{\partial^2 U^j_{NN}}{\partial x^2}$
    \ENDFOR
    \FOR{$i = 1$ to $q$}
        \STATE $u^n_{i,est} = U^i_{NN} - \Delta t \sum_{j} a_{ij}\mathcal{N}[U^j_{NN}]$
    \ENDFOR
    \STATE $u^n_{q+1,est} = u^{n+1}_{NN} - \Delta t \sum_{j} b_j\mathcal{N}[U^j_{NN}]$
 and $\mathcal{L} = \frac{1}{(q+1)N_n}\sum_{i,k} |u^n_{i,est}(x_k) - u^n(x_k)|^2$
    \STATE Update $\theta$ using gradient descent on $\mathcal{L}$
\ENDWHILE
\STATE \textbf{Output:} $u^{n+1}(x) = u^{n+1}_{NN}(x; \theta^*)$
\end{algorithmic}
\end{algorithm}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Algorithm: Adaptive Time-Stepping}

\begin{algorithm}[H]
\caption{Adaptive Discrete-Time PINN}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Initial $u^0(x)$, target time $T$, tolerance $\epsilon$
\STATE Initialize $t = 0$, $\Delta t = \Delta t_0$
\WHILE{$t < T$}
    \STATE Train PINN for step $t \rightarrow t + \Delta t$ (Algorithm 1)
    \STATE Compute error estimate $E$ using embedded RK method:
    \STATE \quad $E = \|u^{n+1}_{high} - u^{n+1}_{low}\|_2$
    \IF{$E < \epsilon$}
        \STATE Accept step: $u^{n+1} = u^{n+1}_{high}$;  $t = t + \Delta t$
        \STATE Adjust: $\Delta t_{new} = 0.9\Delta t(\epsilon/E)^{1/p}$
    \ELSE
        \STATE Reject step, retrain with smaller $\Delta t$ and update $\Delta t = 0.5\Delta t$
    \ENDIF
    \STATE $\Delta t = \min(\Delta t_{new}, T-t)$
\ENDWHILE
\STATE \textbf{Output:} Solution trajectory $\{u^n\}$
\end{algorithmic}
\end{algorithm}

\textbf{Benefits:} Automatic step size control, error estimation, efficiency

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Discrete-Time Results: Burgers Equation}

\begin{columns}
	\column{0.65\textwidth}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/burgers-discrete-time-prediction.pdf}
    \caption*{Discrete-time PINN prediction for Burgers equation using implicit Runge-Kutta}
\end{figure}
\column{0.35\textwidth}
\textbf{Key Results:}
\begin{itemize}
    \item High accuracy with large time steps
    \item Stable evolution over long times
    \item Captures shock formation accurately
\end{itemize}
	
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Advantages of Discrete-Time PINNs}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Computational Efficiency:}
        \begin{itemize}
            \item Loss evaluated only over spatial domain
            \item No time-dimension collocation points
            \item Drastically reduced training points
        \end{itemize}

        \vspace{0.5cm}

        \textbf{Stability:}
        \begin{itemize}
            \item Implicit RK schemes (e.g., Gauss-Legendre)
            \item Stable for large time steps
            \item Essential for stiff problems
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \textbf{Accuracy Control:}
        \begin{itemize}
            \item High-order temporal accuracy via stages
            \item Adding stages only increases output layer
            \item Less costly than deeper networks
        \end{itemize}

        \vspace{0.5cm}

        \textbf{Leverages Numerical Analysis:}
        \begin{itemize}
            \item Decades of research on time-stepping
            \item Proven stability properties
            \item Well-understood error bounds
        \end{itemize}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\section{Variational Formulation and Energy Minimization}

%------------------------------------------------
\begin{frame}
\frametitle{From Strong Form to Energy Minimization}

Many physical systems are governed by variational principles:

\begin{block}{Principle of Minimum Potential Energy}
The true solution $u(\mathbf{x})$ minimizes the total potential energy functional $\Pi(u)$
\end{block}

\textbf{Total Potential Energy:}
$$\Pi(u) = \underbrace{\int_\Omega \Psi(\epsilon(u)) d\Omega}_{\text{Internal Energy}} - \underbrace{\int_\Omega f \cdot u \, d\Omega}_{\text{Body Forces}} - \underbrace{\int_{\Gamma_N} t \cdot u \, d\Gamma}_{\text{Surface Tractions}}$$

\textbf{Deep Ritz Method:} Train PINN by minimizing energy instead of PDE residual

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Deriving the Energy Formulation}

Starting from the strong form (equilibrium equations):
$$\nabla \cdot \sigma + f = 0 \text{ in } \Omega$$

\textbf{Step 1:} Multiply by test function $v$ and integrate:
$$\int_\Omega (\nabla \cdot \sigma) \cdot v \, d\Omega + \int_\Omega f \cdot v \, d\Omega = 0$$

\textbf{Step 2:} Apply integration by parts:
$$-\int_\Omega \sigma : \nabla v \, d\Omega + \int_{\Gamma} (\sigma \cdot n) \cdot v \, d\Gamma + \int_\Omega f \cdot v \, d\Omega = 0$$

\textbf{Step 3:} For $v = \delta u$ (virtual displacement), this becomes:
$$\delta \Pi = 0 \quad \Rightarrow \quad \text{Stationarity of } \Pi(u)$$

\end{frame}

%------------------------------------------------
\begin{frame}[fragile]
\frametitle{Implementation: Strain Energy Density}

For linear elasticity with Lamé parameters $\lambda, \mu$:

\textbf{Strain energy density:}
$$\Psi(\epsilon) = \frac{\lambda}{2}(\text{tr}(\epsilon))^2 + \mu \, \text{tr}(\epsilon^2)$$

Expanding:
$$\Psi = \frac{\lambda}{2}(\epsilon_{xx} + \epsilon_{yy})^2 + \mu(\epsilon_{xx}^2 + \epsilon_{yy}^2 + 2\epsilon_{xy}^2)$$

\begin{lstlisting}
def strain_energy_density(epsilon_xx, epsilon_yy, epsilon_xy):
    # First term: lambda*(trace)^2
    psi_1 = 0.5 * lambda_ * (epsilon_xx + epsilon_yy)**2

    # Second term: mu*tr(epsilon^2)
    psi_2 = mu * (epsilon_xx**2 + epsilon_yy**2 + 2*epsilon_xy**2)

    return psi_1 + psi_2
\end{lstlisting}

\end{frame}

%------------------------------------------------
\begin{frame}[fragile]
\frametitle{Computing Total Potential Energy}

\begin{lstlisting}
def potential_energy(x_colloc, y_colloc, x_bound, y_bound):
    # Compute strains at collocation points
    epsilon_xx, epsilon_yy, epsilon_xy = strain(x_colloc, y_colloc)

    # Strain energy density
    psi = strain_energy_density(epsilon_xx, epsilon_yy, epsilon_xy)

    # Numerical integration over domain
    dx = L / (Nx - 1)
    dy = W / (Ny - 1)
    internal_energy = (psi * dx * dy).sum()

    # External work on boundary
    u_bound = net_u(x_bound, y_bound)
    v_bound = net_v(x_bound, y_bound)
    t_x = traction_x(y_bound)
    t_y = traction_y(y_bound)

    external_work = ((t_x * u_bound + t_y * v_bound) * dy).sum()

    # Total potential energy
    return internal_energy - external_work
\end{lstlisting}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Example: 2D Cantilever Beam}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figs/2d-cantilever.pdf}
            \caption*{Cantilever beam: fixed at $x=0$, loaded at $x=L$}
        \end{figure}
    \end{column}
    \begin{column}{0.5\textwidth}
        \textbf{Problem Setup:}
        \begin{itemize}
            \item Linear elasticity
            \item Fixed end: $u(0,y) = g_u(y)$, $v(0,y) = g_v(y)$
            \item Free end: traction $\mathbf{t}$
        \end{itemize}

        \vspace{0.3cm}
        \textbf{Approach:}
        \begin{itemize}
            \item Strong BC enforcement
            \item Energy minimization
        \end{itemize}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Strong BC Enforcement with Trial Functions}

\textbf{Problem:} Enforce $u(x=0, y) = g_u(y)$ and $v(x=0, y) = g_v(y)$

\textbf{Solution:} Construct trial functions that satisfy BCs by design

$$\tilde{u}(x,y) = g_u(y) + \mathcal{D}(x) \cdot \hat{u}_{NN}(x,y)$$
$$\tilde{v}(x,y) = g_v(y) + \mathcal{D}(x) \cdot \hat{v}_{NN}(x,y)$$

Where:
\begin{itemize}
    \item $\mathcal{D}(x)$: Distance function, zero at boundary
    \item $g_u(y), g_v(y)$: Prescribed boundary values (from beam theory)
    \item $\hat{u}_{NN}, \hat{v}_{NN}$: Network outputs
\end{itemize}

\textbf{Example:} For cantilever beam, $\mathcal{D}(x) = x/L$ (simple linear function)

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{PINN Architecture for Elasticity}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/static-beam-pinn.pdf}
    \caption*{Architecture for linear elasticity using energy minimization}
\end{figure}

\textbf{Key Features:}
\begin{itemize}
    \item Network predicts displacements $(u,v)$
    \item AD computes strains $\boldsymbol{\epsilon}$ and stresses $\boldsymbol{\sigma}$
    \item Loss is total potential energy $\Pi$
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Results: 2D Cantilever Beam}
\begin{columns}
	
\column{0.4\textwidth}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/strong-bc.png}
    \caption*{Error distribution using strong BC enforcement and energy minimization}
\end{figure}

\column{0.6\textwidth}
\textbf{Performance:}
\begin{itemize}
    \item Relative $L^2$ error: $2.4 \times 10^{-3}$
    \item Exact satisfaction of fixed-end BCs
    \item Slightly larger errors near loaded end (expected)
    \item No BC loss terms needed!
\end{itemize}

\end{columns}
\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Static Beam: Complete Solution}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/static-beam-results.png}
    \caption*{Displacement and stress fields computed using energy minimization}
\end{figure}

\textbf{Observations:}
\begin{itemize}
    \item Smooth displacement fields
    \item Accurate stress concentration at fixed end
    \item Physical deformation pattern
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Advantages of Energy Minimization}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Physical Meaning:}
        \begin{itemize}
            \item Direct minimization of physical quantity
            \item Guaranteed energy conservation
            \item Natural handling of constraints
        \end{itemize}

        \vspace{0.5cm}

        \textbf{Computational Benefits:}
        \begin{itemize}
            \item Only first derivatives needed
            \item Better conditioned optimization
            \item No need for BC loss terms
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \textbf{Improved Stability:}
        \begin{itemize}
            \item Convex for linear problems
            \item Smoother loss landscape
            \item Faster convergence
        \end{itemize}

        \vspace{0.5cm}

        \textbf{Applicability:}
        \begin{itemize}
            \item Conservative systems
            \item Elasticity, electrostatics
            \item Not for dissipative systems
        \end{itemize}
    \end{column}
\end{columns}

\vspace{0.5cm}
\centering
\textbf{When applicable, energy minimization often outperforms strong form}

\end{frame}

%------------------------------------------------
\section{Practical Considerations}

%------------------------------------------------
\begin{frame}
\frametitle{Implementation Best Practices}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Architecture design:}
        \begin{itemize}
            \item Start with 4-8 layers, 20-50 neurons
            \item Tanh or Swish for smooth problems
            \item Adaptive activation functions for shocks
            \item Skip connections for deep networks
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Training strategies:}
        \begin{itemize}
            \item Adam optimizer with learning rate decay
            \item Start with $\lambda = 1$, then adapt
            \item Quasi-random collocation points
            \item Mini-batching for large problems
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \textbf{Common pitfalls:}
        \begin{itemize}
            \item Imbalanced loss terms
            \item Too few collocation points
            \item Wrong activation for problem type
            \item Ignoring boundary conditions
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Debugging tips:}
        \begin{itemize}
            \item Visualize loss components separately
            \item Check gradient flow
            \item Start with manufactured solutions
            \item Verify BC/IC satisfaction
        \end{itemize}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{When to Use PINNs}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{PINNs excel at:}
        \begin{itemize}
            \item Inverse problems
            \item Data assimilation
            \item High-dimensional PDEs
            \item Irregular geometries
            \item Parameter discovery
            \item Uncertainty quantification
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \textbf{Consider alternatives when:}
        \begin{itemize}
            \item Need guaranteed accuracy
            \item Have simple, regular geometry
            \item Require real-time solutions
            \item Conservation is critical
            \item Problem is well-suited to FEM/FDM
        \end{itemize}
    \end{column}
\end{columns}

\vspace{0.5cm}

\begin{center}
\textbf{PINNs complement, not replace, traditional methods}
\end{center}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Questions?}

\centering
\Large Thank you!

\vspace{2cm}

\textbf{Contact:} \\
Krishna Kumar \\
\textit{krishnak@utexas.edu} \\
University of Texas at Austin

\end{frame}

\end{document}