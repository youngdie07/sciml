%**************************************************************************************
% License:
% CC BY-NC-SA 4.0 (http://creativecommons.org/licenses/by-nc-sa/4.0/)
%**************************************************************************************

\documentclass[aspectratio=169]{beamer}

\mode<presentation> {
	\usetheme{Madrid}
	
	% Burnt orange
	\definecolor{burntorange}{rgb}{0.8, 0.33, 0.0}
	\colorlet{beamer@blendedblue}{burntorange}
	% Pale yellow
	\definecolor{paleyellow}{rgb}{1.0, 1.0, 0.953}
	\setbeamercolor{background canvas}{bg=paleyellow}
	% Secondary and tertiary palette
	\setbeamercolor*{palette secondary}{use=structure,fg=white,bg=burntorange!80!black}
	\setbeamercolor*{palette tertiary}{use=structure,fg=white,bg=burntorange!60!black}
	
	% Remove navigation symbols
	\setbeamertemplate{navigation symbols}{}
}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[labelsep=space,tableposition=top]{caption}
\renewcommand{\figurename}{Fig.} 
\usepackage{caption,subcaption}
\usepackage{xcolor}
\usepackage{transparent}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usetikzlibrary{shapes,arrows,positioning,calc}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}

% Define math operators
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\ReLU}{ReLU}
\DeclareMathOperator{\sign}{sign}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\tiny\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{green!70!black},
    showstringspaces=false,
    frame=single,
    breaklines=true,
    numbers=left,
    numberstyle=\tiny
}

\AtBeginSection[]{
	\begin{frame}{Outline}
		\tableofcontents[
		currentsection,      % highlight the current section
		hideallsubsections   % show only section titles
		]
	\end{frame}
}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------
\title[Physics-Informed Neural Networks]{Physics-Informed Neural Networks (PINNs)} 
\author{Krishna Kumar} % name
\institute[UT Austin] % institution 
{
University of Texas at Austin \\
\medskip
\textit{
  \url{krishnak@utexas.edu}} % Your email address
}
\date{} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % title page as the first slide
\end{frame}

\begin{frame}
 \frametitle{Overview}
 \tableofcontents
\end{frame}

%----------------------------------------------------------------------------------------
% slides
%----------------------------------------------------------------------------------------

\section{The PINN Concept: Beyond Data-Only}

%------------------------------------------------
\begin{frame}
\frametitle{Learning Objectives}

\begin{itemize}
    \item Understand why standard neural networks fail for physics problems
    \item Learn how to incorporate physics into neural network training
    \item Master automatic differentiation for computing derivatives
    \item Compare data-driven vs physics-informed approaches
\end{itemize}

\vspace{1cm}
\centering
\href{https://colab.research.google.com/github/kks32-courses/sciml/blob/main/docs/01-pinns/pinn.ipynb}{\beamergotobutton{Open Notebook: PINN}}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Problem: A Damped Harmonic Oscillator}

A mass $m$ on a spring (constant $k$) with damping (coefficient $c$). The displacement $u(t)$ satisfies:
\begin{equation*}
m \frac{d^2 u}{dt^2} + c \frac{du}{dt} + ku = 0
\end{equation*}
with initial conditions: $u(0) = 1$, $\frac{du}{dt}(0) = 0$.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{figs/harmonic-oscillator.png}
	\caption*{A classic physics problem to illustrate PINNs.}
\end{figure}

\textbf{The Challenge:} Reconstruct the full solution $u(t)$ from a few sparse, noisy data points.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Stage 1: The Data-Only Approach}

\textbf{Idea:} Train a standard neural network to fit the sparse data.

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Loss Function:} Mean Squared Error
        \begin{equation*}
        \mathcal{L}_{\text{data}}(\theta) = \frac{1}{N} \sum_{i=1}^N |\hat{u}_\theta(t_i) - u_i|^2
        \end{equation*}
        
        \textbf{Architecture:}
        \begin{itemize}
            \item Input: Time $t$
            \item Hidden Layers: Tanh activations
            \item Output: Displacement $\hat{u}_\theta(t)$
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\linewidth]{figs/oscillator-nn.png}
            \caption*{Standard NN for function fitting.}
        \end{figure}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Failure of the Data-Only Approach}

\textbf{Result:} The network fits the training points but fails catastrophically between them.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/oscillator-result-nn.png}
	\caption*{The network overfits to the sparse data and does not respect the underlying physics.}
\end{figure}

\textbf{Conclusion:} The network has no knowledge of the governing laws of physics, leading to unphysical oscillations and poor generalization.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Stage 2: Enter Physics-Informed Neural Networks}

\textbf{The Key Insight:} Don't just fit data. Enforce the differential equation itself!

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Physics Residual:}
        We define a residual based on the ODE:
        \begin{equation*}
        \mathcal{R}_\theta(t) = m\frac{d^2\hat{u}_\theta}{dt^2} + c\frac{d\hat{u}_\theta}{dt} + k\hat{u}_\theta
        \end{equation*}
        If the solution is correct, $\mathcal{R}_\theta(t)$ should be zero.
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\linewidth]{figs/oscillator-pinn-nn.png}
            \caption*{PINN architecture with physics loss.}
        \end{figure}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Complete PINN Loss Function}

The total loss is a combination of data fit and physics enforcement.
\begin{equation*}
\mathcal{L}_{\text{total}}(\theta) = \mathcal{L}_{\text{data}}(\theta) + \lambda \mathcal{L}_{\text{physics}}(\theta)
\end{equation*}

\begin{block}{Data Loss}
Ensures the solution passes through the measurements.
\begin{equation*}
\mathcal{L}_{\text{data}}(\theta) = \frac{1}{N_{\text{data}}} \sum_{i=1}^{N_{\text{data}}} |\hat{u}_\theta(t_i) - u_i|^2
\end{equation*}
\end{block}

\begin{block}{Physics Loss}
Ensures the solution obeys the ODE at random "collocation" points.
\begin{equation*}
\mathcal{L}_{\text{physics}}(\theta) = \frac{1}{N_{\text{colloc}}} \sum_{j=1}^{N_{\text{colloc}}} |\mathcal{R}_\theta(t_j)|^2
\end{equation*}
\end{block}

The hyperparameter $\lambda$ balances the two terms.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Secret Weapon: Automatic Differentiation (AD)}

\textbf{Critical Question:} How do we compute $\frac{d\hat{u}_\theta}{dt}$ and $\frac{d^2\hat{u}_\theta}{dt^2}$?

\begin{alertblock}{Answer: Automatic Differentiation}
AD provides \textbf{exact} derivatives of the neural network output with respect to its input, by applying the chain rule through the computational graph.
\begin{itemize}
    \item No finite difference errors.
    \item Computationally efficient (especially reverse-mode AD).
    \item Built into modern frameworks (PyTorch, TensorFlow, JAX).
\end{itemize}
\end{alertblock}

\textbf{Example:} For $u(t) = \sin(t)$, AD can compute $u'(t)=\cos(t)$ and $u''(t)=-\sin(t)$ to machine precision.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Theoretical Foundation: UAT for Sobolev Spaces}

\textbf{Classical UAT:} NNs can approximate any \textit{continuous function}.

\textbf{Problem:} For PDEs, we need to approximate functions \textbf{and their derivatives}.

\begin{block}{Extended Universal Approximation Theorem}
Neural networks with sufficiently smooth activation functions (e.g., $\tanh$, not ReLU) can approximate functions in \textbf{Sobolev spaces} $H^k(\Omega)$.
\begin{equation*}
\|u - \hat{u}_\theta\|_{H^k} < \epsilon
\end{equation*}
The Sobolev norm $\|u\|_{H^k}^2 = \sum_{|\alpha| \leq k} \|D^\alpha u\|_{L^2}^2$ measures the error in the function and all its derivatives up to order $k$.
\end{block}

\textbf{Why this matters:} For a $k^{th}$-order ODE/PDE, we need an activation function that is at least $k$ times differentiable ($C^k$). For our 2nd-order oscillator, we need a $C^2$ activation like $\tanh$.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Moment of Truth: Standard NN vs. PINN}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{figs/pinn-nn-comparison.png}
	\caption*{Placeholder for the comparison plot from the notebook.}
\end{figure}

\textbf{Observation:}
\begin{itemize}
    \item \textbf{Standard NN:} Fits data points, but fails to generalize.
    \item \textbf{PINN:} Fits data points AND follows the physics, resulting in a globally accurate solution.
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Deep Dive: Derivative and Phase Portrait Analysis}

The ultimate test: Does the PINN learn physically consistent derivatives?

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{figs/derivative-comparison.png}
	\caption*{Derivative  plots.}
\end{figure}

\textbf{Result:} The PINN learns the correct velocity ($du/dt$) and acceleration ($d^2u/dt^2$).

\end{frame}

%------------------------------------------------
\begin{frame}
	\frametitle{Deep Dive: Derivative and Phase Portrait Analysis}
	
	The ultimate test: Does the PINN learn physically consistent derivatives?
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=\textwidth]{figs/oscillator-phase-portrait.png}
		\caption*{Phase portrait plots.}
	\end{figure}
	
	\textbf{Result:} The phase portrait (velocity vs. displacement) traces the correct physical trajectory (a spiral for a damped oscillator).
	
\end{frame}

%------------------------------------------------
\section{Enforcing Boundary Conditions}

%------------------------------------------------
\begin{frame}
\frametitle{The 1D Poisson Problem}

We now tackle a boundary value problem, the 1D Poisson equation:
\begin{equation*}
\frac{d^2u}{dx^2} + \pi \sin(\pi x) = 0, \quad \text{for } x \in [0, 1]
\end{equation*}
with Dirichlet boundary conditions (BCs):
\begin{equation*}
u(0) = 0 \quad \text{and} \quad u(1) = 0
\end{equation*}

\textbf{Goal:} Train a PINN to find the solution using only the governing equation and its BCs.

\vspace{1cm}
\centering
\href{https://colab.research.google.com/github/kks32-courses/sciml/blob/main/docs/01-pinns/poisson.ipynb}{\beamergotobutton{Open Notebook: 1D Poisson}}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Method 1: Soft Constraints}

Treat the boundary conditions as another component of the loss function.

\textbf{Total Loss:}
\begin{equation*}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{PDE}} + \lambda_{BC} \mathcal{L}_{BC}
\end{equation*}

\begin{block}{Boundary Loss}
A mean squared error term that penalizes violations of the BCs.
\begin{equation*}
\mathcal{L}_{BC} = \frac{1}{N_{BC}} \sum_{i=1}^{N_{BC}} |\hat{u}_\theta(x_i) - u_{BC}|^2
\end{equation*}
\end{block}

\begin{alertblock}{Pros \& Cons}
\begin{itemize}
    \item[+]\textbf{Flexible:} Easy to implement for any type of BC (Dirichlet, Neumann, etc.).
    \item[-]\textbf{Approximate:} Satisfaction is not guaranteed, only encouraged.
    \item[-]\textbf{Tuning:} Requires careful tuning of the weight $\lambda_{BC}$.
\end{itemize}
\end{alertblock}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Method 2: Hard Constraints}

Modify the network architecture to satisfy the BCs \textit{by construction}.

For our problem with $u(0)=0$ and $u(1)=0$, we can define a trial solution $\tilde{u}(x)$:
\begin{equation*}
\tilde{u}(x) = \underbrace{x(1-x)}_{D(x)} \cdot \underbrace{\text{NN}(x; \theta)}_{\text{Network Output}}
\end{equation*}
The distance function $D(x)$ is zero at the boundaries, forcing $\tilde{u}(x)$ to be zero there, regardless of the network's output.

\begin{alertblock}{Pros \& Cons}
\begin{itemize}
    \item[+]\textbf{Exact:} BCs are satisfied perfectly.
    \item[+]\textbf{Simpler Loss:} No need for $\mathcal{L}_{BC}$ or $\lambda_{BC}$, leading to more stable training.
    \item[-]\textbf{Inflexible:} Requires designing a specific trial function for the problem's geometry and BCs, which can be difficult for complex cases.
\end{itemize}
\end{alertblock}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Comparison: Soft vs. Hard Constraints}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/poisson-soft-vs-hard.png}
\end{figure}

\textbf{Conclusion:}
\begin{itemize}
    \item Both methods achieve high accuracy.
    \item The hard constraint method shows slightly lower error and, by design, has zero error at the boundaries.
    \item For simple geometries and Dirichlet BCs, \textbf{hard constraints are often superior}.
    \item For complex problems, \textbf{soft constraints offer greater versatility}.
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Inverse Problems: Discovering Physics}

%------------------------------------------------
\begin{frame}
\frametitle{Forward vs. Inverse Problems}

\begin{block}{Forward Problem}
\begin{itemize}
    \item \textbf{Given:} Full physical model (equations + parameters).
    \item \textbf{Find:} The solution $u(x,t)$.
    \item \textit{Example: Simulate temperature given thermal conductivity.}
\end{itemize}
\end{block}

\begin{block}{Inverse Problem}
\begin{itemize}
    \item \textbf{Given:} Sparse measurements of the solution $u(x,t)$.
    \item \textbf{Find:} Unknown physical parameters in the model.
    \item \textit{Example: Infer thermal conductivity from temperature measurements.}
\end{itemize}
\end{block}

\vspace{1em}
\centering
\href{https://colab.research.google.com/github/kks32-courses/sciml/blob/main/docs/01-pinns/inverse-heat.ipynb}{\beamergotobutton{Open Notebook: Inverse Heat}}


\end{frame}

%------------------------------------------------
\begin{frame}
	\frametitle{Forward vs. Inverse Problems}
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.55\textwidth]{figs/inverse-heat-diffusivity.png}
	\end{figure}
	
\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The PINN Approach to Inverse Problems}

\textbf{The Key Insight:} Treat the unknown physical parameters as additional trainable variables in the network.

\textbf{Problem:} 1D steady-state heat conduction.
\begin{equation*}
-k \frac{d^2T}{dx^2} = f(x)
\end{equation*}
Here, the thermal diffusivity $k$ is \textbf{unknown}.

\textbf{PINN Framework:}
\begin{itemize}
    \item The neural network learns the temperature field: $\hat{T}_\theta(x)$.
    \item A new trainable parameter is introduced: $\hat{k}$.
    \item The optimizer updates both the network weights $\theta$ and the parameter $\hat{k}$ simultaneously.
\end{itemize}

\textbf{Loss Function:} $\mathcal{L}(\theta, \hat{k}) = \mathcal{L}_{\text{data}} + \mathcal{L}_{\text{PDE}} + \mathcal{L}_{\text{BC}}$
where the physics loss now includes the trainable parameter $\hat{k}$:
\begin{equation*}
\mathcal{L}_{\text{PDE}} = \frac{1}{N_f}\sum \left|-\hat{k}\frac{d^2\hat{T}_\theta}{dx^2}(x_j) - f(x_j)\right|^2
\end{equation*}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Implementation: Parameter Estimation}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/inverse-problem-setup.png}
	\caption*{"Experimental Setup" plot, showing the true solution and sparse, noisy data points.}
\end{figure}

\textbf{Challenge:} Can the PINN recover the true value of $k$ from only 10 noisy measurements?

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Results: Parameter Recovery}

\begin{minipage}[t]{0.48\textwidth}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{figs/inverse-heat-diffusivity.png}
	\caption*{Parameter convergence and temperature field reconstruction.}
\end{figure}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\begin{itemize}
    \item The estimated parameter $\hat{k}$ converges to the true value.
    \item The PINN simultaneously reconstructs the full, continuous temperature field accurately.
    \item This is achieved from very sparse and noisy data, showcasing the regularizing effect of the physics loss.
\end{itemize}
\end{minipage}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Summary: Why PINNs are Powerful}

\textbf{1. Regularization Effect:}
\begin{itemize}
    \item Physics constraints prevent overfitting and guide the solution in data-sparse regions.
\end{itemize}

\textbf{2. Data Efficiency:}
\begin{itemize}
    \item Physics provides a strong inductive bias, allowing PINNs to learn from very few measurements.
\end{itemize}

\textbf{3. Accurate Derivatives:}
\begin{itemize}
    \item Automatic differentiation provides exact derivatives, which are learned correctly as a consequence of enforcing the physics.
\end{itemize}

\textbf{4. Versatility:}
\begin{itemize}
    \item The same framework can solve forward problems, inverse problems, and handle various boundary conditions.
\end{itemize}

\begin{center}
\textbf{PINNs = Universal Function Approx. + Physics Constraints + Auto. Diff.}
\end{center}

\end{frame}

%------------------------------------------------
\section{Collocation Point Strategies}

%------------------------------------------------
\begin{frame}
\frametitle{Collocation Points: Where to Enforce Physics}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{What are collocation points?}
        \begin{itemize}
            \item Points where we evaluate PDE residual
            \item Do not need measurement data
            \item Distributed throughout domain
            \item More points → better physics enforcement
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Key question:}
        How should we distribute these points for optimal performance?
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figs/sampling.png}
            \caption*{Different collocation sampling strategies}
        \end{figure}
    \end{column}
\end{columns}

\end{frame}



%------------------------------------------------
\begin{frame}
	\frametitle{Collocation Sampling Strategies}
\begin{columns}[T]
    \begin{column}{0.33\textwidth}
        \textbf{Uniform Grid:}
        \begin{itemize}
            \item Simple to implement
            \item Good coverage
            \item Curse of dimensionality
        \end{itemize}
    \end{column}
    \begin{column}{0.33\textwidth}
        \textbf{Random (Monte Carlo):}
        \begin{itemize}
            \item Dimension-independent
            \item May cluster/leave gaps
            \item Easy to add points
        \end{itemize}
    \end{column}
    \begin{column}{0.34\textwidth}
        \textbf{Quasi-Random:}
        \begin{itemize}
            \item Better coverage than random
            \item Low discrepancy sequences
            \item Hammersley, Sobol, Halton
        \end{itemize}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
	\frametitle{Collocation Sampling Strategies}
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.85\textwidth]{figs/collocation-sampling-comparison.png}
		\caption*{Comparison of different sampling strategies for collocation points}
	\end{figure}
\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Impact of Collocation Strategy}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figs/collocation-error-grid.png}
            \caption*{Error with uniform grid sampling}
        \end{figure}
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figs/collocation-error-hammersley.png}
            \caption*{Error with Hammersley sampling}
        \end{figure}
    \end{column}
\end{columns}

\vspace{0.3cm}

\textbf{Observations:}
\begin{itemize}
    \item Quasi-random sampling often outperforms uniform/random
    \item Better space-filling properties lead to lower errors
    \item Particularly important in higher dimensions
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Adaptive Weights and Loss Balancing}

%------------------------------------------------
\begin{frame}
\frametitle{The Loss Balancing Challenge}

\begin{block}{The Problem}
Different loss terms can have vastly different magnitudes and gradients
\end{block}

\textbf{Total PINN loss:}
\begin{equation*}
\mathcal{L} = \mathcal{L}_{\text{data}} + \lambda_{\text{PDE}} \mathcal{L}_{\text{PDE}} + \lambda_{\text{BC}} \mathcal{L}_{\text{BC}} + \lambda_{\text{IC}} \mathcal{L}_{\text{IC}}
\end{equation*}

\vspace{0.5cm}

\textbf{Challenges:}
\begin{itemize}
    \item Data loss: Often $O(10^{-3})$ to $O(10^{-1})$
    \item PDE residual: Can be $O(10^2)$ to $O(10^4)$ initially
    \item Boundary conditions: Variable scale
    \item Poor balance → training instability or failure
\end{itemize}

\vspace{0.5cm}

\textbf{Question:} How to choose $\lambda$ values optimally?

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Gradient-Based Adaptive Weighting}

\textbf{Key idea:} Balance the gradients of different loss terms

\vspace{0.5cm}

\textbf{Algorithm (Wang et al., 2021):}
\begin{enumerate}
    \item Compute gradient statistics:
    $$\bar{g}_i = \frac{1}{|\theta|}\sum_{\theta} |\nabla_\theta \mathcal{L}_i|$$
    
    \item Update weights to balance gradients:
    $$\lambda_i^{(k+1)} = \lambda_i^{(k)} \cdot \left(\frac{\max_j \bar{g}_j}{\bar{g}_i}\right)^\alpha$$
    
    \item Apply exponential moving average for stability
\end{enumerate}

\vspace{0.5cm}

\textbf{Benefits:}
\begin{itemize}
    \item Prevents gradient imbalance
    \item Improves convergence speed
    \item Reduces manual tuning
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Adaptive Weights in Action}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/adaptive-weights-results.png}
    \caption*{Comparison of fixed vs adaptive weighting on a 2D Poisson problem}
\end{figure}

\textbf{Results:}
\begin{itemize}
    \item Adaptive weights: 10× faster convergence
    \item Better final accuracy
    \item More stable training
    \item Automatic adjustment to problem characteristics
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Advanced Application: Burgers Equation}

%------------------------------------------------
\begin{frame}
\frametitle{The Burgers Equation: A Nonlinear PDE Challenge}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Viscous Burgers equation:}
        $$\frac{\partial u}{\partial t} + u\frac{\partial u}{\partial x} = \nu\frac{\partial^2 u}{\partial x^2}$$
        
        \textbf{Domain:} $(x,t) \in [-1,1] \times [0,1]$
        
        \textbf{Initial condition:}
        $$u(x,0) = -\sin(\pi x)$$
        
        \textbf{Boundary conditions:}
        $$u(-1,t) = u(1,t) = 0$$
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figs/burgers-solution.png}
            \caption*{Burgers equation solution showing shock formation}
        \end{figure}
    \end{column}
\end{columns}

\vspace{0.3cm}
\textbf{Challenges:} Nonlinearity, shock formation, multi-scale features

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{PINN for Burgers Equation}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Network architecture:}
        \begin{itemize}
            \item Input: $(x, t)$
            \item Output: $u(x, t)$
            \item Hidden layers: 8 × 20 neurons
            \item Activation: Tanh
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Loss function:}
        \begin{align*}
        \mathcal{L} = &\mathcal{L}_{\text{PDE}} + \mathcal{L}_{\text{IC}} + \mathcal{L}_{\text{BC}} \\
        \mathcal{L}_{\text{PDE}} = &\frac{1}{N_f}\sum \left|\frac{\partial u}{\partial t} + u\frac{\partial u}{\partial x} - \nu\frac{\partial^2 u}{\partial x^2}\right|^2
        \end{align*}
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figs/collocation-burgers.png}
            \caption*{Collocation points for Burgers equation}
        \end{figure}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Burgers Equation: Forward Problem Results}

\begin{columns}[t]
	\column{0.65\textwidth}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/burgers-prediction.png}
    \caption*{PINN prediction vs exact solution for Burgers equation}
\end{figure}

	\column{0.35\textwidth}
\textbf{Performance metrics:}
\begin{itemize}
    \item Relative $L^2$ error: $< 1\%$
    \item Training time: ~5 minutes on GPU
    \item No mesh required
    \item Captures shock accurately
\end{itemize}
	
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Inverse Problem: Parameter Discovery in Burgers}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Problem setup:}
        \begin{itemize}
            \item Unknown viscosity $\nu$
            \item Sparse measurements of $u(x,t)$
            \item Goal: Recover $\nu$ and full solution
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Modified loss:}
        $$\mathcal{L} = \mathcal{L}_{\text{data}} + \lambda\mathcal{L}_{\text{PDE}}(\nu)$$
        
        where $\nu$ is a trainable parameter
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figs/burgers-inverse-prediction.png}
            \caption*{Inverse problem: recovering solution from sparse data}
        \end{figure}
    \end{column}
\end{columns}

\vspace{0.5cm}

\textbf{Results:}
\begin{itemize}
    \item True $\nu = 0.01$, Predicted $\nu = 0.00998$
    \item Error in parameter: $< 0.2\%$
    \item From only 2000 scattered measurements!
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Practical Considerations}

%------------------------------------------------
\begin{frame}
\frametitle{Implementation Best Practices}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{Architecture design:}
        \begin{itemize}
            \item Start with 4-8 layers, 20-50 neurons
            \item Tanh or Swish for smooth problems
            \item Adaptive activation functions for shocks
            \item Skip connections for deep networks
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Training strategies:}
        \begin{itemize}
            \item Adam optimizer with learning rate decay
            \item Start with $\lambda = 1$, then adapt
            \item Quasi-random collocation points
            \item Mini-batching for large problems
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \textbf{Common pitfalls:}
        \begin{itemize}
            \item Imbalanced loss terms
            \item Too few collocation points
            \item Wrong activation for problem type
            \item Ignoring boundary conditions
        \end{itemize}
        
        \vspace{0.5cm}
        
        \textbf{Debugging tips:}
        \begin{itemize}
            \item Visualize loss components separately
            \item Check gradient flow
            \item Start with manufactured solutions
            \item Verify BC/IC satisfaction
        \end{itemize}
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{When to Use PINNs}

\begin{columns}[T]
    \begin{column}{0.5\textwidth}
        \textbf{PINNs excel at:}
        \begin{itemize}
            \item Inverse problems
            \item Data assimilation
            \item High-dimensional PDEs
            \item Irregular geometries
            \item Parameter discovery
            \item Uncertainty quantification
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \textbf{Consider alternatives when:}
        \begin{itemize}
            \item Need guaranteed accuracy
            \item Have simple, regular geometry
            \item Require real-time solutions
            \item Conservation is critical
            \item Problem is well-suited to FEM/FDM
        \end{itemize}
    \end{column}
\end{columns}

\vspace{0.5cm}

\begin{center}
\textbf{PINNs complement, not replace, traditional methods}
\end{center}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Questions?}

\centering
\Large Thank you!

\vspace{2cm}

\textbf{Contact:} \\
Krishna Kumar \\
\textit{krishnak@utexas.edu} \\
University of Texas at Austin

\end{frame}

\end{document}