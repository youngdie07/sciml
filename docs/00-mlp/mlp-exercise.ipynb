{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** [![Open in Colab](https://img.shields.io/badge/Open%20in-Colab-F9AB00?style=flat-square&logo=googlecolab)](https://colab.research.google.com/github/kks32-courses/sciml/blob/main/docs/00-mlp/mlp-exercise.ipynb)\n",
    "**Solution:** [![Open in Colab](https://img.shields.io/badge/Open%20in-Colab-F9AB00?style=flat-square&logo=googlecolab)](https://colab.research.google.com/github/kks32-courses/sciml/blob/main/docs/00-mlp/mlp.ipynb)\n",
    "\n",
    "\n",
    "**Slides:** [![View Presentation](https://img.shields.io/badge/View-PDF-blue?style=flat-square&logo=googleslides&logoColor=white)](https://github.com/kks32-courses/sciml/raw/main/docs/00-mlp/mlp-slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the fundamental concepts of using neural networks for function approximation, a core task in scientific machine learning (SciML). We will build up from the basic building block, the perceptron, to a single-layer network and demonstrate its capacity to approximate a simple function, connecting theory to practice using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Problem set-up and motivation\n",
    "\n",
    "### The 1D Poisson Equation: Our Benchmark Problem\n",
    "\n",
    "We begin with a familiar problem: the one-dimensional Poisson equation on the unit interval $[0, 1]$ with homogeneous Dirichlet boundary conditions:\n",
    "\n",
    "$$-\\frac{d^2u}{dx^2} = f(x), \\quad x \\in [0, 1]$$\n",
    "\n",
    "subject to boundary conditions:\n",
    "$$u(0) = 0, \\quad u(1) = 0$$\n",
    "\n",
    "This equation models diverse physical phenomena: heat conduction in a rod, deflection of a loaded beam, or electrostatic potential in one dimension. The function $u(x)$ represents the unknown solution we seek, while $f(x)$ is the prescribed source term.\n",
    "\n",
    "For our initial exploration, we choose a source term that gives a simple, known solution:\n",
    "$$f(x) = \\pi^2 \\sin(\\pi x)$$\n",
    "\n",
    "This choice yields the analytical solution:\n",
    "$$u(x) = \\sin(\\pi x)$$\n",
    "\n",
    "We can verify this solution by direct substitution. The second derivative of $u(x) = \\sin(\\pi x)$ is $u''(x) = -\\pi^2 \\sin(\\pi x)$, so:\n",
    "$$-\\frac{d^2u}{dx^2} = -(-\\pi^2 \\sin(\\pi x)) = \\pi^2 \\sin(\\pi x) = f(x) \\quad \\checkmark$$\n",
    "\n",
    "The boundary conditions are satisfied: $u(0) = \\sin(0) = 0$ and $u(1) = \\sin(\\pi) = 0$ ✓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T02:20:43.977671Z",
     "start_time": "2025-07-09T02:20:42.207822Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "SEED = 4321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Function Approximation Challenge\n",
    "\n",
    "Consider this question: *Can we learn to approximate $u(x) = \\sin(\\pi x)$ by observing only sparse data points?*\n",
    "\n",
    "Let's explore this with a concrete example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sparse training data\n",
    "n_train = 15\n",
    "\n",
    "# Training points - sparse and noisy\n",
    "x_train = np.random.rand(n_train)\n",
    "x_train[0], x_train[-1] = 0, 1  # Include boundaries\n",
    "x_train = np.sort(x_train)\n",
    "\n",
    "# Target function: sin(πx)\n",
    "def target_func(x):\n",
    "    \"\"\"Target function: sin(πx)\"\"\"\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "y_train = target_func(x_train) + 0.02 * np.random.randn(n_train)  # Add noise\n",
    "\n",
    "# Dense grid for visualization\n",
    "x_test = np.linspace(0, 1, 200)\n",
    "y_exact = target_func(x_test)\n",
    "\n",
    "# Visualize the problem\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_test, y_exact, 'b-', alpha=0.3, linewidth=2, label='True solution')\n",
    "plt.scatter(x_train, y_train, s=50, c='red', zorder=5, label=f'{n_train} noisy samples')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')\n",
    "plt.legend()\n",
    "plt.title('The Challenge: Learn u(x) from Sparse, Noisy Data')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training with {n_train} noisy data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Approximation: Why It Fails\n",
    "\n",
    "Let's first try the classical approach - polynomial fitting. We'll see why high-degree polynomials fail on sparse data due to **Runge's phenomenon**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial import Polynomial\n",
    "\n",
    "# Try different polynomial degrees\n",
    "degrees = [3, 5, 10]  # n_train - 1 = 14 is the maximum meaningful degree\n",
    "poly_models = {}\n",
    "errors = {}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, degree in enumerate(degrees):\n",
    "    # Fit polynomial\n",
    "    poly = Polynomial.fit(x_train, y_train, degree)\n",
    "    y_poly = poly(x_test)\n",
    "    poly_models[f'Poly-{degree}'] = y_poly\n",
    "    \n",
    "    # Compute errors\n",
    "    train_mse = np.mean((poly(x_train) - y_train)**2)\n",
    "    test_error = np.max(np.abs(y_poly - y_exact))\n",
    "    errors[degree] = {'train_mse': train_mse, 'test_error': test_error}\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    ax.plot(x_test, y_exact, 'k--', alpha=0.5, linewidth=2, label='True')\n",
    "    ax.plot(x_test, y_poly, 'b-', linewidth=2, label=f'Polynomial')\n",
    "    ax.scatter(x_train, y_train, s=30, c='red', zorder=5, label='Data')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('u(x)')\n",
    "    ax.set_title(f'Degree {degree}\\nTrain MSE: {train_mse:.2e}\\nMax Error: {test_error:.2e}')\n",
    "    ax.legend()\n",
    "    ax.set_ylim([-0.5, 1.5])\n",
    "    \n",
    "plt.suptitle('Runge\\'s Phenomenon: Higher Degree → Worse Oscillations', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print error summary\n",
    "print(\"Polynomial Approximation Results:\")\n",
    "print(\"-\" * 40)\n",
    "for degree, err in errors.items():\n",
    "    print(f\"Degree {degree:2d}: Train MSE = {err['train_mse']:.4e}, Max Error = {err['test_error']:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observation**: As polynomial degree increases:\n",
    "- Training error decreases (eventually reaching zero for degree n-1)\n",
    "- But test error *increases* due to wild oscillations between data points\n",
    "- This is **Runge's phenomenon** - a fundamental limitation of polynomial interpolation\n",
    "\n",
    "This motivates the need for alternative approaches like neural networks that can provide smooth approximations without these oscillations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Methods: Finite Difference\n",
    "\n",
    "Traditional numerical methods like the Finite Difference Method or Finite Element Method solve PDEs by **discretizing the domain** into a grid or mesh. They approximate the solution $u(x)$ by finding its values at these specific, discrete points.\n",
    "\n",
    "For example, the Finite Difference method approximates the second derivative:\n",
    "$$\\frac{d^2u}{dx^2} \\approx \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2}$$\n",
    "This transforms the differential equation into a system of algebraic equations for the values $u_i$ at grid points $x_i$. The result is a discrete representation of the solution.\n",
    "\n",
    "![FDM](figs/finite-difference-methods.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple finite difference solution for comparison\n",
    "def source_function(x):\n",
    "    \"\"\"Source term for Poisson equation: f(x) = π²sin(πx)\"\"\"\n",
    "    return np.pi**2 * np.sin(np.pi * x)\n",
    "\n",
    "def solve_poisson_fd(n_points=101):\n",
    "    \"\"\"\n",
    "    Solve 1D Poisson equation: -u'' = f(x) using finite differences\n",
    "    with homogeneous Dirichlet boundary conditions.\n",
    "    \"\"\"\n",
    "    # Grid\n",
    "    x_fd = np.linspace(0, 1, n_points)\n",
    "    h = 1.0 / (n_points - 1)\n",
    "    \n",
    "    # Tridiagonal matrix for -u'' (central difference)\n",
    "    n_interior = n_points - 2\n",
    "    A = np.zeros((n_interior, n_interior))\n",
    "    np.fill_diagonal(A, 2.0 / h**2)\n",
    "    np.fill_diagonal(A[1:, :], -1.0 / h**2)\n",
    "    np.fill_diagonal(A[:, 1:], -1.0 / h**2)\n",
    "    \n",
    "    # Right-hand side (source function at interior points)\n",
    "    f_rhs = source_function(x_fd[1:-1])\n",
    "    \n",
    "    # Solve linear system A u_interior = f_rhs\n",
    "    u_interior = np.linalg.solve(A, f_rhs)\n",
    "    \n",
    "    # Complete solution with boundary conditions\n",
    "    u_fd = np.zeros(n_points)\n",
    "    u_fd[0] = 0  # u(0) = 0\n",
    "    u_fd[-1] = 0  # u(1) = 0\n",
    "    u_fd[1:-1] = u_interior\n",
    "    \n",
    "    return x_fd, u_fd\n",
    "\n",
    "# Solve using finite differences with different resolutions\n",
    "x_fd_coarse, u_fd_coarse = solve_poisson_fd(11)  # Coarse grid\n",
    "x_fd_fine, u_fd_fine = solve_poisson_fd(51)      # Fine grid\n",
    "\n",
    "# Create comparison plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Coarse grid\n",
    "ax1.plot(x_test, y_exact, 'b-', linewidth=2, label='Exact')\n",
    "ax1.plot(x_fd_coarse, u_fd_coarse, 'ro-', markersize=6, label=f'FD (11 points)')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('u(x)')\n",
    "ax1.set_title('Finite Difference: Coarse Grid')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Fine grid\n",
    "ax2.plot(x_test, y_exact, 'b-', linewidth=2, label='Exact')\n",
    "ax2.plot(x_fd_fine, u_fd_fine, 'ro-', markersize=3, label=f'FD (51 points)')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('u(x)')\n",
    "ax2.set_title('Finite Difference: Fine Grid')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Traditional Method: Finite Differences')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute errors\n",
    "error_coarse = np.max(np.abs(u_fd_coarse - target_func(x_fd_coarse)))\n",
    "error_fine = np.max(np.abs(u_fd_fine - target_func(x_fd_fine)))\n",
    "\n",
    "print(f\"FD Coarse (11 points): Max error = {error_coarse:.4e}\")\n",
    "print(f\"FD Fine (51 points):   Max error = {error_fine:.4e}\")\n",
    "print(f\"Error reduction factor: {error_coarse/error_fine:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Mathematical Foundations \n",
    "\n",
    "### Weierstrass Polynomial Approximation Theorem\n",
    "\n",
    "**Weierstrass Approximation Theorem (1885):** If $f$ is a continuous function on a closed and bounded interval $[a, b]$, then for any $\\varepsilon > 0$, there exists a polynomial $P(x)$ such that\n",
    "\n",
    "$|f(x) - P(x)| < \\varepsilon$\n",
    "\n",
    "for all $x \\in [a, b]$.\n",
    "\n",
    "### What This Means\n",
    "\n",
    "The theorem tells us that **any continuous function can be approximated arbitrarily closely by polynomials**. No matter how \"weird\" or complicated a continuous function might be, we can always find a polynomial that gets as close as we want to that function over any closed interval.\n",
    "\n",
    "This is remarkable because:\n",
    "- Polynomials are among the simplest functions to work with computationally\n",
    "- They can be differentiated and integrated easily\n",
    "- Yet they're dense in the space of continuous functions\n",
    "\n",
    "### Historical Context\n",
    "\n",
    "Karl Weierstrass proved this theorem in 1885, though the result was actually known to him since 1872. It was groundbreaking because it showed that despite polynomials seeming \"simple,\" they could approximate any continuous behavior.\n",
    "\n",
    "### Constructive Proof: Bernstein Polynomials\n",
    "\n",
    "One elegant way to prove this theorem constructively is using **Bernstein polynomials**. For a function $f$ on $[0,1]$, the $n$-th Bernstein polynomial is:\n",
    "\n",
    "$B_n(x) = \\sum_{k=0}^n f(k/n) \\cdot \\binom{n}{k} \\cdot x^k \\cdot (1-x)^{n-k}$\n",
    "\n",
    "where $\\binom{n}{k}$ is the binomial coefficient. As $n \\to \\infty$, $B_n(x)$ converges uniformly to $f(x)$.\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "The theorem bridges the gap between the abstract world of arbitrary continuous functions and the concrete, computational world of polynomials. It guarantees that we never need to work with functions more complicated than polynomials when dealing with continuous phenomena on bounded intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence analysis: Error vs Polynomial Degree\n",
    "from scipy.special import comb\n",
    "\n",
    "def bernstein_polynomial(f, n, x):\n",
    "    \"\"\"Compute n-th Bernstein polynomial approximation\"\"\"\n",
    "    result = np.zeros_like(x)\n",
    "    for k in range(n + 1):\n",
    "        basis = comb(n, k) * (x ** k) * ((1 - x) ** (n - k))\n",
    "        result += f(k/n) * basis\n",
    "    return result\n",
    "\n",
    "# Test convergence\n",
    "#x_test_bern = np.linspace(0, 1, 1000)\n",
    "#y_true_bern = target_func(x_test_bern)\n",
    "\n",
    "\n",
    "degrees = [5, 10, 20, 50, 100, 200]\n",
    "errors = []\n",
    "\n",
    "for n in degrees:\n",
    "    y_approx = bernstein_polynomial(target_func, n, x_train)\n",
    "    max_error = np.max(np.abs(y_train - y_approx))\n",
    "    errors.append(max_error)\n",
    "    \n",
    "# Theoretical rate: O(1/sqrt(n))\n",
    "theoretical = 1.0 / np.sqrt(degrees)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(degrees, errors, 'bo-', label='Bernstein Approximation', linewidth=2, markersize=8)\n",
    "plt.loglog(degrees, theoretical, 'r--', label='O(1/√n) theoretical', linewidth=2)\n",
    "plt.xlabel('Polynomial Degree n')\n",
    "plt.ylabel('Maximum Error')\n",
    "plt.title('Bernstein Polynomial Convergence')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Convergence rate: {np.polyfit(np.log(degrees[-3:]), np.log(errors[-3:]), 1)[0]:.2f}\")\n",
    "print(\"(Theoretical: -0.5 for O(1/√n))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theoretical convergence rate is $O(1/\\sqrt{n})$. The observed convergence is slow but guaranteed to reach zero. This demonstrates the Weierstrass Approximation Theorem: any continuous function on [0,1] can be uniformly approximated by polynomials to arbitrary precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Proves the Weierstrass Theorem\n",
    "\n",
    "The Bernstein polynomials provide a **constructive proof** of the Weierstrass Approximation Theorem:\n",
    "\n",
    "1. **Guaranteed Convergence**: For any continuous function $f$ on $[0,1]$, the Bernstein polynomials $B_n(f;x)$ converge uniformly to $f$ as $n \\to \\infty$\n",
    "\n",
    "2. **Explicit Construction**: Unlike existence proofs, Bernstein gives us an explicit formula:\n",
    "   $$B_n(f; x) = \\sum_{k=0}^{n} f\\left(\\frac{k}{n}\\right) \\binom{n}{k} x^k (1-x)^{n-k}$$\n",
    "\n",
    "3. **Convergence Rate**: For functions with modulus of continuity $\\omega(f; \\delta)$, the error satisfies:\n",
    "   $$|f(x) - B_n(f; x)| \\leq \\frac{3}{2} \\omega\\left(f; \\frac{1}{\\sqrt{n}}\\right)$$\n",
    "   \n",
    "4. **Key Properties**:\n",
    "   - **Positivity**: If $f \\geq 0$, then $B_n(f) \\geq 0$\n",
    "   - **Monotonicity**: If $f$ is increasing, so is $B_n(f)$\n",
    "   - **End-point interpolation**: $B_n(f; 0) = f(0)$ and $B_n(f; 1) = f(1)$\n",
    "\n",
    "This demonstrates that **any continuous function can be approximated arbitrarily well by polynomials** - the foundation for neural networks' universal approximation capabilities!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Polynomials to Neural Networks: The Weierstrass Approximation Theorem\n",
    "\n",
    "Before diving into the Universal Approximation Theorem for neural networks, let's explore its historical predecessor: the **Weierstrass Approximation Theorem** (1885). This foundational result shows that polynomials can approximate any continuous function, providing the mathematical intuition for why neural networks work.\n",
    "\n",
    "#### The Weierstrass Approximation Theorem\n",
    "\n",
    "**Theorem (Weierstrass, 1885):** Every continuous function on a closed interval $[a, b]$ can be uniformly approximated as closely as desired by a polynomial function.\n",
    "\n",
    "Formally: For any continuous function $f: [a, b] \\to \\mathbb{R}$ and any $\\epsilon > 0$, there exists a polynomial $p(x)$ such that:\n",
    "\n",
    "$$\\sup_{x \\in [a,b]} |f(x) - p(x)| < \\epsilon$$\n",
    "\n",
    "This theorem tells us that the set of polynomials is **dense** in the space of continuous functions under the uniform norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Matters for Neural Networks\n",
    "\n",
    "The Weierstrass theorem establishes a crucial principle: **simple building blocks (monomials $x^n$) can approximate arbitrarily complex continuous functions**. Neural networks follow the same principle but with different building blocks:\n",
    "\n",
    "- **Polynomials**: Build from monomials $(1, x, x^2, x^3, ...)$\n",
    "- **Neural Networks**: Build from activation functions (ReLU, sigmoid, tanh, ...)\n",
    "\n",
    "Both achieve universal approximation, but neural networks often do it more efficiently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.polynomial import Polynomial\n",
    "\n",
    "def approximate_with_polynomial(func, x_range, degree, title=\"Polynomial Approximation\"):\n",
    "    \"\"\"Approximate a function using polynomial regression (least squares)\"\"\"\n",
    "    x = np.linspace(x_range[0], x_range[1], 100)\n",
    "    y = func(x)\n",
    "    # Fit polynomial using numpy\n",
    "    poly = Polynomial.fit(x, y, degree)\n",
    "    y_approx = poly(x)\n",
    "    # Calculate approximation error\n",
    "    error = np.max(np.abs(y - y_approx))\n",
    "    \n",
    "    return poly, error\n",
    "\n",
    "# Target function: sin(πx)\n",
    "target_func = lambda x: np.sin(np.pi * x)\n",
    "print(\"Weierstrass Approximation of sin(πx) with Polynomials\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show increasing polynomial degrees\n",
    "degrees = [3, 5, 9]\n",
    "colors = ['red', 'green', 'orange']\n",
    "errors = []\n",
    "\n",
    "# Create single figure with two subplots\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# First subplot: All approximations together\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Plot target function first\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = target_func(x)\n",
    "plt.plot(x, y, 'b-', linewidth=3, label='Target: $\\\\sin(\\\\pi x)$')\n",
    "\n",
    "# Plot approximations for each degree\n",
    "for i, degree in enumerate(degrees):\n",
    "    poly, error = approximate_with_polynomial(\n",
    "        target_func, [0, 1], degree,\n",
    "        f\"Polynomial Approximation (Degree {degree})\"\n",
    "    )\n",
    "    \n",
    "    # Get approximation data\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    y = target_func(x)\n",
    "    y_approx = poly(x)\n",
    "    \n",
    "    errors.append(error)\n",
    "    \n",
    "    # Plot approximation with different color\n",
    "    plt.plot(x, y_approx, '--', color=colors[i], linewidth=2, \n",
    "             label=f'Degree {degree} (error: {error:.4f})')\n",
    "    \n",
    "    print(f\"Degree {degree:2d}: Max error = {error:.6f}\")\n",
    "    \n",
    "    # Print polynomial coefficients\n",
    "    coeffs = poly.coef\n",
    "    print(f\" Polynomial: p(x) = \", end=\"\")\n",
    "    terms = []\n",
    "    for j, coef in enumerate(coeffs):\n",
    "        if abs(coef) > 1e-10: # Skip near-zero coefficients\n",
    "            if j == 0:\n",
    "                terms.append(f\"{coef:.3f}\")\n",
    "            elif j == 1:\n",
    "                terms.append(f\"{coef:+.3f}x\")\n",
    "            else:\n",
    "                terms.append(f\"{coef:+.3f}x^{j}\")\n",
    "    print(\" \".join(terms[:4]) + \" ...\") # Show first few terms\n",
    "    print()\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Polynomial Approximations of $\\\\sin(\\\\pi x)$')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Second subplot: All errors together\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, degree in enumerate(degrees):\n",
    "    poly, error = approximate_with_polynomial(\n",
    "        target_func, [0, 1], degree,\n",
    "        f\"Polynomial Approximation (Degree {degree})\"\n",
    "    )\n",
    "    \n",
    "    # Get error data\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    y = target_func(x)\n",
    "    y_approx = poly(x)\n",
    "    \n",
    "    # Plot error with same color as approximation\n",
    "    plt.plot(x, np.abs(y - y_approx), '-', color=colors[i], linewidth=2, \n",
    "             label=f'Degree {degree}')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('|Error|')\n",
    "plt.title('Approximation Errors Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale for better visualization\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Analysis\n",
    "\n",
    "As we increase the polynomial degree, the approximation improves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot convergence\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(degrees, errors, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Maximum Approximation Error')\n",
    "plt.title('Weierstrass Convergence')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Compare different degrees visually\n",
    "x = np.linspace(0, 1, 200)\n",
    "y_true = target_func(x)\n",
    "plt.plot(x, y_true, 'k-', linewidth=3, label='sin(πx)', alpha=0.8)\n",
    "\n",
    "for i, degree in enumerate([3, 5, 9]):\n",
    "    poly = Polynomial.fit(np.linspace(0, 1, 100), \n",
    "                          target_func(np.linspace(0, 1, 100)), degree)\n",
    "    plt.plot(x, poly(x), '--', linewidth=2, label=f'Degree {degree}', alpha=0.7)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Multiple Polynomial Approximations')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernstein Polynomials: A Constructive Proof\n",
    "\n",
    "One elegant proof of the Weierstrass theorem uses **Bernstein polynomials**, which provide an explicit construction:\n",
    "\n",
    "$$B_n(f; x) = \\sum_{k=0}^{n} f\\left(\\frac{k}{n}\\right) \\binom{n}{k} x^k (1-x)^{n-k}$$\n",
    "\n",
    "These polynomials converge uniformly to $f$ as $n \\to \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bernstein_polynomial(f, n, x):\n",
    "    \"\"\"Compute n-th Bernstein polynomial approximation\"\"\"\n",
    "    from scipy.special import comb\n",
    "    \n",
    "    result = np.zeros_like(x)\n",
    "    for k in range(n + 1):\n",
    "        # Bernstein basis polynomial\n",
    "        basis = comb(n, k) * (x ** k) * ((1 - x) ** (n - k))\n",
    "        # Weight by function value at k/n\n",
    "        result += f(k / n) * basis\n",
    "    return result\n",
    "\n",
    "# Demonstrate Bernstein approximation\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "x = np.linspace(0, 1, 200)\n",
    "y_true = target_func(x)\n",
    "\n",
    "degrees = [5, 10, 20, 40]\n",
    "for i, n in enumerate(degrees):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    y_bernstein = bernstein_polynomial(target_func, n, x)\n",
    "    \n",
    "    plt.plot(x, y_true, 'b-', linewidth=2, label='sin(πx)', alpha=0.7)\n",
    "    plt.plot(x, y_bernstein, 'r--', linewidth=2, label=f'Bernstein n={n}')\n",
    "    \n",
    "    error = np.max(np.abs(y_true - y_bernstein))\n",
    "    plt.title(f'n = {n}\\nError: {error:.4f}')\n",
    "    plt.xlabel('x')\n",
    "    if i == 0:\n",
    "        plt.ylabel('y')\n",
    "    plt.legend(loc='upper right', fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(-1.2, 1.2)\n",
    "\n",
    "plt.suptitle('Bernstein Polynomial Approximation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomials vs Neural Networks: A Direct Comparison\n",
    "\n",
    "Let's compare polynomial approximation with neural network approximation for the same function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Train neural network on the SAME sparse data as polynomial\n",
    "def train_nn(x_train_data, y_train_data, hidden_size=20, epochs=5000):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    x_train_torch = torch.tensor(x_train_data.reshape(-1, 1), dtype=torch.float32)\n",
    "    y_train_torch = torch.tensor(y_train_data.reshape(-1, 1), dtype=torch.float32)\n",
    "    \n",
    "    model = SimpleNN(hidden_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Train with early stopping\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train_torch)\n",
    "        loss = criterion(y_pred, y_train_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter > 500 or loss.item() < 1e-6:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Compare polynomial vs neural network - both trained on SAME 15 noisy points\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Use the same test grid for evaluation\n",
    "x_test = np.linspace(0, 1, 200)\n",
    "y_true = target_func(x_test)\n",
    "\n",
    "# Polynomial approximation on sparse data\n",
    "poly_degree = 10\n",
    "poly = Polynomial.fit(x_train, y_train, poly_degree)  # Use our 15 sparse points\n",
    "y_poly = poly(x_test)\n",
    "poly_params = poly_degree + 1\n",
    "\n",
    "# Neural network approximation on same sparse data\n",
    "nn_hidden = 10\n",
    "model = train_nn(x_train, y_train, nn_hidden, epochs=10000)  # Use same 15 points\n",
    "x_test_torch = torch.tensor(x_test.reshape(-1, 1), dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    y_nn = model(x_test_torch).numpy().flatten()\n",
    "nn_params = 1 * nn_hidden + nn_hidden + nn_hidden * 1 + 1  # weights + biases\n",
    "\n",
    "# Plot comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x_test, y_true, 'k-', linewidth=3, label='Target', alpha=0.8)\n",
    "plt.plot(x_test, y_poly, 'b--', linewidth=2, label=f'Polynomial (deg {poly_degree})')\n",
    "plt.plot(x_test, y_nn, 'r:', linewidth=2, label=f'Neural Net ({nn_hidden} hidden)')\n",
    "plt.scatter(x_train, y_train, color='black', s=30, zorder=5, alpha=0.5, label='Training Data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Function Approximation\\n(Both trained on 15 sparse points)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x_test, np.abs(y_true - y_poly), 'b-', linewidth=2, \n",
    "         label=f'Polynomial ({poly_params} params)')\n",
    "plt.plot(x_test, np.abs(y_true - y_nn), 'r-', linewidth=2, \n",
    "         label=f'Neural Net ({nn_params} params)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('|Error|')\n",
    "plt.title('Approximation Error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "errors_comparison = {\n",
    "    'Polynomial\\n(11 params)': np.max(np.abs(y_true - y_poly)),\n",
    "    'Neural Net\\n(21 params)': np.max(np.abs(y_true - y_nn))\n",
    "}\n",
    "bars = plt.bar(errors_comparison.keys(), errors_comparison.values(), \n",
    "               color=['blue', 'red'], alpha=0.7)\n",
    "plt.ylabel('Maximum Error')\n",
    "plt.title('Error Comparison')\n",
    "for bar, error in zip(bars, errors_comparison.values()):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0001,\n",
    "             f'{error:.4f}', ha='center', fontsize=10)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Fair Comparison: Polynomial vs Neural Network (Both Trained on 15 Sparse Points)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nComparison Summary (both trained on {n_train} sparse noisy points):\")\n",
    "print(f\"Polynomial (degree {poly_degree}):\")\n",
    "print(f\"  Parameters: {poly_params}\")\n",
    "print(f\"  Max Error: {np.max(np.abs(y_true - y_poly)):.6f}\")\n",
    "print(f\"  Train MSE: {np.mean((poly(x_train) - y_train)**2):.6f}\")\n",
    "print(f\"\\nNeural Network ({nn_hidden} hidden units):\")\n",
    "print(f\"  Parameters: {nn_params}\")\n",
    "print(f\"  Max Error: {np.max(np.abs(y_true - y_nn)):.6f}\")\n",
    "# Calculate train MSE for NN\n",
    "x_train_torch_eval = torch.tensor(x_train.reshape(-1, 1), dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(x_train_torch_eval).numpy().flatten()\n",
    "print(f\"  Train MSE: {np.mean((y_train_pred - y_train)**2):.6f}\")\n",
    "print(f\"\\nKey Observation: NN provides smoother interpolation than polynomial on sparse data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Polynomial Approximation\n",
    "\n",
    "While polynomials can theoretically approximate any continuous function, they have practical limitations:\n",
    "\n",
    "1. **Runge's Phenomenon**: High-degree polynomials oscillate wildly at boundaries\n",
    "2. **Global Support**: Changing one coefficient affects the entire function\n",
    "3. **Computational Instability**: High-degree polynomials suffer from numerical issues\n",
    "4. **Poor Extrapolation**: Polynomials diverge rapidly outside the training interval\n",
    "\n",
    "Neural networks address these limitations:\n",
    "- **Local Support**: ReLU networks create piecewise linear approximations\n",
    "- **Stability**: Bounded activations (sigmoid, tanh) prevent divergence\n",
    "- **Compositionality**: Deep networks build complex functions from simple pieces\n",
    "- **Adaptivity**: Networks learn where to place their \"basis functions\"\n",
    "\n",
    "### From Weierstrass to Universal Approximation\n",
    "\n",
    "The progression from Weierstrass to neural networks represents a evolution in approximation theory:\n",
    "\n",
    "1. **1885 - Weierstrass**: Polynomials are universal approximators\n",
    "2. **1989 - Cybenko**: Single-layer neural networks are universal approximators\n",
    "3. **Modern Deep Learning**: Deep networks are exponentially more efficient\n",
    "\n",
    "This historical perspective shows that neural networks are not magical – they're the latest chapter in a long mathematical story about approximating complex functions with simple building blocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Neural Network Approach: Function Approximation\n",
    "\n",
    "Our goal is to train a neural network $u_{NN}(x; \\theta)$ to approximate the continuous solution $u^*(x) = \\sin(\\pi x)$ over the interval $[0, 1]$. This is a **function approximation** problem.\n",
    "\n",
    "The theoretical foundation for this approach is the **Universal Approximation Theorem (UAT)**, which guarantees that neural networks can approximate any continuous function to arbitrary accuracy with enough neurons. *We'll explore UAT in detail later in this notebook.*\n",
    "\n",
    "### Traditional Numerical Method vs Neural Network: Discrete vs Continuous\n",
    "\n",
    "Traditional numerical methods compute discrete values at grid points. In contrast, the Neural Network approach learns a **continuous function** $u_{NN}(x; \\theta)$ that approximates the true solution $u^*(x)$ over the entire domain. \n",
    "\n",
    "- This function is parameterized by the network's weights and biases $\\theta$\n",
    "- We train by adjusting $\\theta$ so the network's output matches training data\n",
    "- Once trained, we can evaluate the solution at any point, not just grid points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perceptron: Building Block of Neural Networks\n",
    "\n",
    "A perceptron is a linear transformation followed by an activation function:\n",
    "$$\\hat{y} = g(\\mathbf{w}^T\\mathbf{x} + b)$$\n",
    "\n",
    "We'll start with the simplest case: no activation (linear perceptron), implement training from scratch, then add nonlinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate linearly separable data\n",
    "np.random.seed(42)\n",
    "n = 50\n",
    "\n",
    "# Two classes\n",
    "X0 = np.random.randn(n, 2) * 0.5 + [-2, -2]\n",
    "X1 = np.random.randn(n, 2) * 0.5 + [2, 2]\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.hstack([np.zeros(n), np.ones(n)])\n",
    "\n",
    "# Visualize just the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X0[:, 0], X0[:, 1], c='blue', s=50, alpha=0.7, label='Class 0', edgecolors='darkblue')\n",
    "plt.scatter(X1[:, 0], X1[:, 1], c='red', s=50, alpha=0.7, label='Class 1', edgecolors='darkred')\n",
    "\n",
    "plt.xlabel('$x_1$', fontsize=12)\n",
    "plt.ylabel('$x_2$', fontsize=12)\n",
    "plt.title('Linearly Separable Data', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some basic statistics about the data\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Class 0: {len(X0)} samples\")\n",
    "print(f\"Class 1: {len(X1)} samples\")\n",
    "print(f\"\\nClass 0 center: [{X0.mean(axis=0)[0]:.2f}, {X0.mean(axis=0)[1]:.2f}]\")\n",
    "print(f\"Class 1 center: [{X1.mean(axis=0)[0]:.2f}, {X1.mean(axis=0)[1]:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Perceptron in NumPy\n",
    "\n",
    "First, a pure linear model: $\\hat{y} = \\mathbf{w}^T\\mathbf{x} + b$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LinearPerceptron:\n",
    "    def __init__(self, dim):\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute output: y = w^T x + b\"\"\"\n",
    "        \n",
    "    \n",
    "    def predict_batch(self, X):\n",
    "        \"\"\"Predict for multiple samples\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training: Gradient Descent from Scratch\n",
    "\n",
    "To train, we minimize the mean squared error loss:\n",
    "$$L = \\frac{1}{2N}\\sum_{i=1}^N (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Using calculus, we derive the gradients:\n",
    "\n",
    "For a single sample with prediction $\\hat{y} = \\mathbf{w}^T\\mathbf{x} + b$ and loss $L = \\frac{1}{2}(y - \\hat{y})^2$:\n",
    "\n",
    "**Gradient w.r.t weights:**\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{w}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial \\mathbf{w}} = -(y - \\hat{y}) \\cdot \\mathbf{x}$$\n",
    "\n",
    "**Gradient w.r.t bias:**\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial b} = -(y - \\hat{y})$$\n",
    "\n",
    "The update rule:\n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta(y - \\hat{y})\\mathbf{x}$$\n",
    "$$b \\leftarrow b + \\eta(y - \\hat{y})$$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_perceptron(model, X, y, lr=0.01, epochs=100):\n",
    "    \"\"\"Train using gradient descent\"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass for all samples\n",
    "        \n",
    "        \n",
    "        # Compute loss\n",
    "        \n",
    "        \n",
    "        # Compute gradients (vectorized)\n",
    "        \n",
    "        \n",
    "        # Update parameters\n",
    "        \n",
    "        \n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            print(f\"Epoch {epoch+1}: loss={loss:.4f}\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Linear Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linearly separable data\n",
    "np.random.seed(42)\n",
    "n = 50\n",
    "\n",
    "# Two classes\n",
    "X0 = np.random.randn(n, 2) * 0.5 + [-2, -2]\n",
    "X1 = np.random.randn(n, 2) * 0.5 + [2, 2]\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.hstack([np.zeros(n), np.ones(n)])\n",
    "\n",
    "# Train linear perceptron\n",
    "model = LinearPerceptron(2)\n",
    "losses = train_linear_perceptron(model, X, y, lr=0.1, epochs=100)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Loss curve\n",
    "ax1.plot(losses)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE Loss')\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n",
    "Z = model.predict_batch(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "ax2.contour(xx, yy, Z, levels=[0.5], colors='g', linewidths=2)\n",
    "ax2.contourf(xx, yy, Z, levels=[-1, 0.5, 2], alpha=0.3, colors=['blue', 'red'])\n",
    "ax2.scatter(X0[:,0], X0[:,1], c='b', s=30, label='Class 0')\n",
    "ax2.scatter(X1[:,0], X1[:,1], c='r', s=30, label='Class 1')\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_2$')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLearned parameters: w={model.w}, b={model.b:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation: Non-linear Patterns\n",
    "\n",
    "Linear perceptrons fail on non-linearly separable data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate XOR-like pattern (not linearly separable)\n",
    "# Similar to the data in relu.md visualization\n",
    "np.random.seed(42)\n",
    "\n",
    "# Class 0 - diagonal pattern (magenta points)\n",
    "class0_X = np.array([\n",
    "    [-2.75, 0.27], [-3.63, 1.20], [-2.51, 1.95], [-1.85, 3.02], \n",
    "    [-0.81, 2.54], [0.03, 3.28], [1.82, 3.23], [3.37, 2.48], \n",
    "    [4.76, 1.96], [4.74, 0.82], [3.22, 1.02], [0.38, 1.22],\n",
    "    [-0.62, -0.04], [0.52, -0.44], [1.72, -0.31], [2.29, -1.63], \n",
    "    [0.87, -1.84], [-0.87, -1.52]\n",
    "])\n",
    "\n",
    "# Class 1 - upper band pattern (gold points)\n",
    "class1_X = np.array([\n",
    "    [-5.33, 2.15], [-4.88, 3.79], [-3.99, 3.16], [-2.98, 4.30], \n",
    "    [-1.91, 6.07], [-1.06, 4.89], [0.78, 5.01], [-0.22, 6.47], \n",
    "    [1.43, 6.11], [2.98, 4.41], [4.50, 3.61], [5.13, 4.95], \n",
    "    [6.37, 3.01]\n",
    "])\n",
    "\n",
    "# Add some noise and additional points to make it more challenging\n",
    "noise_scale = 0.3\n",
    "class0_X += np.random.normal(0, noise_scale, class0_X.shape)\n",
    "class1_X += np.random.normal(0, noise_scale, class1_X.shape)\n",
    "\n",
    "# Combine data\n",
    "X_nonlinear = np.vstack([class0_X, class1_X])\n",
    "y_nonlinear = np.hstack([np.zeros(len(class0_X)), np.ones(len(class1_X))])\n",
    "\n",
    "# Normalize features for better training\n",
    "X_mean = X_nonlinear.mean(axis=0)\n",
    "X_std = X_nonlinear.std(axis=0)\n",
    "X_nonlinear_norm = (X_nonlinear - X_mean) / X_std\n",
    "\n",
    "# Try to fit with linear perceptron\n",
    "model_linear = LinearPerceptron(2)\n",
    "losses_linear = train_linear_perceptron(model_linear, X_nonlinear_norm, y_nonlinear, lr=0.1, epochs=100)\n",
    "\n",
    "# Visualize failure of linear model\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(losses_linear, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Not Converging to Zero')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, max(losses_linear) * 1.1)\n",
    "\n",
    "# Plot 2: Original space with linear boundary\n",
    "plt.subplot(1, 3, 2)\n",
    "# Create mesh for decision boundary\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X_nonlinear[:,0].min()-1, X_nonlinear[:,0].max()+1, 100),\n",
    "    np.linspace(X_nonlinear[:,1].min()-1, X_nonlinear[:,1].max()+1, 100)\n",
    ")\n",
    "# Normalize mesh points\n",
    "mesh_norm = (np.c_[xx.ravel(), yy.ravel()] - X_mean) / X_std\n",
    "Z_linear = model_linear.predict_batch(mesh_norm).reshape(xx.shape)\n",
    "\n",
    "plt.contour(xx, yy, Z_linear, levels=[0.5], colors='g', linewidths=2, linestyles='--')\n",
    "plt.scatter(class0_X[:,0], class0_X[:,1], c='magenta', s=50, edgecolor='purple', \n",
    "           linewidth=1.5, label='Class 0', alpha=0.7)\n",
    "plt.scatter(class1_X[:,0], class1_X[:,1], c='gold', s=50, edgecolor='orange', \n",
    "           linewidth=1.5, label='Class 1', alpha=0.7)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Linear Boundary Fails')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Accuracy over epochs\n",
    "plt.subplot(1, 3, 3)\n",
    "# Calculate accuracy at each epoch\n",
    "accuracies = []\n",
    "model_temp = LinearPerceptron(2)\n",
    "for epoch in range(100):\n",
    "    train_linear_perceptron(model_temp, X_nonlinear_norm, y_nonlinear, lr=0.1, epochs=1)\n",
    "    predictions = (model_temp.predict_batch(X_nonlinear_norm) > 0.5).astype(int)\n",
    "    accuracy = np.mean(predictions == y_nonlinear)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "plt.plot(accuracies, 'r-', linewidth=2)\n",
    "plt.axhline(y=1.0, color='g', linestyle=':', alpha=0.5, label='Perfect accuracy')\n",
    "plt.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5, label='Random guess')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Classification Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1.05)\n",
    "\n",
    "plt.suptitle('Linear Perceptron Cannot Solve Non-Linearly Separable Data', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses_linear[-1]:.4f}\")\n",
    "print(f\"Final accuracy: {accuracies[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Adding Non-linear Activation\n",
    "\n",
    "The linear perceptron fails because the data is not linearly separable. By adding a hidden layer with non-linear activation (ReLU), we can transform the input space into a feature space where the data becomes linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive Demo: Visualizing Nonlinear Transformation\n",
    "\n",
    "This interactive demo illustrates how a combination of linear transformation and non-linearity can transform data in a way that linear transformations alone cannot. Observe how the data, initially not linearly separable in the input space (X), becomes separable after passing through a linear layer (Y) and then a non-linear activation (Z).\n",
    "\n",
    "This provides intuition for why layers with non-linear activations are powerful: they can map data into a new space where complex patterns become simpler (potentially linearly separable), making them learnable by subsequent layers.\n",
    "\n",
    "[![Button](https://img.shields.io/badge/Go%20to-Interactive%20Demo:ReLU-blue?style=for-the-badge)](../relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-layer network with just 2 hidden units (matching relu.md solution)\n",
    "class TwoNeuronNetwork:\n",
    "    def __init__(self):\n",
    "        # Initialize with rotation-like pattern (based on relu.md solution: -59 degrees, scale 2)\n",
    "        angle = -59 * np.pi / 180\n",
    "        scale = 2.0\n",
    "        \n",
    "        # First layer: linear transformation (rotation + scaling)\n",
    "        self.W1 = np.array([\n",
    "            [np.cos(angle) * scale, -np.sin(angle) * scale],\n",
    "            [np.sin(angle) * scale, np.cos(angle) * scale]\n",
    "        ]).T  # Transpose to match input @ W1 convention\n",
    "        self.b1 = np.zeros(2)\n",
    "        \n",
    "        # Output layer: combine the two ReLU features\n",
    "        self.W2 = np.random.randn(2, 1) * 0.1\n",
    "        self.b2 = np.zeros(1)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # First layer: linear transformation\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        # Apply ReLU activation\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        # Output layer\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def train(self, X, y, epochs=500, lr=0.5):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            eps = 1e-7\n",
    "            output_clipped = np.clip(output.flatten(), eps, 1 - eps)\n",
    "            loss = -np.mean(y * np.log(output_clipped) + \n",
    "                           (1 - y) * np.log(1 - output_clipped))\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            m = len(X)\n",
    "            \n",
    "            # Output layer gradients\n",
    "            dz2 = self.a2 - y.reshape(-1, 1)\n",
    "            dW2 = (self.a1.T @ dz2) / m\n",
    "            db2 = np.mean(dz2, axis=0)\n",
    "            \n",
    "            # Only update output layer (keep transformation fixed for demonstration)\n",
    "            self.W2 -= lr * dW2\n",
    "            self.b2 -= lr * db2\n",
    "            \n",
    "            # Optional: also train first layer after initial epochs\n",
    "            if epoch > 100:\n",
    "                da1 = dz2 @ self.W2.T\n",
    "                dz1 = da1 * self.relu_derivative(self.z1)\n",
    "                dW1 = (X.T @ dz1) / m\n",
    "                db1 = np.mean(dz1, axis=0)\n",
    "                \n",
    "                self.W1 -= lr * 0.01 * dW1  # Small learning rate for first layer\n",
    "                self.b1 -= lr * 0.01 * db1\n",
    "                \n",
    "        return losses\n",
    "\n",
    "# Train the two-neuron network\n",
    "np.random.seed(42)\n",
    "model_twoneuron = TwoNeuronNetwork()\n",
    "losses_twoneuron = model_twoneuron.train(X_nonlinear_norm, y_nonlinear, epochs=500, lr=1.0)\n",
    "\n",
    "# Also train a properly initialized multi-neuron network for comparison\n",
    "class OptimizedNetwork:\n",
    "    def __init__(self, input_dim=2, hidden_dim=4):\n",
    "        # Better initialization\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.5\n",
    "        self.b1 = np.random.randn(hidden_dim) * 0.1\n",
    "        self.W2 = np.random.randn(hidden_dim, 1) * 0.5\n",
    "        self.b2 = np.zeros(1)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        return self.sigmoid(self.z2)\n",
    "    \n",
    "    def train(self, X, y, epochs=500, lr=0.5):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # Forward\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Loss\n",
    "            eps = 1e-7\n",
    "            output_clipped = np.clip(output.flatten(), eps, 1 - eps)\n",
    "            loss = -np.mean(y * np.log(output_clipped) + \n",
    "                           (1 - y) * np.log(1 - output_clipped))\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward\n",
    "            m = len(X)\n",
    "            dz2 = output - y.reshape(-1, 1)\n",
    "            dW2 = (self.a1.T @ dz2) / m\n",
    "            db2 = np.mean(dz2, axis=0)\n",
    "            \n",
    "            da1 = dz2 @ self.W2.T\n",
    "            dz1 = da1 * (self.z1 > 0)\n",
    "            dW1 = (X.T @ dz1) / m\n",
    "            db1 = np.mean(dz1, axis=0)\n",
    "            \n",
    "            # Update with momentum\n",
    "            self.W2 -= lr * dW2\n",
    "            self.b2 -= lr * db2\n",
    "            self.W1 -= lr * dW1\n",
    "            self.b1 -= lr * db1\n",
    "            \n",
    "        return losses\n",
    "\n",
    "model_optimized = OptimizedNetwork(2, 4)\n",
    "losses_optimized = model_optimized.train(X_nonlinear_norm, y_nonlinear, epochs=500, lr=0.5)\n",
    "\n",
    "# Visualize all three models\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Row 1: Loss curves\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(losses_linear, 'b-', label='Linear', alpha=0.7)\n",
    "plt.plot(losses_twoneuron, 'g-', label='2 Hidden Units', alpha=0.7)\n",
    "plt.plot(losses_optimized, 'r-', label='4 Hidden Units', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Row 1: Decision boundaries for each model\n",
    "models = [\n",
    "    (model_linear, 'Linear Model', plt.subplot(2, 3, 2)),\n",
    "    (model_twoneuron, '2 Hidden Units (Like relu.md)', plt.subplot(2, 3, 3))\n",
    "]\n",
    "\n",
    "for model, title, ax in models:\n",
    "    plt.sca(ax)\n",
    "    if hasattr(model, 'predict_batch'):\n",
    "        Z = model.predict_batch(mesh_norm).reshape(xx.shape)\n",
    "    else:\n",
    "        Z = model.forward(mesh_norm).reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.4)\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='g', linewidths=2)\n",
    "    plt.scatter(class0_X[:,0], class0_X[:,1], c='magenta', s=30, \n",
    "               edgecolor='purple', linewidth=1, alpha=0.7)\n",
    "    plt.scatter(class1_X[:,0], class1_X[:,1], c='gold', s=30, \n",
    "               edgecolor='orange', linewidth=1, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Feature space visualization\n",
    "plt.subplot(2, 3, 4)\n",
    "# Visualize the 2-neuron hidden layer features\n",
    "hidden_2neuron = model_twoneuron.relu(X_nonlinear_norm @ model_twoneuron.W1 + model_twoneuron.b1)\n",
    "plt.scatter(hidden_2neuron[y_nonlinear==0, 0], hidden_2neuron[y_nonlinear==0, 1], \n",
    "           c='magenta', s=30, edgecolor='purple', linewidth=1, alpha=0.7, label='Class 0')\n",
    "plt.scatter(hidden_2neuron[y_nonlinear==1, 0], hidden_2neuron[y_nonlinear==1, 1], \n",
    "           c='gold', s=30, edgecolor='orange', linewidth=1, alpha=0.7, label='Class 1')\n",
    "plt.xlabel('Hidden Unit 1')\n",
    "plt.ylabel('Hidden Unit 2')\n",
    "plt.title('2-Neuron Feature Space (After ReLU)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "Z_optimized = model_optimized.forward(mesh_norm).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z_optimized, levels=20, cmap='RdBu', alpha=0.4)\n",
    "plt.contour(xx, yy, Z_optimized, levels=[0.5], colors='g', linewidths=2)\n",
    "plt.scatter(class0_X[:,0], class0_X[:,1], c='magenta', s=30, \n",
    "           edgecolor='purple', linewidth=1, alpha=0.7)\n",
    "plt.scatter(class1_X[:,0], class1_X[:,1], c='gold', s=30, \n",
    "           edgecolor='orange', linewidth=1, alpha=0.7)\n",
    "plt.title('4 Hidden Units')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Accuracy comparison\n",
    "plt.subplot(2, 3, 6)\n",
    "linear_acc = np.mean((model_linear.predict_batch(X_nonlinear_norm) > 0.5) == y_nonlinear)\n",
    "twoneuron_acc = np.mean((model_twoneuron.forward(X_nonlinear_norm).flatten() > 0.5) == y_nonlinear)\n",
    "optimized_acc = np.mean((model_optimized.forward(X_nonlinear_norm).flatten() > 0.5) == y_nonlinear)\n",
    "\n",
    "bars = plt.bar(['Linear', '2 Hidden', '4 Hidden'], \n",
    "               [linear_acc, twoneuron_acc, optimized_acc],\n",
    "               color=['blue', 'green', 'red'], alpha=0.7)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Final Accuracy Comparison')\n",
    "plt.ylim(0, 1.1)\n",
    "for bar, acc in zip(bars, [linear_acc, twoneuron_acc, optimized_acc]):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{acc:.1%}', ha='center', fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('ReLU Activation Enables Non-linear Classification', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Linear model: {linear_acc:.1%} accuracy\")\n",
    "print(f\"2-neuron model: {twoneuron_acc:.1%} accuracy\") \n",
    "print(f\"4-neuron model: {optimized_acc:.1%} accuracy\")\n",
    "print(f\"\\nFinal losses - Linear: {losses_linear[-1]:.4f}, 2-neuron: {losses_twoneuron[-1]:.4f}, 4-neuron: {losses_optimized[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation with Activation Functions\n",
    "\n",
    "To handle non-linear patterns, we add an activation function $g$:\n",
    "$$\\hat{y} = g(\\mathbf{w}^T\\mathbf{x} + b)$$\n",
    "\n",
    "Common choices:\n",
    "- **Sigmoid**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ — Outputs in (0,1)\n",
    "- **ReLU**: $\\text{ReLU}(z) = \\max(0, z)$ — Simple and efficient\n",
    "- **Tanh**: $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ — Outputs in (-1,1)\n",
    "\n",
    "With activation function $g$, the forward pass becomes:\n",
    "1. Linear: $z = \\mathbf{w}^T\\mathbf{x} + b$\n",
    "2. Activation: $\\hat{y} = g(z)$\n",
    "\n",
    "For backpropagation, we use the chain rule. Given loss $L = \\frac{1}{2}(y - \\hat{y})^2$:\n",
    "\n",
    "**Step 1: Error at output**\n",
    "$$\\delta = \\frac{\\partial L}{\\partial \\hat{y}} = -(y - \\hat{y})$$\n",
    "\n",
    "**Step 2: Error before activation**\n",
    "$$\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} = \\delta \\cdot g'(z)$$\n",
    "\n",
    "**Step 3: Gradients**\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{w}} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\mathbf{w}} = \\delta \\cdot g'(z) \\cdot \\mathbf{x}$$\n",
    "$$\\frac{\\partial L}{\\partial b} = \\delta \\cdot g'(z)$$\n",
    "\n",
    "This is what the `backward` method implements:\n",
    "```python\n",
    "def backward(self, y_true):\n",
    "    error = self.a - y_true           # δ = ŷ - y\n",
    "    dz = error * self.g_prime(self.z) # δ * g'(z)\n",
    "    self.w -= self.lr * dz * self.x   # w -= η * ∂L/∂w\n",
    "    self.b -= self.lr * dz             # b -= η * ∂L/∂b\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \n",
    "    return \n",
    "\n",
    "# Generate data\n",
    "z = np.linspace(-10, 10, 1000)\n",
    "sigmoid_vals = sigmoid(z)\n",
    "gradient_vals = sigmoid_prime(z)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, sigmoid_vals, 'b-', linewidth=2, label='Sigmoid σ(z)')\n",
    "plt.plot(z, gradient_vals, 'r-', linewidth=2, label=\"Sigmoid Gradient σ'(z)\")\n",
    "plt.xlabel('Input (z)')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Sigmoid Function and Its Gradient')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, dim, lr=0.01):\n",
    "        self.w = np.random.randn(dim) * 0.01\n",
    "        self.b = 0.0\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        # Compute gradients using chain rule\n",
    "        \n",
    "        \n",
    "        # Update parameters\n",
    "        \n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Critical Role of Nonlinearity\n",
    "\n",
    "Why do we need the activation function $g$? What happens if we just use a linear function, like $g(z) = z$?\n",
    "\n",
    "Consider a network with multiple layers, but *no* non-linear activation functions between them. The output of one layer is just a linear transformation of its input. If we stack these linear layers:\n",
    "\n",
    "Let the first layer be $h_1 = W_1 x + b_1$.\n",
    "Let the second layer be $h_2 = W_2 h_1 + b_2$.\n",
    "\n",
    "Substituting the first into the second:\n",
    "$$h_2 = W_2 (W_1 x + b_1) + b_2$$\n",
    "$$h_2 = W_2 W_1 x + W_2 b_1 + b_2$$\n",
    "\n",
    "This can be rewritten as:\n",
    "$$h_2 = (W_2 W_1) x + (W_2 b_1 + b_2)$$\n",
    "\n",
    "Let $W_{eq} = W_2 W_1$ and $b_{eq} = W_2 b_1 + b_2$. Then:\n",
    "$$h_2 = W_{eq} x + b_{eq}$$\n",
    "\n",
    "This is just another linear transformation! No matter how many linear layers we stack, the entire network will only be able to compute a single linear function of the input. A linear network can only learn:\n",
    "\n",
    "> Linear network can only learn: y = mx + b (a straight line in 1D)\n",
    "> But sin(πx) is curved - impossible with just linear transformations!\n",
    "\n",
    "To approximate complex, non-linear functions like $\\sin(\\pi x)$, we **must** introduce non-linearity using activation functions between the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Linear Network Failure\n",
    "class LinearNetwork(nn.Module):\n",
    "    \"\"\"A simple network with only linear layers (no activation)\"\"\"\n",
    "    def __init__(self, width):\n",
    "        super().__init__()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return \n",
    "\n",
    "# Prepare training data as tensors (using both naming conventions for compatibility)\n",
    "x_train_tensor = torch.FloatTensor(x_train.reshape(-1, 1))\n",
    "y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1))\n",
    "u_train_tensor = y_train_tensor  # Alias for compatibility with later cells\n",
    "\n",
    "# Also create test tensors for evaluation\n",
    "x_test_tensor = torch.FloatTensor(x_test.reshape(-1, 1))\n",
    "y_test_tensor = torch.FloatTensor(y_exact.reshape(-1, 1))\n",
    "u_test_tensor = y_test_tensor  # Alias for compatibility\n",
    "\n",
    "# Create a simple linear model (no hidden layer, just for comparison)\n",
    "linear_model =\n",
    "criterion = \n",
    "optimizer = \n",
    "\n",
    "epochs = 3000\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.6f}\")\n",
    "\n",
    "# Test linear model\n",
    "linear_pred = linear_model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "# Compare linear vs nonlinear data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_test, y_exact, 'b-', linewidth=2, label='True (Nonlinear)')\n",
    "plt.plot(x_test, linear_pred, 'r--', linewidth=2, label='Linear Model')\n",
    "plt.scatter(x_train, y_train, s=50, c='black', zorder=5, alpha=0.5, label='Training Data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')\n",
    "plt.title('Linear Model Fails on Nonlinear Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Linear model can only fit a straight line to nonlinear data!\")\n",
    "print(f\"Final MSE: {np.mean((linear_pred - y_exact)**2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear Activation Functions\n",
    "\n",
    "Activation functions are applied element-wise to the output of a linear transformation within a neuron or layer. They introduce the non-linearity required for neural networks to learn complex mappings.\n",
    "\n",
    "Some common activation functions include:\n",
    "\n",
    "**Sigmoid**\n",
    "Squashes input to (0, 1). Useful for binary classification output. Can suffer from vanishing gradients.\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "**Tanh**\n",
    "Squashes input to (-1, 1). Zero-centered, often preferred over Sigmoid for hidden layers. Can also suffer from vanishing gradients.\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**\n",
    "Outputs input directly if positive, zero otherwise. Computationally efficient, helps mitigate vanishing gradients for positive inputs. Can suffer from \"dead neurons\" if inputs are always negative.\n",
    "$$ f(x) = \\max(0, x)$$\n",
    "\n",
    "**LeakyReLU**\n",
    "Similar to ReLU but allows a small gradient for negative inputs, preventing dead neurons.\n",
    "$$f(x) = \\max(\\alpha x, x) \\quad (\\alpha \\text{ is a small positive constant, e.g., 0.01})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Parameterized Activation Functions ---\n",
    "\n",
    "def sigmoid(x, a=1.0):\n",
    "    \"\"\"\n",
    "    Parameterized Sigmoid activation function.\n",
    "    'a' controls the steepness.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-a * x))\n",
    "\n",
    "def tanh(x, a=1.0):\n",
    "    \"\"\"\n",
    "    Parameterized Hyperbolic Tangent activation function.\n",
    "    'a' controls the steepness.\n",
    "    \"\"\"\n",
    "    return np.tanh(a * x)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (ReLU) activation function.\n",
    "    It has no parameters.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Parameterized Leaky ReLU activation function.\n",
    "    'alpha' is the slope for negative inputs.\n",
    "    \"\"\"\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "# --- 2. Setup for Plotting ---\n",
    "\n",
    "# Input data range\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Create a 2x2 subplot grid\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Common Activation Functions (Parameterized)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# --- 3. Plotting each function on its subplot ---\n",
    "\n",
    "# Sigmoid Plot (Top-Left)\n",
    "axs[0, 0].plot(x, sigmoid(x, a=1), label='a=1 (Standard)')\n",
    "axs[0, 0].plot(x, sigmoid(x, a=2), label='a=2 (Steeper)', linestyle='--')\n",
    "axs[0, 0].plot(x, sigmoid(x, a=0.5), label='a=0.5 (Less Steep)', linestyle=':')\n",
    "axs[0, 0].set_title('Sigmoid Function')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Tanh Plot (Top-Right)\n",
    "axs[0, 1].plot(x, tanh(x, a=1), label='a=1 (Standard)')\n",
    "axs[0, 1].plot(x, tanh(x, a=2), label='a=2 (Steeper)', linestyle='--')\n",
    "axs[0, 1].plot(x, tanh(x, a=0.5), label='a=0.5 (Less Steep)', linestyle=':')\n",
    "axs[0, 1].set_title('Tanh Function')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# ReLU Plot (Bottom-Left)\n",
    "axs[1, 0].plot(x, relu(x), label='ReLU')\n",
    "axs[1, 0].set_title('ReLU Function')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Leaky ReLU Plot (Bottom-Right)\n",
    "axs[1, 1].plot(x, leaky_relu(x, alpha=0.1), label='α=0.1 (Standard)')\n",
    "axs[1, 1].plot(x, leaky_relu(x, alpha=0.3), label='α=0.3', linestyle='--')\n",
    "axs[1, 1].plot(x, leaky_relu(x, alpha=0.01), label='α=0.01', linestyle=':')\n",
    "axs[1, 1].set_title('Leaky ReLU Function')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# --- 4. Final Touches for all subplots ---\n",
    "\n",
    "# Apply common labels, grids, and axis lines to all subplots\n",
    "for ax in axs.flat:\n",
    "    ax.set_xlabel('Input z')\n",
    "    ax.set_ylabel('Output g(z)')\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.8)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.8)\n",
    "\n",
    "# Adjust layout to prevent titles and labels from overlapping\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Capacity: The Single Hidden Layer Neural Network\n",
    "\n",
    "A single perceptron is limited in the complexity of functions it can represent. To increase capacity, we combine multiple perceptrons into a **layer**. A single-layer feedforward neural network (also known as a shallow Multi-Layer Perceptron or MLP) consists of an input layer, one hidden layer of neurons, and an output layer.\n",
    "\n",
    "For our 1D input $x$, a single-layer network with $N_h$ hidden neurons works as follows:\n",
    "\n",
    "1.  **Input Layer**: Receives the input $x$.\n",
    "2.  **Hidden Layer**: Each of the $N_h$ neurons in this layer performs a linear transformation on the input $x$ and applies a non-linear activation function $g$. The output of this layer is a vector $\\boldsymbol{h}$ of size $N_h$.\n",
    "    *   Pre-activation vector $\\boldsymbol{z}^{(1)}$ (size $N_h$): $\\boldsymbol{z}^{(1)} = W^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)}$\n",
    "        (Here, $W^{(1)}$ is a $N_h \\times 1$ weight matrix, $\\boldsymbol{x}$ is treated as a $1 \\times 1$ vector, and $\\boldsymbol{b}^{(1)}$ is a $N_h \\times 1$ bias vector).\n",
    "    *   Activation vector $\\boldsymbol{h}$ (size $N_h$): $\\boldsymbol{h} = g(\\boldsymbol{z}^{(1)})$ (where $g$ is applied element-wise).\n",
    "3.  **Output Layer**: This layer takes the vector $\\boldsymbol{h}$ from the hidden layer and performs another linear transformation to produce the final scalar output $\\hat{y}$. For regression, the output layer typically has a linear activation (or no activation function explicitly applied after the linear transformation).\n",
    "    *   Pre-activation scalar $z^{(2)}$: $z^{(2)} = W^{(2)}\\boldsymbol{h} + b^{(2)}$\n",
    "        (Here, $W^{(2)}$ is a $1 \\\\times N_h$ weight matrix, and $b^{(2)}$ is a scalar bias).\n",
    "    *   Final output $\\hat{y}$: $\\hat{y} = z^{(2)}$\n",
    "\n",
    "![Single layer NN](figs/single-layer-nn2.png?raw=true)\n",
    "\n",
    "> Credits: Alexander Amini, MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Single-Layer NN in PyTorch\n",
    "\n",
    "class SingleLayerNN(nn.Module):\n",
    "    \"\"\"Single hidden layer neural network for 1D input/output\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size=10):\n",
    "        super(SingleLayerNN, self).__init__()\n",
    "        \n",
    "        # Input layer (1D) to Hidden layer (hidden_size)\n",
    "       \n",
    "        \n",
    "        # Hidden layer (hidden_size) to Output layer (1D)\n",
    "         \n",
    "        # Choose activation function for the hidden layer\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass through hidden layer and apply activation\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Pass through output layer (linear output)\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's try to plot before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for compatibility with legacy code\n",
    "x_plot = x_test  # Use the test grid we already defined\n",
    "u_analytical_plot = y_exact  # The true function values\n",
    "x_train_np = x_train  # Training x values\n",
    "u_train_noisy_np = y_train  # Training y values with noise\n",
    "\n",
    "# Define SingleLayerNN if not already defined\n",
    "class SingleLayerNN(nn.Module):\n",
    "    \"\"\"Single hidden layer neural network for 1D input/output\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size=10):\n",
    "        super(SingleLayerNN, self).__init__()\n",
    "        self.hidden = nn.Linear(1, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.hidden(x))\n",
    "        return self.output(x)\n",
    "\n",
    "model = SingleLayerNN(10)\n",
    "\n",
    "# Visualize the untrained network's prediction\n",
    "x_test_tensor_plot = torch.tensor(x_plot.reshape(-1, 1), dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    u_pred_linear = model(x_test_tensor_plot).numpy().flatten()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ax.plot(x_plot, u_analytical_plot, 'b-', linewidth=3, label='True Function: $\\\\sin(\\\\pi x)$', alpha=0.7)\n",
    "ax.plot(x_plot, u_pred_linear, 'r--', linewidth=2, label='Untrained NN (Random Weights)')\n",
    "ax.scatter(x_train_np, u_train_noisy_np, color='k', s=40, alpha=0.7, label='Training Data', zorder=5)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('u(x)', fontsize=12)\n",
    "ax.set_title('Untrained Neural Network with Random Initialization', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: The untrained network produces random output due to random weight initialization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network\n",
    "\n",
    "Neural networks are trained using an optimization algorithm that iteratively updates the network's weights and biases to minimize a loss function. The loss function measures how far the network's predictions are from the true target outputs in the training data. It is a measure of the model's error.\n",
    "\n",
    "We quantify this difference using a **Loss Function**, Some common loss functions include:\n",
    "\n",
    "* Mean squared error (MSE) - The average of the squared differences between the predicted and actual values. Measures the square of the error. Used for regression problems.\n",
    "\n",
    "* Cross-entropy loss - Measures the divergence between the predicted class probabilities and the true distribution. Used for classification problems. Penalizes confident incorrect predictions.\n",
    "\n",
    "* Hinge loss - Used for Support Vector Machines classifiers. Penalizes predictions that are on the wrong side of the decision boundary.\n",
    "\n",
    "For our function approximation (regression) task, the Mean Squared Error (MSE) is a common choice:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\left(u_{NN}(x_i; \\theta) - u_i\\right)^2$$\n",
    "\n",
    "Minimizing this loss function with respect to the parameters $\\theta$ is an optimization problem.\n",
    "\n",
    "\n",
    "Loss optimization is the process of finding the network weights that acheives the lowest loss.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\boldsymbol{w^*} &= \\argmin_{\\boldsymbol{w}}\\frac{1}{n}\\sum_{i=1}^n \\mathcal{L}(f(x^{(i)};\\boldsymbol{w}),y^{(i)})\\\\\n",
    "\\boldsymbol{w^*} &= \\argmin_{\\boldsymbol{w}} J(\\boldsymbol{w})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The training process works like this:\n",
    "\n",
    "1. **Initialization**: The weights and biases of the network are initialized, often with small random numbers.\n",
    "\n",
    "2. **Forward Pass**: The input is passed through the network, layer by layer, applying the necessary transformations (e.g., linear combinations of weights and inputs followed by activation functions) until an output is obtained.\n",
    "\n",
    "3. **Calculate Loss**: A loss function is used to quantify the difference between the predicted output and the actual target values.\n",
    "\n",
    "4. **Backward Pass (Backpropagation)**: The gradients of the loss with respect to the parameters (weights and biases) are computed using the chain rule for derivatives. This process is known as backpropagation.\n",
    "\n",
    "5. **Update Parameters**: The gradients computed in the backward pass are used to update the parameters of the network, typically using optimization algorithms like stochastic gradient descent (SGD) or more sophisticated ones like Adam. The update is done in the direction that minimizes the loss.\n",
    "\n",
    "6. **Repeat**: Steps 2-5 are repeated using the next batch of data until a stopping criterion is met, such as a set number of epochs (full passes through the training dataset) or convergence to a minimum loss value.\n",
    "\n",
    "7. **Validation**: The model is evaluated on a separate validation set to assess its generalization to unseen data.\n",
    "\n",
    "The goal of training is to find the optimal set of weights and biases $\\theta^*$ for the network that minimize the difference between the network's output $u_{NN}(x; \\theta)$ and the true training data $u_{train}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing gradients with Automatic Differentiation\n",
    "\n",
    "> The Core Insight: Functions Are Computational Graphs\n",
    "\n",
    "Every computer program that evaluates a mathematical function can be viewed as a **computational graph**. Consider this simple function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Button](https://img.shields.io/badge/Go%20to-Interactive%20Demo-blue?style=for-the-badge&logo=airplayvideo&logoColor=white)](../ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1, x2):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a computational graph where each operation is a node. This decomposition is the key insight that makes automatic differentiation possible.\n",
    "\n",
    "![AD forward pass](figs/ad3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Mode Automatic Differentiation\n",
    "\n",
    "Forward mode AD computes derivatives by propagating derivative information **forward** through the computational graph, following the same path as the function evaluation.\n",
    "\n",
    "![AD forward evaluation](figs/forward-mode-ad.png)\n",
    "\n",
    "#### Forward Mode: Computing $\\frac{\\partial y}{\\partial x_1}$\n",
    "\n",
    "Starting with our function $y = x_1^2 + x_2$, let's trace through the computation:\n",
    "\n",
    "1. **Seed the input**: Set $\\dot{x}_1 = 1$ and $\\dot{x}_2 = 0$ (we're differentiating w.r.t. $x_1$)\n",
    "\n",
    "2. **Forward propagation**:\n",
    "   - $v_1 = x_1^2$, so $\\dot{v}_1 = 2x_1 \\cdot \\dot{x}_1 = 2x_1 \\cdot 1 = 2x_1$\n",
    "   - $y = v_1 + x_2$, so $\\dot{y} = \\dot{v}_1 + \\dot{x}_2 = 2x_1 + 0 = 2x_1$\n",
    "\n",
    "3. **Result**: $\\frac{\\partial y}{\\partial x_1} = 2x_1$\n",
    "\n",
    "#### Forward Mode: Computing $\\frac{\\partial y}{\\partial x_2}$\n",
    "\n",
    "To get the derivative w.r.t. $x_2$, we seed differently:\n",
    "\n",
    "1. **Seed the input**: Set $\\dot{x}_1 = 0$ and $\\dot{x}_2 = 1$\n",
    "\n",
    "2. **Forward propagation**:\n",
    "   - $v_1 = x_1^2$, so $\\dot{v}_1 = 2x_1 \\cdot \\dot{x}_1 = 2x_1 \\cdot 0 = 0$\n",
    "   - $y = v_1 + x_2$, so $\\dot{y} = \\dot{v}_1 + \\dot{x}_2 = 0 + 1 = 1$\n",
    "\n",
    "3. **Result**: $\\frac{\\partial y}{\\partial x_2} = 1$\n",
    "\n",
    "**Key insight**: Forward mode requires one pass per input variable to compute all partial derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse Mode Automatic Differentiation\n",
    "\n",
    "Reverse mode AD (also called **backpropagation**) computes derivatives by propagating derivative information **backward** through the computational graph.\n",
    "\n",
    "#### The Backward Pass Algorithm\n",
    "\n",
    "1. **Forward pass**: Compute function values and store intermediate results\n",
    "2. **Seed the output**: Set $\\bar{y} = 1$ (derivative of output w.r.t. itself)\n",
    "3. **Backward pass**: Use the chain rule to propagate derivatives backward\n",
    "\n",
    "![Final chain rule AD](figs/ad7.png)\n",
    "\n",
    "#### Computing All Partial Derivatives in One Pass\n",
    "\n",
    "The beauty of reverse mode is that it computes **all** partial derivatives in a single backward pass:\n",
    "\n",
    "1. **Forward pass**: $y = x_1^2 + x_2$ (store intermediate values)\n",
    "\n",
    "2. **Backward pass with $\\bar{y} = 1$**:\n",
    "   - $\\frac{\\partial y}{\\partial x_1} = \\frac{\\partial y}{\\partial v_1} \\cdot \\frac{\\partial v_1}{\\partial x_1} = 1 \\cdot 2x_1 = 2x_1$\n",
    "   - $\\frac{\\partial y}{\\partial x_2} = \\frac{\\partial y}{\\partial x_2} = 1$\n",
    "\n",
    "**Key insight**: Reverse mode computes gradients w.r.t. all inputs in a single backward pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AD: The Mathematical Foundation\n",
    "\n",
    "Automatic differentiation works because of a fundamental theorem:\n",
    "\n",
    "**Chain Rule**: For composite functions $f(g(x))$:\n",
    "$$\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)$$\n",
    "\n",
    "By systematically applying the chain rule to each operation in a computational graph, AD can compute exact derivatives for arbitrarily complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation in Practice: PyTorch\n",
    "\n",
    "Let's see how automatic differentiation works in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define variables that require gradients\n",
    "\n",
    "\n",
    "# Define the function\n",
    "\n",
    "\n",
    "# Compute gradients using reverse mode AD\n",
    "\n",
    "\n",
    "# Access the computed gradients\n",
    "print(f\"dy/dx1: {x1.grad.item()}\")  # Should be 2*x1 = 4.0\n",
    "print(f\"dy/dx2: {x2.grad.item()}\")  # Should be 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Complex Example: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Implement Single-Layer NN in PyTorch\n",
    "\n",
    "class SingleLayerNN(nn.Module):\n",
    "    \"\"\"Single hidden layer neural network for 1D input/output\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size=10):\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "\n",
    "\n",
    "# Create network and data\n",
    "\n",
    "# Define MSE loss\n",
    "\n",
    "# Forward pass: compute predictions\n",
    "    \n",
    "# Calculate loss (((output - target)**2).mean())\n",
    "    \n",
    "# Backward pass: compute gradients\n",
    "            # Compute gradients of the loss w.r.t. parameters\n",
    "\n",
    "# Access gradients\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: gradient shape {param.grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Forward vs Reverse Mode\n",
    "\n",
    "The choice depends on the structure of your problem:\n",
    "\n",
    "- **Forward Mode**: Efficient when **few inputs, many outputs** (e.g., $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ with $n \\ll m$)\n",
    "- **Reverse Mode**: Efficient when **many inputs, few outputs** (e.g., $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ with $n \\gg m$)\n",
    "\n",
    "In machine learning, we typically have millions of parameters (inputs) and a single loss function (output), making reverse mode the natural choice.\n",
    "\n",
    "### Computational Considerations\n",
    "\n",
    "#### Memory vs Computation Trade-offs\n",
    "\n",
    "**Forward Mode**:\n",
    "- Memory: O(1) additional storage\n",
    "- Computation: O(n) for n input variables\n",
    "\n",
    "**Reverse Mode**:\n",
    "- Memory: O(computation graph size)\n",
    "- Computation: O(1) for any number of input variables\n",
    "\n",
    "#### Modern Optimizations\n",
    "\n",
    "1. **Checkpointing**: Trade computation for memory by recomputing intermediate values\n",
    "2. **JIT compilation**: Compile computational graphs for faster execution\n",
    "3. **Parallelization**: Distribute gradient computation across multiple devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Gradient Descent is a first-order iterative optimization algorithm used to find the minimum of a differentiable function. In the context of training a neural network, we are trying to minimize the loss function. \n",
    "\n",
    "1. **Initialize Parameters**:\n",
    "\n",
    "Choose an initial point (i.e., initial values for the weights and biases) in the parameter space, and set a learning rate that determines the step size in each iteration.\n",
    "\n",
    "2. **Compute the Gradient**:\n",
    "\n",
    "Calculate the gradient of the loss function with respect to the parameters at the current point. The gradient is a vector that points in the direction of the steepest increase of the function. It is obtained by taking the partial derivatives of the loss function with respect to each parameter.\n",
    "\n",
    "3. **Update Parameters**:\n",
    "\n",
    "Move in the opposite direction of the gradient by a distance proportional to the learning rate. This is done by subtracting the gradient times the learning rate from the current parameters:\n",
    "\n",
    "$$\\boldsymbol{w} = \\boldsymbol{w} - \\eta \\nabla J(\\boldsymbol{w})$$\n",
    "\n",
    "Here, $\\boldsymbol{w}$ represents the parameters, $\\eta$ is the learning rate, and $\\nabla J (\\boldsymbol{w})$ is the gradient of the loss function $J$ with respect to $\\boldsymbol{w}$.\n",
    "\n",
    "4. **Repeat**:\n",
    "\n",
    "Repeat steps 2 and 3 until the change in the loss function falls below a predefined threshold, or a maximum number of iterations is reached.\n",
    "\n",
    "#### Algorithm:\n",
    "\n",
    "1. Initialize weights randomly $\\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "2. Loop until convergence\n",
    "3.   Compute gradient, $\\frac{\\partial J(\\boldsymbol{w})}{\\partial \\boldsymbol{w}}$\n",
    "4.   Update weights, $\\boldsymbol{w} \\leftarrow \\boldsymbol{w} - \\eta \\frac{\\partial J(\\boldsymbol{w})}{\\partial \\boldsymbol{w}}$\n",
    "5. Return weights\n",
    "\n",
    "![SGD](figs/sgd.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming a loss function is mean squared error (MSE). Let's compute the gradient of the loss with respect to the input weights. \n",
    "\n",
    "The loss function is mean squared error:\n",
    "\n",
    "$$\\text{loss} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Where $y_i$ are the true target and $\\hat{y}_i$ are the predicted values.\n",
    "\n",
    "To minimize this loss, we need to compute the gradients with respect to the weights $\\mathbf{w}$ and bias $b$:\n",
    "\n",
    "Using the chain rule, the gradient of the loss with respect to the weights is:\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial \\mathbf{w}} = \\frac{2}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i) \\frac{\\partial y_i}{\\partial \\mathbf{w}}$$\n",
    "\n",
    "The term inside the sum is the gradient of the loss with respect to the output $y_i$, which we called $\\text{grad\\_output}$:\n",
    "$$\\text{grad\\_output} = \\frac{2}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)$$\n",
    "\n",
    "The derivative $\\frac{\\partial y_i}{\\partial \\mathbf{w}}$ is just the input $\\mathbf{x}_i$ multiplied by the derivative of the activation. For simplicity, let's assume linear activation, so this is just $\\mathbf{x}_i$:\n",
    "\n",
    "$$\\therefore \\frac{\\partial \\text{loss}}{\\partial \\mathbf{w}} = \\mathbf{X}^T\\text{grad\\_output}$$\n",
    "\n",
    "The gradient for the bias is simpler:\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial b} = \\sum_{i=1}^{n}\\text{grad\\_output}_i$$\n",
    "\n",
    "Finally, we update the weights and bias by gradient descent:\n",
    "\n",
    "$$\\mathbf{w} = \\mathbf{w} - \\eta \\frac{\\partial \\text{loss}}{\\partial \\mathbf{w}}$$\n",
    "\n",
    "$$b = b - \\eta \\frac{\\partial \\text{loss}}{\\partial b}$$\n",
    "\n",
    "Where $\\eta$ is the learning rate.\n",
    "\n",
    "#### Variants:\n",
    "\n",
    "There are several variants of Gradient Descent that modify or enhance these basic steps, including:\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD)**: Instead of using the entire dataset to compute the gradient, SGD uses a single random data point (or small batch) at each iteration. This adds noise to the gradient but often speeds up convergence and can escape local minima.\n",
    "\n",
    "- **Momentum**: Momentum methods use a moving average of past gradients to dampen oscillations and accelerate convergence, especially in cases where the loss surface has steep valleys.\n",
    "\n",
    "- **Adaptive Learning Rate Methods**: Techniques like Adagrad, RMSprop, and Adam adjust the learning rate individually for each parameter, often leading to faster convergence.\n",
    "\n",
    "#### Limitations:\n",
    "\n",
    "* It may converge to a local minimum instead of a global minimum if the loss surface is not convex.\n",
    "* Convergence can be slow if the learning rate is not properly tuned.\n",
    "* Sensitive to the scaling of features; poorly scaled data can cause the gradient descent to take a long time to converge or even diverge.\n",
    "\n",
    "#### Effect of learning rate\n",
    "\n",
    "The learning rate in gradient descent is a critical hyperparameter that can significantly influence the model's training dynamics. Let us now look at how the learning rate affects local minima, overshooting, and convergence:\n",
    "\n",
    "1. Effect on Local Minima:\n",
    "\n",
    "- High Learning Rate: A large learning rate can help the model escape shallow local minima, leading to the discovery of deeper (potentially global) minima. However, it can also cause instability, making it hard to settle in a good solution.\n",
    "\n",
    "- Low Learning Rate: A small learning rate may cause the model to get stuck in local minima, especially in complex loss landscapes with many shallow valleys. The model can lack the \"energy\" to escape these regions.\n",
    "\n",
    "2. Effect on Overshooting:\n",
    "\n",
    "- High Learning Rate: If the learning rate is set too high, the updates may be so large that they overshoot the minimum and cause the algorithm to diverge, or oscillate back and forth across the valley without ever reaching the bottom. This oscillation can be detrimental to convergence.\n",
    "   \n",
    "- Low Learning Rate: A very low learning rate will likely avoid overshooting but may lead to extremely slow convergence, as the updates to the parameters will be minimal. It might result in getting stuck in plateau regions where the gradient is small.\n",
    "\n",
    "3. Effect on Convergence:\n",
    "\n",
    "- High Learning Rate: While it can speed up convergence initially, a too-large learning rate risks instability and divergence, as mentioned above. The model may never converge to a satisfactory solution.\n",
    "   \n",
    "- Low Learning Rate: A small learning rate ensures more stable and reliable convergence but can significantly slow down the process. If set too low, it may also lead to premature convergence to a suboptimal solution.\n",
    "\n",
    "##### Finding the Right Balance:\n",
    "\n",
    "Choosing the right learning rate is often a trial-and-error process, sometimes guided by techniques like learning rate schedules or adaptive learning rate algorithms like Adam. These approaches attempt to balance the trade-offs by adjusting the learning rate throughout training, often starting with larger values to escape local minima and avoid plateaus, then reducing it to stabilize convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Button](https://img.shields.io/badge/Go%20to-Interactive%20Demo:SGD-blue?style=for-the-badge&logo=airplayvideo&logoColor=white)](../sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement PyTorch Training Loop Function\n",
    "def train_network(model, x_train, u_train, epochs=5000, lr=0.01):\n",
    "    \"\"\"Train a neural network model using MSE loss and Adam optimizer\"\"\"\n",
    "    criterion =    # Mean Squared Error Loss\n",
    "    optimizer =    # Adam optimizer\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass: compute predictions\n",
    "        \n",
    "        \n",
    "        # Calculate loss\n",
    "        \n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "                # Clear previous gradients\n",
    "                # Compute gradients of the loss w.r.t. parameters\n",
    "        \n",
    "        # Optimizer step: update parameters\n",
    "                # Perform a single optimization step\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Optional: Print loss periodically\n",
    "        # if (epoch + 1) % 1000 == 0:\n",
    "        #     print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "\n",
    "model = SingleLayerNN(hidden_size=hidden_size)\n",
    "\n",
    "# Train the model\n",
    "losses = train_network(model, x_train_tensor, u_train_tensor, epochs=8000, lr=0.01)\n",
    "\n",
    "# Handle different types of loss values\n",
    "if isinstance(losses[-1], (list, np.ndarray)):\n",
    "    # If it's a list or array, get the first element\n",
    "    final_loss = float(losses[-1][0]) if len(losses[-1]) > 0 else float(losses[-1])\n",
    "elif hasattr(losses[-1], 'item'):\n",
    "    # If it's a tensor with .item() method\n",
    "    final_loss = losses[-1].item()\n",
    "else:\n",
    "    # If it's already a scalar\n",
    "    final_loss = float(losses[-1])\n",
    "\n",
    "print(f\"Final loss for {hidden_size} neurons: {final_loss:.6f}\")\n",
    "\n",
    "# Get predictions from the trained model\n",
    "x_plot = x_test  # Use our test grid\n",
    "x_test_tensor_plot = torch.tensor(x_plot.reshape(-1, 1), dtype=torch.float32)\n",
    "u_analytical_plot = y_exact  # True function values\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_pred = model(x_test_tensor_plot).numpy().flatten()\n",
    "\n",
    "# Plotting\n",
    "# Plot true function\n",
    "plt.plot(x_plot, u_analytical_plot, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "\n",
    "# Plot NN prediction\n",
    "plt.plot(x_plot, u_pred, 'b-', linewidth=2.5, \n",
    "        label=f'NN ({hidden_size} neurons)')\n",
    "\n",
    "# Plot training data points\n",
    "plt.scatter(x_train, y_train, color='red', s=40, alpha=0.7, \n",
    "            label='Training Data', zorder=5)\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('u(x)', fontsize=12)\n",
    "plt.title(f'Single Hidden Layer NN: {hidden_size} Neurons', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate errors\n",
    "train_pred = model(x_train_tensor).detach().numpy().flatten()\n",
    "train_error = np.mean((train_pred - y_train)**2)\n",
    "test_error = np.mean((u_pred - u_analytical_plot)**2)\n",
    "\n",
    "print(f\"Training MSE: {train_error:.6f}\")\n",
    "print(f\"Test MSE: {test_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Universal Approximation Theorem: The Mathematical Foundation\n",
    "\n",
    "The Universal Approximation Theorem (UAT) is the theoretical cornerstone that explains why neural networks are so powerful for function approximation. It guarantees that with enough neurons, a single-hidden-layer network can approximate any continuous function to arbitrary accuracy.\n",
    "\n",
    "### What the Theorem States\n",
    "\n",
    "**Universal Approximation Theorem (Cybenko, 1989; Hornik, 1991):**\n",
    "A feedforward neural network with:\n",
    "- A single hidden layer\n",
    "- Finite number of neurons\n",
    "- Non-polynomial activation function (e.g., sigmoid, ReLU, tanh)\n",
    "\n",
    "Can approximate any continuous function on a compact subset of ℝⁿ to arbitrary accuracy.\n",
    "\n",
    "Mathematically: Neural networks form a **dense subset** in C([a,b]) - the space of continuous functions on [a,b].\n",
    "\n",
    "### Why This Matters for Scientific Machine Learning\n",
    "\n",
    "1. **PDE Solutions**: Most PDE solutions are continuous functions, so neural networks can represent them\n",
    "2. **Data Fitting**: Any smooth experimental data can be approximated\n",
    "3. **Surrogate Modeling**: Complex simulations can be replaced with neural network approximators\n",
    "4. **Theoretical Guarantee**: We know the approximation is *possible* (though not how to find it)\n",
    "\n",
    "### Interactive Exploration\n",
    "\n",
    "Explore how neural networks build complex functions from simple components:\n",
    "\n",
    "[![Interactive Demo](https://img.shields.io/badge/Go%20to-Interactive%20Demo:UAT-blue?style=for-the-badge)](./uat-demo)\n",
    "\n",
    "The interactive demo includes:\n",
    "- **ReLU decomposition** - See how ReLU units combine to approximate functions\n",
    "- **Step function construction** - Watch as more neurons improve approximation\n",
    "- **Activation comparison** - Compare ReLU, sigmoid, and non-universal activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Foundations: Function Spaces\n",
    "\n",
    "To understand what neural networks can approximate, we need a mathematical framework for measuring \"closeness\" between functions.\n",
    "\n",
    "#### Banach Spaces: The Setting for Approximation\n",
    "\n",
    "A **Banach space** is a complete normed vector space. For our purposes:\n",
    "\n",
    "1. **Vector Space of Functions**: We can add functions and multiply by scalars\n",
    "   - $(f + g)(x) = f(x) + g(x)$\n",
    "   - $(\\alpha f)(x) = \\alpha \\cdot f(x)$\n",
    "\n",
    "2. **Norm**: A way to measure the \"size\" of a function\n",
    "   - $L^\\infty$ norm (supremum): $\\|f\\|_\\infty = \\sup_{x \\in [a,b]} |f(x)|$\n",
    "   - $L^2$ norm: $\\|f\\|_2 = \\left(\\int_a^b |f(x)|^2 dx\\right)^{1/2}$\n",
    "   - $L^1$ norm: $\\|f\\|_1 = \\int_a^b |f(x)| dx$\n",
    "\n",
    "3. **Completeness**: Cauchy sequences converge to a limit in the space\n",
    "\n",
    "#### Hilbert Spaces: Adding Geometry\n",
    "\n",
    "A **Hilbert space** is a Banach space with an inner product:\n",
    "\n",
    "$$\\langle f, g \\rangle = \\int_a^b f(x)g(x) dx$$\n",
    "\n",
    "This enables:\n",
    "- **Orthogonality**: Functions can be perpendicular ($\\langle f, g \\rangle = 0$)\n",
    "- **Projections**: Finding the best approximation in a subspace\n",
    "- **Basis expansions**: $f = \\sum_{i=1}^\\infty c_i \\phi_i$ where $\\{\\phi_i\\}$ is an orthonormal basis\n",
    "\n",
    "#### Density: The Key Concept\n",
    "\n",
    "A subset $S$ is **dense** in a space $X$ if:\n",
    "\n",
    "$$\\forall f \\in X, \\forall \\epsilon > 0, \\exists g \\in S : \\|f - g\\| < \\epsilon$$\n",
    "\n",
    "In words: Every function in $X$ can be approximated arbitrarily well by functions from $S$.\n",
    "\n",
    "**The Universal Approximation Theorem states**: Neural networks form a dense subset in $C([a,b])$ (continuous functions on $[a,b]$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Proofs of the Universal Approximation Theorem\n",
    "\n",
    "#### 1. Constructive Proof with ReLU\n",
    "\n",
    "The ReLU activation $\\text{ReLU}(x) = \\max(0, x)$ allows us to build approximations constructively.\n",
    "\n",
    "#### Building Blocks\n",
    "- A single ReLU neuron creates a \"hinge\" function: $h_i(x) = \\max(0, w_i x + b_i)$\n",
    "- The bias $b_i$ determines the \"breakpoint\" at $x = -b_i/w_i$\n",
    "- Two ReLU units create a \"bump\": $\\text{bump}(x) = \\text{ReLU}(x - a) - \\text{ReLU}(x - b)$\n",
    "\n",
    "#### Algorithm\n",
    "1. Divide domain into $n$ intervals\n",
    "2. Create a bump for each interval  \n",
    "3. Set bump height to match target function\n",
    "4. As $n \\to \\infty$, approximation becomes exact\n",
    "\n",
    "Let's demonstrate this with sin(πx):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class ReLUNet(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1, width)\n",
    "        self.output = nn.Linear(width, 1)\n",
    "        \n",
    "        # Glorot initialization\n",
    "        nn.init.xavier_normal_(self.hidden.weight)\n",
    "        nn.init.xavier_normal_(self.output.weight)\n",
    "        nn.init.zeros_(self.hidden.bias)\n",
    "        nn.init.zeros_(self.output.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.output(torch.relu(self.hidden(x)))\n",
    "\n",
    "def target_sinpi(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Create training data\n",
    "x_train = torch.linspace(0, 1, 100).reshape(-1, 1)\n",
    "y_train = torch.tensor([target_sinpi(x.item()) for x in x_train], dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Train networks with different widths\n",
    "widths = [2, 5, 10, 20, 50, 100]\n",
    "models = []\n",
    "errors = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, width in enumerate(widths):\n",
    "    best_model = None\n",
    "    best_error = float('inf')\n",
    "    \n",
    "    # Multiple restarts to avoid bad local minima\n",
    "    for trial in range(5):\n",
    "        model = ReLUNet(width)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Train\n",
    "        epochs = max(3000, width * 30)\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_train)\n",
    "            loss = criterion(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate this trial\n",
    "        x_test = torch.linspace(0, 1, 200).reshape(-1, 1)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x_test).numpy().flatten()\n",
    "            x_test_np = x_test.numpy().flatten()\n",
    "            y_true = np.array([target_sinpi(x) for x in x_test_np])\n",
    "            error = np.max(np.abs(y_true - y_pred))\n",
    "        \n",
    "        if error < best_error:\n",
    "            best_error = error\n",
    "            best_model = model\n",
    "    \n",
    "    models.append(best_model)\n",
    "    errors.append(best_error)\n",
    "    \n",
    "    # Plot best result\n",
    "    x_test = torch.linspace(0, 1, 200).reshape(-1, 1)\n",
    "    with torch.no_grad():\n",
    "        y_pred = best_model(x_test).numpy().flatten()\n",
    "        x_test_np = x_test.numpy().flatten()\n",
    "        y_true = np.array([target_sinpi(x) for x in x_test_np])\n",
    "    \n",
    "    axes[idx].plot(x_test_np, y_true, 'b-', linewidth=3, label='sin(πx)', alpha=0.7)\n",
    "    axes[idx].plot(x_test_np, y_pred, 'r--', linewidth=2, label=f'NN ({width} neurons)')\n",
    "    axes[idx].fill_between(x_test_np, y_true - best_error, y_true + best_error,\n",
    "                           alpha=0.2, color='gray', label='Error band')\n",
    "    axes[idx].set_title(f'{width} Neurons', fontweight='bold')\n",
    "    axes[idx].set_xlabel('x')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].legend(loc='upper right', fontsize=9)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim(-1.5, 1.5)\n",
    "    \n",
    "    axes[idx].text(0.5, -1.3, f'Max Error: {best_error:.4f}',\n",
    "                   ha='center', fontsize=10, fontweight='bold',\n",
    "                   bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Universal Approximation of sin(πx): Error Decreases with Network Width',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot error vs width\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(widths, errors, 'bo-', linewidth=2, markersize=10)\n",
    "plt.xlabel('Number of Neurons', fontsize=12)\n",
    "plt.ylabel('Maximum Approximation Error', fontsize=12)\n",
    "plt.title('Approximation Error vs Network Width', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "for i, (w, e) in enumerate(zip(widths, errors)):\n",
    "    plt.annotate(f'{e:.4f}', (w, e), textcoords=\"offset points\",\n",
    "                 xytext=(0,10), ha='center', fontsize=9)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(widths, errors, 'ro-', linewidth=2, markersize=10)\n",
    "plt.xlabel('Number of Neurons', fontsize=12)\n",
    "plt.ylabel('Maximum Error (log scale)', fontsize=12)\n",
    "plt.title('Error Decay (Log Scale)', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Convergence to sin(πx) as Width Increases', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Universal Approximation Theorem Demonstrated for sin(πx):\")\n",
    "print(f\"{'Width':<10} {'Max Error':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 40)\n",
    "for i, (w, e) in enumerate(zip(widths, errors)):\n",
    "    if i == 0:\n",
    "        print(f\"{w:<10} {e:<15.6f} {'(baseline)':<15}\")\n",
    "    else:\n",
    "        improvement = errors[0] / e\n",
    "        print(f\"{w:<10} {e:<15.6f} {improvement:<15.2f}x\")\n",
    "\n",
    "print(f\"\\nAs the number of neurons increases from {widths[0]} to {widths[-1]}:\")\n",
    "print(f\"Error decreases by {errors[0]/errors[-1]:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Insight from Constructive Proof\n",
    "\n",
    "The constructive proof shows:\n",
    "1. **Two ReLU units** create a \"bump\" function  \n",
    "2. **Multiple bumps** approximate step functions\n",
    "3. **Finer steps** (more neurons) → better approximation\n",
    "4. As $n \\to \\infty$, we can achieve any desired accuracy $\\epsilon$\n",
    "\n",
    "### 2. Proof by Contradiction (Hahn-Banach)\n",
    "\n",
    "We'll prove UAT using $\\sin(\\pi x)$ to make the contradiction concrete.\n",
    "\n",
    "#### The Setup\n",
    "**Assumption**: Neural networks CANNOT approximate sin(πx). There exists $\\epsilon > 0$ such that for ANY neural network $N$:\n",
    "$$\\|\\sin(\\pi x) - N(x)\\|_\\infty > \\epsilon$$\n",
    "\n",
    "#### The Contradiction  \n",
    "If neural networks can't approximate $\\sin(\\pi x)$, there must exist a linear functional $L$ that:\n",
    "- \"Sees\" our target: $L(\\sin(\\pi x)) \\neq 0$\n",
    "- \"Blind\" to all networks: $L(N) = 0$ for every neural network $N$\n",
    "\n",
    "By Riesz Representation, this corresponds to a measure $\\mu$ with:\n",
    "$$\\int \\sigma(wx + b) \\, d\\mu(x) = 0 \\quad \\forall w, b$$\n",
    "\n",
    "#### Why This Is Impossible\n",
    "1. Sigmoids approximate step functions as steepness increases\n",
    "2. Step functions are characteristic functions of half-spaces\n",
    "3. Half-spaces can isolate any point → $\\mu = 0$ everywhere\n",
    "4. But then $\\mu$ can't detect $\\sin(\\pi x)$ either → **Contradiction!**\n",
    "\n",
    "Therefore, neural networks CAN approximate any continuous function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Neural Networks Cannot Approximate: The Continuity Requirement\n",
    "\n",
    "While UAT guarantees approximation of continuous functions, neural networks with continuous activations have fundamental limitations with discontinuous functions.\n",
    "\n",
    "#### The Detector That Distinguishes Discontinuities\n",
    "\n",
    "Consider a detector measure that's +1 on [0, 0.5) and -1 on [0.5, 1]. This measure can distinguish:\n",
    "- **True step function**: Integral = -0.5 (non-zero)\n",
    "- **Any continuous function**: Integral ≈ 0 (including NN approximations)\n",
    "\n",
    "This shows neural networks cannot perfectly approximate discontinuous functions in the supremum norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating what neural networks CAN and CANNOT approximate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define our detector measure μ: +1 on [0, 0.5), -1 on [0.5, 1]\n",
    "def detector_measure(x):\n",
    "    \"\"\"A signed measure that's +1 on [0, 0.5) and -1 on [0.5, 1]\"\"\"\n",
    "    return np.where(x < 0.5, 1, -1)\n",
    "\n",
    "# Define various test functions\n",
    "def step_function(x):\n",
    "    \"\"\"True discontinuous step function\"\"\"\n",
    "    return np.where(x < 0.5, 0, 1)\n",
    "\n",
    "def smooth_step(x, steepness=50):\n",
    "    \"\"\"Smooth approximation of step using sigmoid\"\"\"\n",
    "    return 1 / (1 + np.exp(-steepness * (x - 0.5)))\n",
    "\n",
    "def sin_pi(x):\n",
    "    \"\"\"Our continuous friend sin(πx)\"\"\"\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Compute integrals with the detector\n",
    "def compute_integral(func, measure, x_range=(0, 1), n_points=1000):\n",
    "    \"\"\"Compute ∫ f(x) dμ(x) numerically\"\"\"\n",
    "    x = np.linspace(x_range[0], x_range[1], n_points)\n",
    "    f_vals = func(x)\n",
    "    mu_vals = measure(x)\n",
    "    # Approximate integral\n",
    "    dx = (x_range[1] - x_range[0]) / n_points\n",
    "    return np.sum(f_vals * mu_vals) * dx\n",
    "\n",
    "# Create visualization with better layout\n",
    "fig, axes = plt.subplots(3, 4, figsize=(15, 9))\n",
    "fig.subplots_adjust(hspace=0.35, wspace=0.3)\n",
    "\n",
    "# Plot 1: The detector measure\n",
    "ax1 = axes[0, 0]\n",
    "x = np.linspace(0, 1, 1000)\n",
    "mu = detector_measure(x)\n",
    "ax1.plot(x, mu, 'g-', linewidth=2)\n",
    "ax1.fill_between(x, 0, mu, where=(mu > 0), alpha=0.3, color='green', label='μ = +1')\n",
    "ax1.fill_between(x, 0, mu, where=(mu <= 0), alpha=0.3, color='red', label='μ = -1')\n",
    "ax1.axvline(x=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "ax1.set_title('Detector Measure μ(x)', fontweight='bold', fontsize=10)\n",
    "ax1.set_xlabel('x', fontsize=9)\n",
    "ax1.set_ylabel('μ(x)', fontsize=9)\n",
    "ax1.legend(fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(labelsize=8)\n",
    "\n",
    "# Test functions and their integrals - ONLY 3 functions\n",
    "test_functions = [\n",
    "    (step_function, \"Step Function\", 'b'),\n",
    "    (lambda x: smooth_step(x, 50), \"Smooth Step\", 'orange'),\n",
    "    (sin_pi, \"sin(πx)\", 'purple')\n",
    "]\n",
    "\n",
    "# Plot each function and compute integral\n",
    "for idx, (func, name, color) in enumerate(test_functions):\n",
    "    ax = axes[0, idx + 1]\n",
    "    \n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    y = func(x)\n",
    "    mu = detector_measure(x)\n",
    "    \n",
    "    # Plot function\n",
    "    ax.plot(x, y, color=color, linewidth=2)\n",
    "    ax.fill_between(x, 0, y, alpha=0.2, color=color)\n",
    "    \n",
    "    # Overlay detector regions\n",
    "    ax.axvspan(0, 0.5, alpha=0.1, color='green')\n",
    "    ax.axvspan(0.5, 1, alpha=0.1, color='red')\n",
    "    ax.axvline(x=0.5, color='black', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Compute integral\n",
    "    integral = compute_integral(func, detector_measure)\n",
    "    \n",
    "    ax.set_title(f'{name}', fontweight='bold', fontsize=10)\n",
    "    ax.set_xlabel('x', fontsize=9)\n",
    "    ax.set_ylabel('f(x)', fontsize=9)\n",
    "    \n",
    "    # Place integral text inside plot to avoid overlap\n",
    "    ax.text(0.95, 0.05, f'∫f dμ = {integral:.3f}', \n",
    "            transform=ax.transAxes, ha='right', va='bottom',\n",
    "            fontweight='bold', fontsize=9,\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8),\n",
    "            color='red' if abs(integral) > 0.01 else 'green')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(labelsize=8)\n",
    "\n",
    "# Hide the empty plot in row 2, column 1\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Plot 7: Product visualization for step function\n",
    "ax7 = axes[1, 1]\n",
    "x = np.linspace(0, 1, 1000)\n",
    "y_step = step_function(x)\n",
    "mu = detector_measure(x)\n",
    "product = y_step * mu\n",
    "ax7.plot(x, product, 'b-', linewidth=2)\n",
    "ax7.fill_between(x, 0, product, where=(product > 0), alpha=0.3, color='blue')\n",
    "ax7.fill_between(x, 0, product, where=(product <= 0), alpha=0.3, color='red')\n",
    "ax7.set_title('Step × μ', fontweight='bold', fontsize=10)\n",
    "ax7.set_xlabel('x', fontsize=9)\n",
    "ax7.set_ylabel('f(x) × μ(x)', fontsize=9)\n",
    "integral = compute_integral(step_function, detector_measure)\n",
    "ax7.text(0.5, 0.95, f'∫ = {integral:.3f}', transform=ax7.transAxes,\n",
    "         ha='center', va='top', fontweight='bold', fontsize=10,\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow'))\n",
    "ax7.grid(True, alpha=0.3)\n",
    "ax7.tick_params(labelsize=8)\n",
    "\n",
    "# Plot 8: Product visualization for smooth step\n",
    "ax8 = axes[1, 2]\n",
    "y_smooth = smooth_step(x, 50)\n",
    "product = y_smooth * mu\n",
    "ax8.plot(x, product, 'orange', linewidth=2)\n",
    "ax8.fill_between(x, 0, product, where=(product > 0), alpha=0.3, color='orange')\n",
    "ax8.fill_between(x, 0, product, where=(product <= 0), alpha=0.3, color='brown')\n",
    "ax8.set_title('Smooth Step × μ', fontweight='bold', fontsize=10)\n",
    "ax8.set_xlabel('x', fontsize=9)\n",
    "ax8.set_ylabel('f(x) × μ(x)', fontsize=9)\n",
    "integral = compute_integral(lambda x: smooth_step(x, 50), detector_measure)\n",
    "ax8.text(0.5, 0.95, f'∫ ≈ {integral:.3f}', transform=ax8.transAxes,\n",
    "         ha='center', va='top', fontweight='bold', fontsize=10,\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow'))\n",
    "ax8.grid(True, alpha=0.3)\n",
    "ax8.tick_params(labelsize=8)\n",
    "\n",
    "# Plot 9: Product visualization for sin(πx)\n",
    "ax9 = axes[1, 3]\n",
    "y_sin = sin_pi(x)\n",
    "product = y_sin * mu\n",
    "ax9.plot(x, product, 'purple', linewidth=2)\n",
    "ax9.fill_between(x, 0, product, where=(product > 0), alpha=0.3, color='purple')\n",
    "ax9.fill_between(x, 0, product, where=(product <= 0), alpha=0.3, color='pink')\n",
    "ax9.set_title('sin(πx) × μ', fontweight='bold', fontsize=10)\n",
    "ax9.set_xlabel('x', fontsize=9)\n",
    "ax9.set_ylabel('f(x) × μ(x)', fontsize=9)\n",
    "integral = compute_integral(sin_pi, detector_measure)\n",
    "ax9.text(0.5, 0.95, f'∫ = {integral:.3f}', transform=ax9.transAxes,\n",
    "         ha='center', va='top', fontweight='bold', fontsize=10,\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow'))\n",
    "ax9.grid(True, alpha=0.3)\n",
    "ax9.tick_params(labelsize=8)\n",
    "\n",
    "# Neural network approximation comparison\n",
    "ax10 = axes[2, 0]\n",
    "steepness_values = [5, 10, 20, 50, 100, 200]\n",
    "integrals = []\n",
    "for s in steepness_values:\n",
    "    integral = compute_integral(lambda x: smooth_step(x, s), detector_measure)\n",
    "    integrals.append(integral)\n",
    "\n",
    "ax10.plot(steepness_values, integrals, 'ro-', linewidth=2, markersize=6)\n",
    "ax10.axhline(y=-0.5, color='b', linestyle='--', label='True step', linewidth=1.5)\n",
    "ax10.axhline(y=0, color='g', linestyle='--', alpha=0.5, label='Zero', linewidth=1.5)\n",
    "ax10.set_xlabel('Sigmoid Steepness', fontsize=9)\n",
    "ax10.set_ylabel('∫f dμ', fontsize=9)\n",
    "ax10.set_title('NN Approx. vs True Step', fontweight='bold', fontsize=10)\n",
    "ax10.legend(fontsize=8)\n",
    "ax10.grid(True, alpha=0.3)\n",
    "ax10.tick_params(labelsize=8)\n",
    "\n",
    "# Summary text box\n",
    "ax11 = axes[2, 1]\n",
    "ax11.axis('off')\n",
    "summary_text = \"\"\"KEY INSIGHTS:\n",
    "\n",
    "• Step function: ∫ = -0.5\n",
    "• Smooth approx: ∫ ≈ 0\n",
    "• sin(πx): ∫ = 0\n",
    "\n",
    "As NN gets steeper:\n",
    "integral → 0 (not -0.5!)\n",
    "\n",
    "NNs cannot capture\n",
    "true discontinuities\"\"\"\n",
    "\n",
    "ax11.text(0.1, 0.9, summary_text, fontsize=9, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "# Main conclusion\n",
    "ax12 = axes[2, 2]\n",
    "ax12.axis('off')\n",
    "ax12.text(0.5, 0.75, '✓ Continuous', fontsize=12, ha='center', color='green', fontweight='bold')\n",
    "ax12.text(0.5, 0.6, 'NNs CAN\\napproximate', fontsize=9, ha='center')\n",
    "ax12.text(0.5, 0.3, '✗ Discontinuous', fontsize=12, ha='center', color='red', fontweight='bold')\n",
    "ax12.text(0.5, 0.15, 'NNs CANNOT\\ncapture jumps', fontsize=9, ha='center')\n",
    "\n",
    "# Hide unused subplot\n",
    "axes[2, 3].axis('off')\n",
    "\n",
    "plt.suptitle('What Neural Networks Can and Cannot Approximate: The Role of Continuity', \n",
    "             fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.show()\n",
    "\n",
    "# Print numerical results table\n",
    "print(\"Integral Results with Detector μ(x):\")\n",
    "print(\"-\" * 45)\n",
    "for func, name, _ in test_functions:\n",
    "    integral = compute_integral(func, detector_measure)\n",
    "    print(f\"{name:<20}: ∫f dμ = {integral:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: The Continuity Requirement\n",
    "\n",
    "The detector analysis reveals why UAT specifically applies to continuous functions:\n",
    "\n",
    "- **True step function has non-zero integral** (-0.5) with our detector\n",
    "- **Continuous functions** (including NN approximations) have ≈0 integral  \n",
    "- **This detector can distinguish discontinuous from continuous functions**\n",
    "\n",
    "Therefore, neural networks with continuous activations cannot perfectly approximate discontinuous functions in the supremum norm. This is why the Universal Approximation Theorem is stated for the space C([a,b]) of continuous functions, not for broader function spaces that include discontinuities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Implications\n",
    "\n",
    "1. **Neural networks CAN approximate discontinuous functions in L² sense** - Good for energy norms and integration\n",
    "2. **Error concentrates near discontinuities** - Away from jumps, approximation is excellent\n",
    "3. **L∞ convergence impossible for true discontinuities** - There will always be error at the jump\n",
    "4. **Special techniques needed** for shocks, material interfaces in scientific computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UAT is an existence theorem\n",
    "\n",
    "| Method | Domain | Rate (General) | Rate (Hölder-α) | Constructive | Dimension |\n",
    "|--------|--------|----------------|-----------------|-------------|-----------|\n",
    "| **Bernstein Polynomials** | $[0,1]$ | $O(\\omega_f(n^{-1/2}))$ | $O(n^{-\\alpha/2})$ | ✅ Explicit | 1D only |\n",
    "| **Chebyshev Polynomials** | $[-1,1]$ | $O(e^{-cn})$ | $O(e^{-cn})$ | ✅ Explicit | 1D only |\n",
    "| **Classical UAT** | $K \\subset \\mathbb{R}^d$ | ❌ Unknown | ❌ Unknown | ❌ Existence only | Any $d$ |\n",
    "| **Barron's UAT** | $\\mathbb{R}^d$ | $O(n^{-1/2})$ | $O(n^{-1/2})$ | ✅ Constructive | Any $d$ * |\n",
    "| **Yarotsky Fixed Depth** | $[0,1]^\\nu$ | $O(\\omega_f(W^{-1/\\nu}))$ | $O(W^{-\\alpha/\\nu})$ | ✅ Constructive | Any $\\nu$ |\n",
    "| **Yarotsky Variable Depth** | $[0,1]^\\nu$ | $O(\\omega_f(W^{-2/\\nu}))$ | $O(W^{-2\\alpha/\\nu})$ | ✅ Constructive | Any $\\nu$ |\n",
    "\n",
    "*Only for functions with bounded Fourier spectrum\n",
    "\n",
    "## For sin(πx) on [0,1]\n",
    "\n",
    "**Hölder analysis:**\n",
    "- $\\alpha = 1$ (Lipschitz: $|\\sin(\\pi x) - \\sin(\\pi y)| \\leq \\pi |x-y|$)\n",
    "- $\\nu = 1$ (one dimension)\n",
    "\n",
    "**Yarotsky rates:**\n",
    "- **Fixed Depth:** $O(W^{-\\alpha/\\nu}) = O(W^{-1 \\cdot 1/1}) = O(W^{-1})$, slope = $-1.0$\n",
    "- **Variable Depth:** $O(W^{-2\\alpha/\\nu}) = O(W^{-2 \\cdot 1/1}) = O(W^{-2})$, slope = $-2.0$\n",
    "\n",
    "**Variable depth should give TWICE the slope of fixed depth**\n",
    "\n",
    "## Why Your Code Results Are Bad\n",
    "\n",
    "From Yarotsky's paper requirements:\n",
    "\n",
    "1. **Variable depth needs:** $L \\sim W$ (you use $L \\sim \\sqrt{W}$)\n",
    "2. **Architecture:** Constant width, very deep (you use variable width)  \n",
    "3. **Weight assignment:** Explicit construction with discontinuous weights (you use SGD)\n",
    "4. **Method:** Bit extraction technique and two-scale approximation (you use standard training)\n",
    "\n",
    "Your networks show similar performance because both use SGD training, not Yarotsky's specialized constructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard ReLU Networks: Normal Training Convergence Analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class FixedDepthReLUNet(nn.Module):\n",
    "    \"\"\"Fixed depth ReLU network with L=2 hidden layers\"\"\"\n",
    "    def __init__(self, width):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(width, width),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(width, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "class VariableDepthReLUNet(nn.Module):\n",
    "    \"\"\"Variable depth ReLU network where L ~ sqrt(W)\"\"\"\n",
    "    def __init__(self, total_weights):\n",
    "        super().__init__()\n",
    "        # Choose depth L ~ sqrt(W) for balanced architecture\n",
    "        self.depth = max(2, int(np.sqrt(total_weights / 10)))\n",
    "        self.width = max(5, total_weights // (self.depth * 2))\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Linear(1, self.width))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        for _ in range(self.depth - 1):\n",
    "            layers.append(nn.Linear(self.width, self.width))\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "        layers.append(nn.Linear(self.width, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "def target_function(x):\n",
    "    \"\"\"High-frequency challenge: sin(πx) + 0.3*sin(10πx)\"\"\"\n",
    "    return np.sin(np.pi * x) + 0.3 * np.sin(10 * np.pi * x)\n",
    "\n",
    "def train_network(model, x_train, y_train, epochs=2000, lr=0.01):\n",
    "    \"\"\"Standard SGD training\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    x_tensor = torch.FloatTensor(x_train.reshape(-1, 1))\n",
    "    y_tensor = torch.FloatTensor(y_train.reshape(-1, 1))\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate on [0,1]\n",
    "    x_test = np.linspace(0, 1, 1000)\n",
    "    y_test = target_function(x_test)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_test_tensor = torch.FloatTensor(x_test.reshape(-1, 1))\n",
    "        y_pred = model(x_test_tensor).numpy().flatten()\n",
    "    \n",
    "    max_error = np.max(np.abs(y_test - y_pred))\n",
    "    return max_error, x_test, y_test, y_pred\n",
    "\n",
    "# Run analysis\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "x_train = np.linspace(0, 1, 50)\n",
    "y_train = target_function(x_train)\n",
    "\n",
    "weight_counts = [20, 50, 100, 200, 400, 800, 1600]\n",
    "fixed_errors = []\n",
    "variable_errors = []\n",
    "actual_weights_fixed = []\n",
    "actual_weights_variable = []\n",
    "\n",
    "for target_weights in weight_counts:\n",
    "    # Fixed depth\n",
    "    width = max(5, target_weights // 6)\n",
    "    fixed_net = FixedDepthReLUNet(width)\n",
    "    actual_weights_fixed.append(fixed_net.count_parameters())\n",
    "    fixed_error, _, _, _ = train_network(fixed_net, x_train, y_train, epochs=1500)\n",
    "    fixed_errors.append(fixed_error)\n",
    "    \n",
    "    # Variable depth\n",
    "    variable_net = VariableDepthReLUNet(target_weights)\n",
    "    actual_weights_variable.append(variable_net.count_parameters())\n",
    "    variable_error, x_test, y_true, y_pred = train_network(variable_net, x_train, y_train, epochs=1500)\n",
    "    variable_errors.append(variable_error)\n",
    "\n",
    "# Compute empirical slopes\n",
    "def compute_slope(weights, errors):\n",
    "    log_w = np.log(weights)\n",
    "    log_e = np.log(errors)\n",
    "    slope = np.polyfit(log_w, log_e, 1)[0]\n",
    "    return slope\n",
    "\n",
    "slope_fixed = compute_slope(actual_weights_fixed, fixed_errors)\n",
    "slope_variable = compute_slope(actual_weights_variable, variable_errors)\n",
    "\n",
    "# Yarotsky theoretical slopes for sin(πx): α=1, ν=1\n",
    "yarotsky_fixed_slope = -1.0      # O(W^(-α/ν)) = O(W^(-1))\n",
    "yarotsky_variable_slope = -2.0   # O(W^(-2α/ν)) = O(W^(-2))\n",
    "\n",
    "# Plot results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Fixed depth\n",
    "ax1.loglog(actual_weights_fixed, fixed_errors, 'ro-', markersize=8, linewidth=2)\n",
    "W_theory = np.array(actual_weights_fixed)\n",
    "theory_fixed = 0.5 * W_theory**yarotsky_fixed_slope\n",
    "ax1.loglog(W_theory, theory_fixed, 'r--', linewidth=2, alpha=0.7)\n",
    "ax1.set_title(f'Fixed Depth (L=2)\\nSGD Slope: {slope_fixed:.2f} | Yarotsky Theory: {yarotsky_fixed_slope:.1f}')\n",
    "ax1.set_xlabel('Parameters W')\n",
    "ax1.set_ylabel('Max Error on [0,1]')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Variable depth  \n",
    "ax2.loglog(actual_weights_variable, variable_errors, 'bo-', markersize=8, linewidth=2)\n",
    "theory_variable = 0.1 * W_theory**yarotsky_variable_slope\n",
    "ax2.loglog(W_theory, theory_variable, 'b--', linewidth=2, alpha=0.7)\n",
    "ax2.set_title(f'Variable Depth (L~√W)\\nSGD Slope: {slope_variable:.2f} | Yarotsky Theory: {yarotsky_variable_slope:.1f}')\n",
    "ax2.set_xlabel('Parameters W')\n",
    "ax2.set_ylabel('Max Error on [0,1]')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison\n",
    "ax3.loglog(actual_weights_fixed, fixed_errors, 'ro-', label=f'Fixed (SGD: {slope_fixed:.2f})', markersize=6)\n",
    "ax3.loglog(actual_weights_variable, variable_errors, 'bo-', label=f'Variable (SGD: {slope_variable:.2f})', markersize=6)\n",
    "ax3.loglog(W_theory, theory_fixed, 'r:', alpha=0.5, label='Yarotsky Fixed (-1.0)')\n",
    "ax3.loglog(W_theory, theory_variable, 'b:', alpha=0.5, label='Yarotsky Variable (-2.0)')\n",
    "ax3.set_title('Standard Training vs Yarotsky Theory')\n",
    "ax3.set_xlabel('Parameters W')\n",
    "ax3.set_ylabel('Max Error')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Function approximation\n",
    "ax4.plot(x_test, y_true, 'k-', linewidth=3, label='sin(πx)')\n",
    "ax4.plot(x_test, y_pred, 'b-', linewidth=2, label='Variable Depth NN')\n",
    "ax4.set_title('Final Approximation on [0,1]')\n",
    "ax4.set_xlabel('x')\n",
    "ax4.set_ylabel('f(x)')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEMPIRICAL RESULTS:\")\n",
    "print(f\"Fixed depth slope:    {slope_fixed:.3f}\")\n",
    "print(f\"Variable depth slope: {slope_variable:.3f}\")\n",
    "print(f\"Yarotsky fixed theory:    {yarotsky_fixed_slope:.1f}\")\n",
    "print(f\"Yarotsky variable theory: {yarotsky_variable_slope:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Exploration and Key Takeaways\n",
    "\n",
    "#### Explore UAT Concepts Interactively\n",
    "\n",
    "##### ReLU Function Decomposition\n",
    "See how neural networks build complex functions from simple ReLU components:\n",
    "\n",
    "[![ReLU Demo](https://img.shields.io/badge/Go%20to-Interactive%20Demo:ReLU_Decomposition-blue?style=for-the-badge)](./uat-demo#relu-components-demo)\n",
    "\n",
    "##### Step Function Approximation  \n",
    "Watch how more neurons improve approximation quality:\n",
    "\n",
    "[![Step Demo](https://img.shields.io/badge/Go%20to-Interactive%20Demo:Step_Functions-green?style=for-the-badge)](./uat-demo#relu-construction)\n",
    "\n",
    "##### Activation Function Comparison\n",
    "Compare universal (ReLU, sigmoid) vs non-universal (parabolic) activations:\n",
    "\n",
    "[![Activation Demo](https://img.shields.io/badge/Go%20to-Interactive%20Demo:Activation_Comparison-orange?style=for-the-badge)](./uat-demo#activation-comparison-demo)\n",
    "\n",
    "#### Approximation Rates\n",
    "\n",
    "For smooth functions, approximation error scales as:\n",
    "$$\\|f - f_{NN}\\|_\\infty = O\\left(\\frac{1}{\\sqrt{n}}\\right)$$\n",
    "\n",
    "where $n$ is the number of hidden neurons. Deeper networks can achieve exponentially better rates for certain function classes.\n",
    "\n",
    "#### The Big Picture: Why UAT Matters for SciML\n",
    "\n",
    "The Universal Approximation Theorem provides the theoretical foundation for using neural networks in scientific computing:\n",
    "\n",
    "1. **Guaranteed Approximation**: Any continuous function (including most PDE solutions) can be approximated\n",
    "2. **Width vs Depth**: One hidden layer suffices (UAT) but deep networks are often more efficient\n",
    "3. **Bias as Breakpoints**: In ReLU networks, bias terms determine where piecewise segments break\n",
    "4. **Sobolev Spaces**: Neural networks can approximate not just functions but also their derivatives\n",
    "5. **The Caveat**: UAT tells us approximation is POSSIBLE but not:\n",
    "   - How many neurons we need\n",
    "   - How to find the weights (optimization challenge)\n",
    "   - How well it generalizes beyond training data\n",
    "\n",
    "The theorem is our theoretical permission slip - it guarantees that neural networks CAN solve our scientific problems, though finding the solution requires careful engineering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating what neural networks CAN and CANNOT approximate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define our detector measure μ: +1 on [0, 0.5), -1 on [0.5, 1]\n",
    "def detector_measure(x):\n",
    "    \"\"\"A signed measure that's +1 on [0, 0.5) and -1 on [0.5, 1]\"\"\"\n",
    "    return np.where(x < 0.5, 1, -1)\n",
    "\n",
    "# Define various test functions\n",
    "def step_function(x):\n",
    "    \"\"\"True discontinuous step function\"\"\"\n",
    "    return np.where(x < 0.5, 0, 1)\n",
    "\n",
    "def smooth_step(x, steepness=50):\n",
    "    \"\"\"Smooth approximation of step using sigmoid\"\"\"\n",
    "    return 1 / (1 + np.exp(-steepness * (x - 0.5)))\n",
    "\n",
    "def sin_pi(x):\n",
    "    \"\"\"Our continuous friend sin(πx)\"\"\"\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Compute integrals with the detector\n",
    "def compute_integral(func, measure, x_range=(0, 1), n_points=1000):\n",
    "    \"\"\"Compute ∫ f(x) dμ(x) numerically\"\"\"\n",
    "    x = np.linspace(x_range[0], x_range[1], n_points)\n",
    "    f_vals = func(x)\n",
    "    mu_vals = measure(x)\n",
    "    # Approximate integral\n",
    "    dx = (x_range[1] - x_range[0]) / n_points\n",
    "    return np.sum(f_vals * mu_vals) * dx\n",
    "\n",
    "# Create visualization with better layout\n",
    "fig, axes = plt.subplots(3, 4, figsize=(15, 9))\n",
    "fig.subplots_adjust(hspace=0.35, wspace=0.3)\n",
    "\n",
    "# Plot 1: The detector measure\n",
    "ax1 = axes[0, 0]\n",
    "x = np.linspace(0, 1, 1000)\n",
    "mu = detector_measure(x)\n",
    "ax1.plot(x, mu, 'g-', linewidth=2)\n",
    "ax1.fill_between(x, 0, mu, where=(mu > 0), alpha=0.3, color='green', label='μ = +1')\n",
    "ax1.fill_between(x, 0, mu, where=(mu <= 0), alpha=0.3, color='red', label='μ = -1')\n",
    "ax1.axvline(x=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "ax1.set_title('Detector Measure μ(x)', fontweight='bold', fontsize=10)\n",
    "ax1.set_xlabel('x', fontsize=9)\n",
    "ax1.set_ylabel('μ(x)', fontsize=9)\n",
    "ax1.legend(fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(labelsize=8)\n",
    "\n",
    "# Test functions and their integrals - ONLY 3 functions\n",
    "test_functions = [\n",
    "    (step_function, \"Step Function\", 'b'),\n",
    "    (lambda x: smooth_step(x, 50), \"Smooth Step\", 'orange'),\n",
    "    (sin_pi, \"sin(πx)\", 'purple')\n",
    "]\n",
    "\n",
    "# Plot each function and compute integral\n",
    "for idx, (func, name, color) in enumerate(test_functions):\n",
    "    ax = axes[0, idx + 1]\n",
    "    \n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    y = func(x)\n",
    "    mu = detector_measure(x)\n",
    "    \n",
    "    # Plot function\n",
    "    ax.plot(x, y, color=color, linewidth=2)\n",
    "    ax.fill_between(x, 0, y, alpha=0.2, color=color)\n",
    "    \n",
    "    # Overlay detector regions\n",
    "    ax.axvspan(0, 0.5, alpha=0.1, color='green')\n",
    "    ax.axvspan(0.5, 1, alpha=0.1, color='red')\n",
    "    ax.axvline(x=0.5, color='black', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Compute integral\n",
    "    integral = compute_integral(func, detector_measure)\n",
    "    \n",
    "    ax.set_title(f'{name}', fontweight='bold', fontsize=10)\n",
    "    ax.set_xlabel('x', fontsize=9)\n",
    "    ax.set_ylabel('f(x)', fontsize=9)\n",
    "    \n",
    "    # Place integral text inside plot to avoid overlap\n",
    "    ax.text(0.95, 0.05, f'∫f dμ = {integral:.3f}', \n",
    "            transform=ax.transAxes, ha='right', va='bottom',\n",
    "            fontweight='bold', fontsize=9,\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8),\n",
    "            color='red' if abs(integral) > 0.01 else 'green')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(labelsize=8)\n",
    "\n",
    "# Hide the empty plot in row 2, column 1\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Plot 7: Product visualization for step function\n",
    "ax7 = axes[1, 1]\n",
    "x = np.linspace(0, 1, 1000)\n",
    "y_step = step_function(x)\n",
    "mu = detector_measure(x)\n",
    "product = y_step * mu\n",
    "ax7.plot(x, product, 'b-', linewidth=2)\n",
    "ax7.fill_between(x, 0, product, where=(product > 0), alpha=0.3, color='blue')\n",
    "ax7.fill_between(x, 0, product, where=(product <= 0), alpha=0.3, color='red')\n",
    "ax7.set_title('Step × μ', fontweight='bold', fontsize=10)\n",
    "ax7.set_xlabel('x', fontsize=9)\n",
    "ax7.set_ylabel('f(x) × μ(x)', fontsize=9)\n",
    "integral = compute_integral(step_function, detector_measure)\n",
    "ax7.text(0.5, 0.95, f'∫ = {integral:.3f}', transform=ax7.transAxes,\n",
    "         ha='center', va='top', fontweight='bold', fontsize=10,\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow'))\n",
    "ax7.grid(True, alpha=0.3)\n",
    "ax7.tick_params(labelsize=8)\n",
    "\n",
    "# Plot 8: Product visualization for smooth step\n",
    "ax8 = axes[1, 2]\n",
    "y_smooth = smooth_step(x, 50)\n",
    "product = y_smooth * mu\n",
    "ax8.plot(x, product, 'orange', linewidth=2)\n",
    "ax8.fill_between(x, 0, product, where=(product > 0), alpha=0.3, color='orange')\n",
    "ax8.fill_between(x, 0, product, where=(product <= 0), alpha=0.3, color='brown')\n",
    "ax8.set_title('Smooth Step × μ', fontweight='bold', fontsize=10)\n",
    "ax8.set_xlabel('x', fontsize=9)\n",
    "ax8.set_ylabel('f(x) × μ(x)', fontsize=9)\n",
    "integral = compute_integral(lambda x: smooth_step(x, 50), detector_measure)\n",
    "ax8.text(0.5, 0.95, f'∫ ≈ {integral:.3f}', transform=ax8.transAxes,\n",
    "         ha='center', va='top', fontweight='bold', fontsize=10,\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow'))\n",
    "ax8.grid(True, alpha=0.3)\n",
    "ax8.tick_params(labelsize=8)\n",
    "\n",
    "# Plot 9: Product visualization for sin(πx)\n",
    "ax9 = axes[1, 3]\n",
    "y_sin = sin_pi(x)\n",
    "product = y_sin * mu\n",
    "ax9.plot(x, product, 'purple', linewidth=2)\n",
    "ax9.fill_between(x, 0, product, where=(product > 0), alpha=0.3, color='purple')\n",
    "ax9.fill_between(x, 0, product, where=(product <= 0), alpha=0.3, color='pink')\n",
    "ax9.set_title('sin(πx) × μ', fontweight='bold', fontsize=10)\n",
    "ax9.set_xlabel('x', fontsize=9)\n",
    "ax9.set_ylabel('f(x) × μ(x)', fontsize=9)\n",
    "integral = compute_integral(sin_pi, detector_measure)\n",
    "ax9.text(0.5, 0.95, f'∫ = {integral:.3f}', transform=ax9.transAxes,\n",
    "         ha='center', va='top', fontweight='bold', fontsize=10,\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow'))\n",
    "ax9.grid(True, alpha=0.3)\n",
    "ax9.tick_params(labelsize=8)\n",
    "\n",
    "# Neural network approximation comparison\n",
    "ax10 = axes[2, 0]\n",
    "steepness_values = [5, 10, 20, 50, 100, 200]\n",
    "integrals = []\n",
    "for s in steepness_values:\n",
    "    integral = compute_integral(lambda x: smooth_step(x, s), detector_measure)\n",
    "    integrals.append(integral)\n",
    "\n",
    "ax10.plot(steepness_values, integrals, 'ro-', linewidth=2, markersize=6)\n",
    "ax10.axhline(y=-0.5, color='b', linestyle='--', label='True step', linewidth=1.5)\n",
    "ax10.axhline(y=0, color='g', linestyle='--', alpha=0.5, label='Zero', linewidth=1.5)\n",
    "ax10.set_xlabel('Sigmoid Steepness', fontsize=9)\n",
    "ax10.set_ylabel('∫f dμ', fontsize=9)\n",
    "ax10.set_title('NN Approx. vs True Step', fontweight='bold', fontsize=10)\n",
    "ax10.legend(fontsize=8)\n",
    "ax10.grid(True, alpha=0.3)\n",
    "ax10.tick_params(labelsize=8)\n",
    "\n",
    "# Summary text box\n",
    "ax11 = axes[2, 1]\n",
    "ax11.axis('off')\n",
    "summary_text = \"\"\"KEY INSIGHTS:\n",
    "\n",
    "• Step function: ∫ = -0.5\n",
    "• Smooth approx: ∫ ≈ 0\n",
    "• sin(πx): ∫ = 0\n",
    "\n",
    "As NN gets steeper:\n",
    "integral → 0 (not -0.5!)\n",
    "\n",
    "NNs cannot capture\n",
    "true discontinuities\"\"\"\n",
    "\n",
    "ax11.text(0.1, 0.9, summary_text, fontsize=9, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "# Main conclusion\n",
    "ax12 = axes[2, 2]\n",
    "ax12.axis('off')\n",
    "ax12.text(0.5, 0.75, '✓ Continuous', fontsize=12, ha='center', color='green', fontweight='bold')\n",
    "ax12.text(0.5, 0.6, 'NNs CAN\\napproximate', fontsize=9, ha='center')\n",
    "ax12.text(0.5, 0.3, '✗ Discontinuous', fontsize=12, ha='center', color='red', fontweight='bold')\n",
    "ax12.text(0.5, 0.15, 'NNs CANNOT\\ncapture jumps', fontsize=9, ha='center')\n",
    "\n",
    "# Hide unused subplot\n",
    "axes[2, 3].axis('off')\n",
    "\n",
    "plt.suptitle('What Neural Networks Can and Cannot Approximate: The Role of Continuity', \n",
    "             fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.show()\n",
    "\n",
    "# Print numerical results table\n",
    "print(\"Integral Results with Detector μ(x):\")\n",
    "print(\"-\" * 45)\n",
    "for func, name, _ in test_functions:\n",
    "    integral = compute_integral(func, detector_measure)\n",
    "    print(f\"{name:<20}: ∫f dμ = {integral:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: The Continuity Requirement\n",
    "\n",
    "The detector analysis reveals why UAT specifically applies to continuous functions:\n",
    "\n",
    "- **True step function has non-zero integral** (-0.5) with our detector\n",
    "- **Continuous functions** (including NN approximations) have ≈0 integral  \n",
    "- **This detector can distinguish discontinuous from continuous functions**\n",
    "\n",
    "Therefore, neural networks with continuous activations cannot perfectly approximate discontinuous functions in the supremum norm. This is why the Universal Approximation Theorem is stated for the space C([a,b]) of continuous functions, not for broader function spaces that include discontinuities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Detector Works for Discontinuities\n",
    "\n",
    "The key insight is that this detector measure $\\mu$ defined as:\n",
    "$$\\mu = \\begin{cases} +1 & \\text{on } [0, 0.5) \\\\ -1 & \\text{on } [0.5, 1] \\end{cases}$$\n",
    "\n",
    "has a special property:\n",
    "\n",
    "1. **For the step function** $H(x) = \\chi_{[0.5,1]}(x)$:\n",
    "   $$\\int_0^1 H(x) d\\mu = \\int_0^{0.5} 0 \\cdot 1 dx + \\int_{0.5}^1 1 \\cdot (-1) dx = -0.5$$\n",
    "\n",
    "2. **For any continuous function** $f$ that crosses smoothly at $x = 0.5$:\n",
    "   - The positive contribution from $[0, 0.5)$ tends to cancel with negative contribution from $[0.5, 1]$\n",
    "   - Specifically, if $f(0.5^-) = f(0.5^+)$ (continuous), the integral approaches 0\n",
    "\n",
    "3. **For neural network approximations** with continuous activations:\n",
    "   - They can only create continuous functions\n",
    "   - As they try to approximate the step more closely (steeper sigmoids), the integral still → 0, not -0.5\n",
    "   - This is because the \"jump\" is always smoothed out over some interval\n",
    "\n",
    "This demonstrates a fundamental limitation: **Neural networks with continuous activations form a dense subset of C([a,b]) but NOT of L¹([a,b]) or other spaces that include discontinuous functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural networks CAN approximate discontinuous functions in L² sense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "# Train a neural network to approximate a step function\n",
    "class StepApproximator(nn.Module):\n",
    "    def __init__(self, width=100):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, width),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(width, width),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(width, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Generate training data\n",
    "x_train = torch.linspace(0, 1, 200).reshape(-1, 1)\n",
    "y_train = (x_train > 0.5).float()\n",
    "\n",
    "# Train model\n",
    "model = StepApproximator(width=50)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate\n",
    "x_test = torch.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_test).numpy().flatten()\n",
    "\n",
    "x_test_np = x_test.numpy().flatten()\n",
    "y_true = (x_test_np > 0.5).astype(float)\n",
    "\n",
    "# Compute different norms\n",
    "l2_error = np.sqrt(np.mean((y_pred - y_true)**2))\n",
    "l_inf_error = np.max(np.abs(y_pred - y_true))\n",
    "l1_error = np.mean(np.abs(y_pred - y_true))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: The approximation\n",
    "axes[0].plot(x_test_np, y_true, 'b-', linewidth=2, label='True Step')\n",
    "axes[0].plot(x_test_np, y_pred, 'r--', linewidth=2, label='NN Approximation')\n",
    "axes[0].axvline(x=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].set_xlim(0, 1.1)\n",
    "axes[0].set_ylim(-0.1, 1.1)\n",
    "axes[0].set_title('Step Function Approximation')\n",
    "axes[0].legend(loc='center left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add zoom inset to show approximation detail around discontinuity\n",
    "inset = inset_axes(axes[0], width=\"40%\", height=\"40%\", loc='lower right')\n",
    "zoom_mask = (x_test_np >= 0.4) & (x_test_np <= 0.6)\n",
    "x_zoom = x_test_np[zoom_mask]\n",
    "y_true_zoom = y_true[zoom_mask]\n",
    "y_pred_zoom = y_pred[zoom_mask]\n",
    "\n",
    "inset.plot(x_zoom, y_true_zoom, 'b-', linewidth=2, label='True Step')\n",
    "inset.plot(x_zoom, y_pred_zoom, 'r--', linewidth=2, label='NN Approximation')\n",
    "inset.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "inset.set_xlim(0.49, 0.51)\n",
    "inset.set_ylim(0.95, 1.005)\n",
    "inset.grid(True, alpha=0.3)\n",
    "inset.set_title('Zoom: Discontinuity Region', fontsize=10)\n",
    "\n",
    "# Plot 2: Error in different regions\n",
    "axes[1].plot(x_test_np, np.abs(y_pred - y_true), 'g-', linewidth=2)\n",
    "axes[1].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Discontinuity')\n",
    "axes[1].fill_between(x_test_np, 0, np.abs(y_pred - y_true), alpha=0.3, color='green')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('|Error|')\n",
    "axes[1].set_title('Pointwise Error')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Norm comparison\n",
    "norms = ['L²', 'L¹', 'L∞']\n",
    "errors = [l2_error, l1_error, l_inf_error]\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "bars = axes[2].bar(norms, errors, color=colors, alpha=0.7)\n",
    "axes[2].set_ylabel('Error')\n",
    "axes[2].set_title('Error in Different Norms')\n",
    "axes[2].set_ylim(0, max(errors) * 1.2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, error in zip(bars, errors):\n",
    "    height = bar.get_height()\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{error:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Add grid\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Neural Network Approximation of Discontinuous Functions', fontsize=14, fontweight='bold')\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "# Print numerical results only\n",
    "print(f\"L² norm error: {l2_error:.4f}\")\n",
    "print(f\"L¹ norm error: {l1_error:.4f}\")\n",
    "print(f\"L∞ norm error: {l_inf_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximation Quality in Different Norms\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "1. **Neural networks CAN approximate discontinuous functions in L² sense** - The L² error is small, meaning the overall \"energy\" of the approximation is good\n",
    "\n",
    "2. **The error concentrates near the discontinuity** - Away from x=0.5, the approximation is excellent; the error spike occurs in a narrow band around the jump\n",
    "\n",
    "3. **L∞ (pointwise) convergence is impossible for true discontinuities** - No matter how many neurons we use, there will always be a maximum error around 0.5 at the discontinuity\n",
    "\n",
    "4. **This is why UAT is stated for CONTINUOUS functions in supremum norm** - The theorem guarantees arbitrary accuracy only for continuous targets\n",
    "\n",
    "**Practical Implications:**\n",
    "- For most applications (integration, energy norms), neural network approximation works well even for discontinuous functions\n",
    "- For pointwise accuracy at discontinuities, neural networks have fundamental limits\n",
    "- Special techniques (adaptive activations, discontinuity tracking) may be needed for shock waves or material interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Practical Aspects\n",
    "### The Key Experiment: Width vs Approximation Quality\n",
    "\n",
    "**Hypothesis**: More neurons → better approximation (Universal Approximation Theorem)\n",
    "\n",
    "**Test**: Train networks with 5, 10, 20, 50 neurons and measure performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models with different hidden layer sizes\n",
    "hidden_sizes = [5, 10, 20, 50]\n",
    "epochs = 5000\n",
    "lr = 0.01\n",
    "\n",
    "single_layer_models = {}\n",
    "single_layer_losses = {}\n",
    "\n",
    "print(\"Training multiple models with different hidden layer sizes...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Ensure variables are defined for plotting\n",
    "x_plot = x_test\n",
    "u_analytical_plot = y_exact\n",
    "x_train_np = x_train\n",
    "u_train_noisy_np = y_train\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    print(f\"\\nTraining with {hidden_size} hidden neurons...\")\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = SingleLayerNN(hidden_size=hidden_size)\n",
    "    \n",
    "    # Train the model\n",
    "    losses = train_network(model, x_train_tensor, u_train_tensor, epochs=epochs, lr=lr)\n",
    "    \n",
    "    # Store the trained model and loss history\n",
    "    single_layer_models[hidden_size] = model\n",
    "    single_layer_losses[hidden_size] = losses\n",
    "    \n",
    "    # Handle different types of loss values\n",
    "    if isinstance(losses[-1], (list, np.ndarray)):\n",
    "        # If it's a list or array, get the first element\n",
    "        final_loss = float(losses[-1][0]) if len(losses[-1]) > 0 else float(losses[-1])\n",
    "    elif hasattr(losses[-1], 'item'):\n",
    "        # If it's a tensor with .item() method\n",
    "        final_loss = losses[-1].item()\n",
    "    else:\n",
    "        # If it's already a scalar\n",
    "        final_loss = float(losses[-1])\n",
    "    \n",
    "    print(f\"Final loss for {hidden_size} neurons: {final_loss:.6f}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Results: Approximation Quality vs. Width\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "x_test_tensor = torch.tensor(x_plot.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "for i, hidden_size in enumerate(hidden_sizes):\n",
    "    ax = axes[i]\n",
    "    model = single_layer_models[hidden_size]\n",
    "    \n",
    "    # Get predictions from the trained model\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        u_pred = model(x_test_tensor).numpy().flatten()\n",
    "    \n",
    "    # Plot true function\n",
    "    ax.plot(x_plot, u_analytical_plot, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "    \n",
    "    # Plot NN prediction\n",
    "    ax.plot(x_plot, u_pred, 'b-', linewidth=2.5, \n",
    "            label=f'NN ({hidden_size} neurons)')\n",
    "    \n",
    "    # Plot training data\n",
    "    ax.scatter(x_train_np, u_train_noisy_np, color='red', s=40, alpha=0.7, \n",
    "              label='Training Data', zorder=5)\n",
    "    \n",
    "    # Calculate and display error metrics\n",
    "    mse = np.mean((u_pred - u_analytical_plot)**2)\n",
    "    max_error = np.max(np.abs(u_pred - u_analytical_plot))\n",
    "    \n",
    "    ax.text(0.05, 0.95, f'MSE: {mse:.6f}\\nMax Error: {max_error:.4f}', \n",
    "            transform=ax.transAxes, \n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "            verticalalignment='top', fontsize=10)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('u(x)')\n",
    "    ax.set_title(f'Single Layer: {hidden_size} Neurons', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-0.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Results: Training Convergence vs. Width\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for hidden_size in hidden_sizes:\n",
    "    losses = single_layer_losses[hidden_size]\n",
    "    plt.semilogy(losses, label=f'{hidden_size} neurons', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss (log scale)')\n",
    "plt.title('Training Convergence', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "final_losses = [single_layer_losses[h][-1] for h in hidden_sizes]\n",
    "plt.loglog(hidden_sizes, final_losses, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Neurons')\n",
    "plt.ylabel('Final Loss (log scale)')\n",
    "plt.title('Final Loss vs Network Width', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Loss Summary:\")\n",
    "for hidden_size in hidden_sizes:\n",
    "    loss = single_layer_losses[hidden_size][-1]\n",
    "    print(f\"{hidden_size:2d} neurons: {loss:.8f}\")\n",
    "\n",
    "print(f\"\\nImprovement from 5 to 50 neurons: {final_losses[0]/final_losses[-1]:.1f}x better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Approximation, Width, and Practicalities\n",
    "\n",
    "The visualizations from our experiment (Cells 18 & 19) show a clear trend: as we increased the number of neurons in the hidden layer (the network's **width**), the network's ability to approximate the $\\sin(\\pi x)$ function significantly improved, and the final training loss decreased.\n",
    "\n",
    "This experimental result **experimentally validates** the statement of the **Universal Approximation Theorem** (from Cell 5) – a single hidden layer with non-linearity *does* have the capacity to approximate continuous functions, and increasing the number of neurons provides more of this capacity, allowing it to better fit the target function.\n",
    "\n",
    "However, the theorem guarantees existence, not practicality. Our experiment also hints at practical considerations:\n",
    "\n",
    "*   **Number of Neurons Needed**: While 50 neurons did a good job for $\\sin(\\pi x)$, approximating more complex functions might require a very large number of neurons in a single layer. This can be computationally expensive and require a lot of data.\n",
    "*   **Training Difficulty**: The theorem doesn't guarantee that gradient descent will successfully *find* the optimal parameters. Training can be challenging, especially for very wide networks or complex functions.\n",
    "*   **Remaining Errors**: Even with 50 neurons, there's still some error. For more complex functions or higher accuracy requirements, a single layer might struggle or need excessive width.\n",
    "\n",
    "### Practical Considerations: Overfitting and Hyperparameters\n",
    "\n",
    "As network capacity increases (e.g., by adding more neurons), there's a risk of **overfitting**. This occurs when the model learns the training data (including noise) too well, capturing spurious patterns that don't generalize to unseen data, leading to poor performance outside the training set.\n",
    "\n",
    "![overfitting](figs/overfitting.png?raw=true)\n",
    "\n",
    "> Example of under and overfitting the data\n",
    "\n",
    "Overfitting can be detected by monitoring performance on a separate **validation set** during training. If the validation loss starts increasing while the training loss continues to decrease, it's a sign of overfitting.\n",
    "\n",
    "Strategies to mitigate overfitting include using more training data, regularization techniques, early stopping (stopping training when validation performance degrades), or reducing model complexity.\n",
    "\n",
    "**Problem**: High-capacity networks can memorize training data instead of learning the true function\n",
    "\n",
    "**Detection**: Monitor validation loss - if it increases while training loss decreases, you're overfitting\n",
    "\n",
    "**Solutions**: \n",
    "- More training data\n",
    "- Regularization (L1/L2, dropout)\n",
    "- Early stopping\n",
    "- Simpler architectures\n",
    "\n",
    "### Hyperparameter Choices\n",
    "Hyperparameters are settings chosen *before* training that significantly influence the learning process and the final model. Key hyperparameters we've encountered include:\n",
    "\n",
    "*   **Learning Rate** ($\\eta$): Controls the step size in gradient descent. Too high can cause divergence; too low can lead to slow convergence or getting stuck in local minima.\n",
    "*   **Number of Epochs**: How many times the training data is passed through the network. Too few may result in underfitting; too many can cause overfitting.\n",
    "*   **Hidden Layer Size** ($N_h$): The number of neurons in the hidden layer. Impacts model capacity. Too small can underfit; too large can overfit.\n",
    "*   **Choice of Activation Function**: Impacts the network's ability to model specific shapes and the training dynamics (e.g., Tanh/Sigmoid for smooth functions but potential vanishing gradients, ReLU for efficiency but \"dead neuron\" issue). The best choice can be problem-dependent.\n",
    "\n",
    "Finding the right balance of hyperparameters is crucial for successful training and generalization.\n",
    "\n",
    "#### How to detect overfitting with validation dataset\n",
    "\n",
    "In practice, the learning algorithm does not actually ﬁnd the best function, but merely one thatsigniﬁcantly reduces the training error. These additional limitations, such as theimperfection of the optimization algorithm, mean that the learning algorithm’seﬀective capacitymay be less than the representational capacity of the modelfamily.\n",
    "\n",
    "Our modern ideas about improving the generalization of machine learningmodels are reﬁnements of thought dating back to philosophers at least as early as Ptolemy. Many early scholars invoke a principle of parsimony that is now mostwidely known as `Occam’s razor` (c. 1287–1347). This principle states that amongcompeting hypotheses that explain known observations equally well, we shouldchoose the “simplest” one. This idea was formalized and made more precise in the twentieth century by the founders of statistical learning theory.\n",
    "\n",
    "We must remember that while simpler functions are more likely to generalize(to have a small gap between training and test error), we must still choose asuﬃciently complex hypothesis to achieve low training error. Typically, trainingerror decreases until it asymptotes to the minimum possible error value as modelcapacity increases (assuming the error measure has a minimum value). Typically generalization error has a U-shaped curve as a function of model capacity.\n",
    "\n",
    "At the left end of the graph, training error and generalization errorare both high. This is the **underfitting regime**. As we increase capacity, training error decreases, but the gap between training and generalization error increases. Eventually,the size of this gap outweighs the decrease in training error, and we enter the **overfitting regime**, where capacity is too large, above the **optimal capacity**.\n",
    "\n",
    "![Training validation fit](figs/training-validation-fit.png)\n",
    "\n",
    "> Image credits: Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Demonstrating potential overfitting with a very wide network...\")\n",
    "\n",
    "# Define analytical solution function if not already defined\n",
    "def analytical_solution(x):\n",
    "    \"\"\"The true solution: sin(πx)\"\"\"\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Train a very wide network\n",
    "width = 500\n",
    "wide_model = SingleLayerNN(width)\n",
    "wide_losses = train_network(wide_model, x_train_tensor, u_train_tensor, epochs=5000, lr=0.01)\n",
    "\n",
    "normal_model = SingleLayerNN(50)\n",
    "normal_losses = train_network(normal_model, x_train_tensor, u_train_tensor, epochs=5000, lr=0.01)\n",
    "\n",
    "# Compare to our best previous model\n",
    "x_test = np.linspace(0, 1, 200)\n",
    "x_test_tensor = torch.tensor(x_test.reshape(-1, 1), dtype=torch.float32)\n",
    "u_true_test = analytical_solution(x_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    wide_pred = wide_model(x_test_tensor).numpy().flatten()\n",
    "    normal_pred = normal_model(x_test_tensor).numpy().flatten()\n",
    "\n",
    "# Compare networks\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x_test, u_true_test, 'k-', linewidth=3, label='True Function')\n",
    "plt.plot(x_test, normal_pred, 'b-', linewidth=3, label='50 Neurons')\n",
    "plt.scatter(x_train, y_train, color='red', s=40, alpha=0.7, zorder=5)\n",
    "plt.title('Normal Network (50 neurons)', fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x_test, u_true_test, 'k-', linewidth=3, label='True Function')\n",
    "plt.plot(x_test, wide_pred, 'r-', linewidth=3, label=f'{width:d} Neurons')\n",
    "plt.scatter(x_train, y_train, color='red', s=40, alpha=0.7, zorder=5)\n",
    "plt.title(f'Very Wide Network ({width:d} neurons)', fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.semilogy(wide_losses, label=f'{width:d} Neurons', color='red', linewidth=2)\n",
    "plt.semilogy(normal_losses, label='50 Neurons', color='blue', linewidth=2)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Training Loss Comparison', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Width Impact: More Parameters Can Lead to Overfitting', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "wide_error = np.max(np.abs(wide_pred - u_true_test))\n",
    "normal_error = np.max(np.abs(normal_pred - u_true_test))\n",
    "\n",
    "print(f\"Max error - 50 neurons: {normal_error:.4e}\")\n",
    "print(f\"Max error - {width} neurons: {wide_error:.4e}\")\n",
    "print(f\"Final loss - 50 neurons: {normal_losses[-1]:.4e}\")\n",
    "print(f\"Final loss - {width} neurons: {wide_losses[-1]:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The story so far**: Single-layer networks can approximate any function (Universal Approximation) but may need impractically many neurons. **The question**: Can depth be more efficient than width?\n",
    "\n",
    "### The Need for Depth - The XOR Problem: A Historical Turning Point\n",
    "\n",
    "The XOR problem exposed fundamental limitations of **true** single-layer perceptrons, causing the \"AI winter\" of the 1970s. This simple problem reveals why depth is essential.\n",
    "\n",
    "**XOR Truth Table**:\n",
    "```\n",
    "x₁  x₂  │  y\n",
    "────────┼────\n",
    " 0   0  │  0\n",
    " 0   1  │  1  \n",
    " 1   0  │  1\n",
    " 1   1  │  0\n",
    "```\n",
    "\n",
    "**The crisis**: No single line can separate these classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (14, 8)})\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_xor = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X_xor_tensor = torch.tensor(X_xor)\n",
    "y_xor_tensor = torch.tensor(y_xor)\n",
    "\n",
    "print(\"XOR Dataset:\")\n",
    "for i in range(4):\n",
    "    print(f\"({X_xor[i,0]}, {X_xor[i,1]}) → {y_xor[i,0]}\")\n",
    "\n",
    "# Visualize the impossibility\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: XOR problem with failed linear attempts\n",
    "colors = ['red', 'blue']\n",
    "for i in range(2):\n",
    "    mask = y_xor.flatten() == i\n",
    "    ax1.scatter(X_xor[mask, 0], X_xor[mask, 1], \n",
    "               c=colors[i], s=200, alpha=0.8, \n",
    "               label=f'Class {i}', edgecolors='black', linewidth=2)\n",
    "\n",
    "# Failed linear separation attempts\n",
    "x_line = np.linspace(-0.5, 1.5, 100)\n",
    "ax1.plot(x_line, 0.5 * np.ones_like(x_line), 'g--', linewidth=2, alpha=0.7, label='Failed Line 1')\n",
    "ax1.plot(0.5 * np.ones_like(x_line), x_line, 'm--', linewidth=2, alpha=0.7, label='Failed Line 2')\n",
    "ax1.plot(x_line, x_line, 'orange', linestyle='--', linewidth=2, alpha=0.7, label='Failed Line 3')\n",
    "\n",
    "ax1.set_xlim(-0.3, 1.3)\n",
    "ax1.set_ylim(-0.3, 1.3)\n",
    "ax1.set_xlabel('x₁')\n",
    "ax1.set_ylabel('x₂')\n",
    "ax1.set_title('XOR: No Linear Separation Possible', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Required non-linear boundary\n",
    "for i in range(2):\n",
    "    mask = y_xor.flatten() == i\n",
    "    ax2.scatter(X_xor[mask, 0], X_xor[mask, 1], \n",
    "               c=colors[i], s=200, alpha=0.8, \n",
    "               label=f'Class {i}', edgecolors='black', linewidth=2)\n",
    "\n",
    "# Conceptual non-linear boundary\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "x_circle1 = 0.25 + 0.15*np.cos(theta)\n",
    "y_circle1 = 0.25 + 0.15*np.sin(theta)\n",
    "x_circle2 = 0.75 + 0.15*np.cos(theta)\n",
    "y_circle2 = 0.75 + 0.15*np.sin(theta)\n",
    "\n",
    "ax2.plot(x_circle1, y_circle1, 'g-', linewidth=3, alpha=0.8, label='Required Boundary')\n",
    "ax2.plot(x_circle2, y_circle2, 'g-', linewidth=3, alpha=0.8)\n",
    "\n",
    "ax2.set_xlim(-0.3, 1.3)\n",
    "ax2.set_ylim(-0.3, 1.3)\n",
    "ax2.set_xlabel('x₁')\n",
    "ax2.set_ylabel('x₂')\n",
    "ax2.set_title('Required Non-Linear Boundary', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe fundamental problem: XOR is NOT linearly separable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Critical Distinction: True Single-Layer vs Multi-Layer\n",
    "\n",
    "**Historical confusion**: What Minsky & Papert analyzed was a **TRUE** single-layer perceptron (Input → Output directly). This is different from our \"single-layer\" networks that have hidden layers!\n",
    "\n",
    "**Architecture comparison**:\n",
    "- **True Single-Layer**: Input → Output (NO hidden layers)\n",
    "- **Multi-Layer**: Input → Hidden → Output (1+ hidden layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architectures correctly\n",
    "class TrueSingleLayerPerceptron(nn.Module):\n",
    "    \"\"\"TRUE single-layer perceptron: Input → Output (NO hidden layers)\n",
    "    This is what Minsky & Papert showed cannot solve XOR!\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2, 1)  # Direct: 2 inputs → 1 output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.layer(x))\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron: Input → Hidden → Output\n",
    "    This CAN solve XOR!\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=4):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = torch.sigmoid(self.hidden(x))\n",
    "        return torch.sigmoid(self.output(h))\n",
    "\n",
    "def train_xor_model(model, X, y, epochs=3000, lr=10.0):\n",
    "    \"\"\"Train model on XOR problem\"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            accuracy = ((pred > 0.5).float() == y).float().mean()\n",
    "            print(f'Epoch {epoch+1}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.4f}')\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "print(\"Network architectures defined:\")\n",
    "print(\"1. TrueSingleLayerPerceptron: Input → Output (what fails)\")\n",
    "print(\"2. MultiLayerPerceptron: Input → Hidden → Output (what works)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Historical Failure: True Single-Layer on XOR\n",
    "\n",
    "**Prediction**: The true single-layer perceptron will fail spectacularly at XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the historical failure\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING TRUE SINGLE-LAYER PERCEPTRON\")\n",
    "print(\"(This is what Minsky & Papert showed fails!)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "true_single = TrueSingleLayerPerceptron()\n",
    "true_single_loss = train_xor_model(true_single, X_xor_tensor, y_xor_tensor)\n",
    "\n",
    "# Analyze the failure\n",
    "with torch.no_grad():\n",
    "    pred = true_single(X_xor_tensor)\n",
    "    accuracy = ((pred > 0.5).float() == y_xor_tensor).float().mean()\n",
    "    \n",
    "    print(f\"\\nFINAL RESULTS:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} (should be ~0.5 = random guessing)\")\n",
    "    print(f\"Final loss: {true_single_loss:.4f}\")\n",
    "    print(\"\\nPredictions vs Targets:\")\n",
    "    for i in range(4):\n",
    "        print(f\"  ({X_xor[i,0]}, {X_xor[i,1]}) → {pred[i,0]:.4f} (target: {y_xor[i,0]})\")\n",
    "    \n",
    "    print(\"\\n❌ FAILURE CONFIRMED: True single-layer cannot solve XOR!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Solution: Adding Hidden Layers\n",
    "\n",
    "**Hypothesis**: Adding just ONE hidden layer should solve XOR completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the solution\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MULTI-LAYER PERCEPTRON\")\n",
    "print(\"(Adding ONE hidden layer should solve XOR!)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "multi_layer = MultiLayerPerceptron(4)\n",
    "multi_layer_loss = train_xor_model(multi_layer, X_xor_tensor, y_xor_tensor)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = multi_layer(X_xor_tensor)\n",
    "    accuracy = ((pred > 0.5).float() == y_xor_tensor).float().mean()\n",
    "    \n",
    "    print(f\"\\nFINAL RESULTS:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} (should be 1.0000!)\")\n",
    "    print(f\"Final loss: {multi_layer_loss:.4f}\")\n",
    "    print(\"\\nPredictions vs Targets:\")\n",
    "    for i in range(4):\n",
    "        print(f\"  ({X_xor[i,0]}, {X_xor[i,1]}) → {pred[i,0]:.4f} (target: {y_xor[i,0]})\")\n",
    "    \n",
    "    print(\"\\n✅ SUCCESS: Multi-layer network solves XOR perfectly!\")\n",
    "    print(\"\\nImprovement factor: Infinite (from failure to perfect solution)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Decision Boundaries\n",
    "\n",
    "**The geometric insight**: Linear vs non-linear decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision boundary visualization\n",
    "def plot_decision_boundary(model, title, ax):\n",
    "    h = 0.01\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    grid_points = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        Z = model(grid_points).numpy()\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Create contour plot\n",
    "    contour = ax.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=3)\n",
    "    \n",
    "    # Plot XOR points\n",
    "    colors = ['red', 'blue']\n",
    "    markers = ['o', 's']\n",
    "    for i in range(2):\n",
    "        mask = y_xor.flatten() == i\n",
    "        ax.scatter(X_xor[mask, 0], X_xor[mask, 1], \n",
    "                  c=colors[i], s=300, alpha=1.0, \n",
    "                  edgecolors='black', linewidth=3,\n",
    "                  marker=markers[i], label=f'Class {i}')\n",
    "    \n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_title(title, fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel('x₁')\n",
    "    ax.set_ylabel('x₂')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Create comparison plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "plot_decision_boundary(true_single, 'True Single-Layer\\n(Linear - FAILS)', ax1)\n",
    "plot_decision_boundary(multi_layer, 'Multi-Layer\\n(Non-Linear - SUCCEEDS)', ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"• True single-layer: Can only create straight lines → FAILS\")\n",
    "print(\"• Multi-layer: Creates curved boundaries → SUCCEEDS\")\n",
    "print(\"• Hidden layers enable non-linear transformations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Explanation: Why Depth Solves XOR\n",
    "\n",
    "**True single-layer limitation**: \n",
    "$$y = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
    "Decision boundary: $w_1 x_1 + w_2 x_2 + b = 0$ (always a straight line)\n",
    "\n",
    "**Multi-layer solution**: Decompose XOR into simpler operations\n",
    "$$h_1 = \\sigma(w_{11} x_1 + w_{12} x_2 + b_1) \\quad \\text{(≈ OR gate)}$$\n",
    "$$h_2 = \\sigma(w_{21} x_1 + w_{22} x_2 + b_2) \\quad \\text{(≈ AND gate)}$$\n",
    "$$y = \\sigma(v_1 h_1 + v_2 h_2 + b_3) \\quad \\text{(≈ OR AND NOT)}$$\n",
    "\n",
    "**Result**: XOR = (OR) AND (NOT AND) = compositional solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Implications for SciML\n",
    "\n",
    "#### 1. Width vs Depth Trade-offs\n",
    "\n",
    "- **UAT (single layer)**: Requires exponentially many neurons for some functions\n",
    "- **Deep networks**: Can be exponentially more efficient\n",
    "- **Approximation rates**: \n",
    "  - Single layer: $O(n^{-1/d})$ for $d$-dimensional inputs\n",
    "  - Deep networks: Can achieve $O(e^{-cn})$ for smooth functions\n",
    "  \n",
    "#### Beyond XOR: High-Frequency Functions\n",
    "\n",
    "**The deeper question**: Does the depth advantage extend beyond simple classification?\n",
    "\n",
    "**Test case**: High-frequency function $f(x) = \\sin(\\pi x) + 0.3\\sin(10\\pi x)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-frequency function challenge\n",
    "def high_freq_function(x):\n",
    "    return np.sin(np.pi * x) + 0.3 * np.sin(10 * np.pi * x)\n",
    "\n",
    "# Generate data\n",
    "x_hf = np.linspace(0, 1, 200)\n",
    "y_hf_true = high_freq_function(x_hf)\n",
    "\n",
    "# Sparse training data\n",
    "x_hf_train = np.linspace(0, 1, 25)\n",
    "y_hf_train = high_freq_function(x_hf_train) + 0.01 * np.random.randn(25)\n",
    "\n",
    "# Convert to tensors\n",
    "x_hf_train_t = torch.tensor(x_hf_train.reshape(-1, 1), dtype=torch.float32)\n",
    "y_hf_train_t = torch.tensor(y_hf_train.reshape(-1, 1), dtype=torch.float32)\n",
    "x_hf_test_t = torch.tensor(x_hf.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Define architectures\n",
    "class ShallowNetwork(nn.Module):\n",
    "    \"\"\"Single hidden layer with many neurons\"\"\"\n",
    "    def __init__(self, width=100):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(1, width),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(width, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class DeepNetwork(nn.Module):\n",
    "    \"\"\"Multiple hidden layers with fewer neurons each\"\"\"\n",
    "    def __init__(self, width=25, depth=4):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(1, width), nn.Tanh()]\n",
    "        for _ in range(depth-1):\n",
    "            layers.extend([nn.Linear(width, width), nn.Tanh()])\n",
    "        layers.append(nn.Linear(width, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_regressor(model, x_train, y_train, epochs=5000, lr=0.01):\n",
    "    \"\"\"Train regression model\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        pred = model(x_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f'Epoch {epoch+1}: Loss = {loss.item():.6f}')\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Train models\n",
    "print(\"Training shallow network (1 layer, 100 neurons)...\")\n",
    "shallow_net = ShallowNetwork(100)\n",
    "shallow_loss = train_regressor(shallow_net, x_hf_train_t, y_hf_train_t)\n",
    "\n",
    "print(\"\\nTraining deep network (4 layers, 25 neurons each)...\")\n",
    "deep_net = DeepNetwork(25, 4)\n",
    "deep_loss = train_regressor(deep_net, x_hf_train_t, y_hf_train_t)\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"Shallow final loss: {shallow_loss:.6f}\")\n",
    "print(f\"Deep final loss: {deep_loss:.6f}\")\n",
    "print(f\"Improvement: {shallow_loss/deep_loss:.1f}x better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize high-frequency results\n",
    "with torch.no_grad():\n",
    "    shallow_pred = shallow_net(x_hf_test_t).numpy().flatten()\n",
    "    deep_pred = deep_net(x_hf_test_t).numpy().flatten()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Shallow network\n",
    "ax1.plot(x_hf, y_hf_true, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "ax1.plot(x_hf, shallow_pred, 'r-', linewidth=2, label='Shallow Network (100 neurons)')\n",
    "ax1.scatter(x_hf_train, y_hf_train, color='blue', s=30, alpha=0.7, zorder=5)\n",
    "\n",
    "shallow_mse = np.mean((shallow_pred - y_hf_true)**2)\n",
    "ax1.text(0.05, 0.95, f'MSE: {shallow_mse:.4f}', transform=ax1.transAxes,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "         fontsize=12, fontweight='bold')\n",
    "\n",
    "ax1.set_title('Shallow Network (1 Hidden Layer)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "\n",
    "# Deep network\n",
    "ax2.plot(x_hf, y_hf_true, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "ax2.plot(x_hf, deep_pred, 'g-', linewidth=2, label='Deep Network (4 layers)')\n",
    "ax2.scatter(x_hf_train, y_hf_train, color='blue', s=30, alpha=0.7, zorder=5)\n",
    "\n",
    "deep_mse = np.mean((deep_pred - y_hf_true)**2)\n",
    "ax2.text(0.05, 0.95, f'MSE: {deep_mse:.4f}', transform=ax2.transAxes,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "         fontsize=12, fontweight='bold')\n",
    "\n",
    "ax2.set_title('Deep Network (4 Hidden Layers)', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "\n",
    "plt.suptitle('High-Frequency Function: Shallow vs Deep', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Parameter comparison\n",
    "shallow_params = sum(p.numel() for p in shallow_net.parameters())\n",
    "deep_params = sum(p.numel() for p in deep_net.parameters())\n",
    "\n",
    "print(f\"\\nParameter Efficiency:\")\n",
    "print(f\"Shallow network: {shallow_params} parameters, MSE: {shallow_mse:.6f}\")\n",
    "print(f\"Deep network: {deep_params} parameters, MSE: {deep_mse:.6f}\")\n",
    "print(f\"\\nDeep network: {shallow_mse/deep_mse:.1f}x better performance\")\n",
    "print(f\"              {shallow_params/deep_params:.1f}x more parameters\")\n",
    "print(f\"\\nConclusion: Deep networks are more parameter-efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Timeline: From Crisis to Revolution\n",
    "\n",
    "**The XOR crisis and its resolution transformed AI:**\n",
    "\n",
    "| Year | Event | Impact |\n",
    "|------|-------|--------|\n",
    "| 1943 | McCulloch-Pitts neuron | Foundation laid |\n",
    "| 1957 | Rosenblatt's Perceptron | First learning success |\n",
    "| **1969** | **Minsky & Papert: XOR problem** | **Showed true single-layer limits** |\n",
    "| 1970s-80s | \"AI Winter\" | Funding dried up |\n",
    "| 1986 | Backpropagation algorithm | Enabled multi-layer training |\n",
    "| 1989 | Universal Approximation Theorem | Theoretical foundation |\n",
    "| 2006+ | Deep Learning Revolution | Depth proves essential |\n",
    "\n",
    "**The lesson**: XOR taught us that **depth is not luxury—it's necessity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Depth Matters: The Four Key Insights\n",
    "\n",
    "1. **Representation Efficiency**\n",
    "- **Shallow networks**: May need exponentially many neurons\n",
    "- **Deep networks**: Hierarchical composition is exponentially more efficient\n",
    "- **Example**: XOR impossible with 1 layer, trivial with 2 layers\n",
    "\n",
    "2. **Feature Hierarchy**\n",
    "- **Layer 1**: Simple features (edges, basic patterns)\n",
    "- **Layer 2**: Feature combinations (corners, textures)\n",
    "- **Layer 3+**: Complex abstractions (objects, concepts)\n",
    "- **Key insight**: Real-world problems have hierarchical structure\n",
    "\n",
    "3. **Geometric Transformation**\n",
    "- Each layer performs **coordinate transformation**\n",
    "- Deep networks \"unfold\" complex data manifolds\n",
    "- **XOR example**: Transform non-separable → separable\n",
    "- **General principle**: Depth enables progressive simplification\n",
    "\n",
    "4. **Compositional Learning**\n",
    "- Complex functions = composition of simple functions\n",
    "- **Mathematical**: $f(x) = f_L(f_{L-1}(...f_1(x)))$\n",
    "- **Practical**: Build complexity incrementally\n",
    "- **Universal**: Applies across domains (vision, language, science)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📖 Reading Activity: [Do ML models memorize or generalize?](https://pair.withgoogle.com/explorables/grokking/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
