# MLP Module - Neural Networks for Function Approximation

This module has been reorganized into focused, manageable notebooks:

## Core Notebooks (Start Here)

1. **`01-problem-setup.ipynb`** - Problem Setup & Motivation
   - 1D Poisson equation
   - Function approximation challenge
   - Traditional methods vs neural networks

2. **`02-perceptron-basics.ipynb`** - The Perceptron
   - Linear perceptron
   - Activation functions
   - Gradient descent from scratch
   - Limitations and solutions

3. **`03-neural-networks.ipynb`** - Neural Networks
   - Multi-layer networks
   - Backpropagation
   - Training and visualization
   - Comparison with polynomials

4. **`04-universal-approximation.ipynb`** - Universal Approximation Theorem
   - Mathematical foundations
   - Weierstrass theorem connection
   - Width vs approximation quality
   - Practical implications

5. **`05-practical-aspects.ipynb`** - Practical Considerations
   - Overfitting and validation
   - Hyperparameter selection
   - Depth vs width (XOR problem)
   - High-frequency functions

## Reference Notebooks

- **`polynomial.ipynb`** - Polynomial approximation baseline
- **`uat-theory.ipynb`** - Theoretical foundations (optional deep dive)
- **`mlp-exercise.ipynb`** - Exercises and problems

## Original Comprehensive Notebook

- **`mlp-full.ipynb`** - Complete original notebook (106 cells)
  - Contains all material in one place
  - Use as reference or for complete workshop

## Getting Started

Start with notebooks 1-3 for the core concepts, then explore 4-5 for deeper understanding.

Each notebook is self-contained and uses the `learnsciml` library for cleaner code.