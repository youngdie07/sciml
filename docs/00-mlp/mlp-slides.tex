%**************************************************************************************
% License:
% CC BY-NC-SA 4.0 (http://creativecommons.org/licenses/by-nc-sa/4.0/)
%**************************************************************************************

\documentclass[notes]{beamer}

\mode<presentation> {

\usetheme{Madrid}

% Burnt orange
\definecolor{burntorange}{rgb}{0.8, 0.33, 0.0}
\colorlet{beamer@blendedblue}{burntorange}
% Pale yellow
\definecolor{paleyellow}{rgb}{1.0, 1.0, 0.953}
\setbeamercolor{background canvas}{bg=paleyellow}
% Secondary and tertiary palette
\setbeamercolor*{palette secondary}{use=structure,fg=white,bg=burntorange!80!black}
\setbeamercolor*{palette tertiary}{use=structure,fg=white,bg=burntorange!60!black}

% To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}
% To replace the footer line in all slides with a simple slide count uncomment this line
%\setbeamertemplate{footline}[page number]

% To remove the navigation symbols from the bottom of all slides uncomment this line
%\setbeamertemplate{navigation symbols}{}
}

\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{bm}
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{breqn}
\usepackage{cleveref}
\usepackage{graphicx} % for figures
\usepackage[labelsep=space,tableposition=top]{caption}
\renewcommand{\figurename}{Fig.} 
\usepackage{caption,subcaption}% http://ctan.org/pkg/{caption,subcaption}
\usepackage{xcolor}

% To print 2 slides on a page
%\usepackage{handoutWithNotes}
%\pgfpagesuselayout{2 on 1}[border shrink=2mm]

\AtBeginSection[]{
	\begin{frame}{Outline}
		\tableofcontents[
		currentsection,      % highlight the current section
		hideallsubsections   % show only section titles
		]
	\end{frame}
}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------
% The short title appears at the bottom of every slide, the full title is only on the title page
\title[Neural Networks \& Function Approximation]{Neural Networks and Function Approximation} 
\author{Krishna Kumar} % name
\institute[UT Austin] % institution 
{
University of Texas at Austin \\
\medskip
\textit{
  \url{krishnak@utexas.edu}} % Your email address
}
\date{} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % title page as the first slide
\end{frame}

\begin{frame}
 % Table of contents slide, comment this block out to remove it
 \frametitle{Overview}
 % Throughout your presentation, if you choose to use \section{} and \subsection{} 
 % commands, these %will automatically be printed on this slide as an overview 
 \tableofcontents
\end{frame}

%----------------------------------------------------------------------------------------
% slides
%----------------------------------------------------------------------------------------

\section{Introduction and Motivation}

%------------------------------------------------
\begin{frame}
\frametitle{The 1D Poisson Equation: Our Benchmark Problem}

Consider the one-dimensional Poisson equation on $[0, 1]$:

\mode<beamer>{
\begin{equation*}
-\frac{d^2u}{dx^2} = f(x), \quad x \in [0, 1]
\end{equation*}
with boundary conditions: $u(0) = 0, \quad u(1) = 0$
}
\mode<handout>{
\vspace{2cm}
}

\textbf{Chosen source term:} $f(x) = \pi^2 \sin(\pi x)$

\textbf{Analytical solution:} $u(x) = \sin(\pi x)$

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6\textwidth]{figs/poisson-analytical-solution.png}
	\caption*{Analytical solution to 1D Poisson equation}
\end{figure}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Traditional Methods: Finite Difference}

\textbf{Finite difference approach:}
\begin{itemize}
\item Discretize domain: $x_i = i\Delta x$, $i = 0, 1, ..., N$
\item Approximate derivatives: $u''(x_i) \approx \frac{u_{i+1} - 2u_i + u_{i-1}}{\Delta x^2}$
\item Solve linear system: $Au = f$
\end{itemize}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/finite-difference-methods.png}
	\caption*{Finite difference solution for different grid sizes}
\end{figure}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Neural Network Approach}

\textbf{Different paradigm:} Learn a continuous function $u_{NN}(x; \theta)$ that approximates $u(x)$.

\begin{itemize}
\item Network is a parameterized function approximator
\item Train on sample points from the domain
\item Evaluate anywhere (not just at grid points)
\item Leverage automatic differentiation for derivatives
\end{itemize}

\textbf{Key question:} Can neural networks approximate arbitrary continuous functions?

\end{frame}

%----------------------------------------------------------------------------------------
\section{Neural Network Fundamentals}

%------------------------------------------------
\begin{frame}
\frametitle{The Perceptron: From Linear to Nonlinear}

\textbf{Step 1: Linear Perceptron}
\mode<beamer>{
\begin{equation*}
\hat{y} = \mathbf{w}^T\mathbf{x} + b = \sum_{i=1}^n w_i x_i + b
\end{equation*}

This is just a linear transformation - can only model linear relationships.
}
\mode<handout>{
\vspace{2cm}
}

\textbf{Step 2: Add Activation Function}
\mode<beamer>{
\begin{align*}
z &= \mathbf{w}^T\mathbf{x} + b \quad \text{(pre-activation)} \\
\hat{y} &= g(z) \quad \text{(output)}
\end{align*}

The activation function $g$ introduces nonlinearity.
}
\mode<handout>{
\vspace{2cm}
}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6\textwidth]{figs/perceptron.png}
	\caption*{Perceptron: linear transformation + activation}
\end{figure}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Why We Need Nonlinearity}

\textbf{Linear Limitation:} A linear perceptron creates a hyperplane decision boundary.

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Linearly Separable}
\begin{itemize}
\item Single line can separate classes
\item Linear perceptron works
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Not Linearly Separable}
\begin{itemize}
\item No single line works
\item Need nonlinear boundaries
\end{itemize}
\end{column}
\end{columns}

\vspace{0.5cm}

\textbf{Example: Circular Pattern}

Points inside circle (class 0) vs outside (class 1) - impossible for linear boundary.

\textbf{Solution:} Activation functions enable learning curved decision boundaries.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Common Activation Functions}

\begin{columns}[T]
\begin{column}{0.5\textwidth}
\textbf{Sigmoid:} $\sigma(z) = \frac{1}{1 + e^{-z}}$
\begin{itemize}
\item Output in $(0,1)$
\item Smooth gradient
\item Can saturate
\end{itemize}

\textbf{ReLU:} $\text{ReLU}(z) = \max(0, z)$
\begin{itemize}
\item Simple and efficient
\item No saturation for $z > 0$
\item Dead neurons for $z < 0$
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Tanh:} $\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$
\begin{itemize}
\item Output in $(-1,1)$
\item Zero-centered
\item Can saturate
\end{itemize}

\textbf{Leaky ReLU:} $\max(\alpha z, z)$
\begin{itemize}
\item Prevents dead neurons
\item Small slope for $z < 0$
\end{itemize}
\end{column}
\end{columns}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/activation-functions.png}
\end{figure}

\href{https://kks32-courses.github.io/sciml/00-mlp/relu/}{\beamergotobutton{ReLU Demo}}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Training: Gradient Descent}

\textbf{Objective:} Minimize loss function $L = \frac{1}{2N}\sum_{i=1}^N (y_i - \hat{y}_i)^2$

\textbf{Gradient Computation (Single Sample):}

For prediction $\hat{y} = \mathbf{w}^T\mathbf{x} + b$ and loss $L = \frac{1}{2}(y - \hat{y})^2$:

\mode<beamer>{
\begin{align*}
\frac{\partial L}{\partial \mathbf{w}} &= \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial \mathbf{w}} = -(y - \hat{y}) \cdot \mathbf{x} \\
\frac{\partial L}{\partial b} &= \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial b} = -(y - \hat{y})
\end{align*}
}
\mode<handout>{
\vspace{2cm}
}

\textbf{Update Rule:}
\mode<beamer>{
\begin{align*}
\mathbf{w} &\leftarrow \mathbf{w} - \eta \frac{\partial L}{\partial \mathbf{w}} = \mathbf{w} + \eta(y - \hat{y})\mathbf{x} \\
b &\leftarrow b - \eta \frac{\partial L}{\partial b} = b + \eta(y - \hat{y})
\end{align*}

where $\eta$ is the learning rate.
}
\mode<handout>{
\vspace{2cm}
}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Backpropagation with Activation Functions}

With activation $g$, the forward pass: $z = \mathbf{w}^T\mathbf{x} + b$, $\hat{y} = g(z)$

\textbf{Chain Rule for Gradients:}

\mode<beamer>{
\begin{enumerate}
\item \textbf{Output error:} $\delta = \frac{\partial L}{\partial \hat{y}} = -(y - \hat{y})$
\item \textbf{Pre-activation gradient:} $\frac{\partial L}{\partial z} = \delta \cdot g'(z)$
\item \textbf{Weight gradient:} $\frac{\partial L}{\partial \mathbf{w}} = \frac{\partial L}{\partial z} \cdot \mathbf{x} = \delta \cdot g'(z) \cdot \mathbf{x}$
\item \textbf{Bias gradient:} $\frac{\partial L}{\partial b} = \delta \cdot g'(z)$
\end{enumerate}
}
\mode<handout>{
\vspace{3cm}
}

\textbf{Implementation:}
%\begin{verbatim}
%def backward(self, y_true):
%    error = self.a - y_true           # δ
%    dz = error * g_prime(self.z)      # δ * g'(z)
%    self.w -= lr * dz * self.x        # update weights
%    self.b -= lr * dz                 # update bias
%\end{verbatim}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The XOR Problem: Motivation for Deep Networks}

\textbf{XOR Truth Table:}
\begin{center}
\begin{tabular}{cc|c}
$x_1$ & $x_2$ & XOR \\
\hline
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0 \\
\end{tabular}
\end{center}

\textbf{The Problem:} No single line can separate the classes.

\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figs/xor-problem.png}
	\caption*{XOR data points}
\end{figure}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Key Insight:}
\begin{itemize}
\item Single perceptron fails (even with nonlinearity)
\item Need multiple layers
\item Hidden layer learns intermediate features
\item Output layer combines features
\end{itemize}
\end{column}
\end{columns}

\textbf{Historical Impact:} Led to first AI winter (Minsky \& Papert, 1969)

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Multi-Layer Networks: Solution to XOR}

\textbf{Two-layer network can solve XOR:}

\begin{columns}[T]
\begin{column}{0.5\textwidth}
\textbf{Hidden Layer:}
\begin{itemize}
\item Neuron 1: Detects $(x_1=1, x_2=0)$
\item Neuron 2: Detects $(x_1=0, x_2=1)$
\end{itemize}

\textbf{Output Layer:}
\begin{itemize}
\item Combines hidden features
\item OR operation on detectors
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{figs/xor-decision-boundaries.png}
	\caption*{How hidden layer transforms XOR}
\end{figure}
\end{column}
\end{columns}

\textbf{Key Concept:} Each layer transforms data into new representation where it becomes more linearly separable.

\end{frame}

%------------------------------------------------
\section{Universal Approximation Theorem}

%------------------------------------------------
\begin{frame}
\frametitle{Universal Approximation Theorem}

\textbf{Theorem (Cybenko, 1989):} A single hidden layer network with sufficient neurons can approximate any continuous function to arbitrary accuracy.

\mode<beamer>{
\begin{equation*}
F(x) = \sum_{i=1}^{N} w_i \sigma(v_i x + b_i) + w_0
\end{equation*}

\textbf{Formal Statement:} For any continuous $f: [0,1] \to \mathbb{R}$ and $\epsilon > 0$, there exists $N$ and parameters such that:
$$|F(x) - f(x)| < \epsilon \quad \forall x \in [0,1]$$
}
\mode<handout>{
\vspace{3cm}
}

\textbf{Important Caveats:}
\begin{itemize}
\item Theorem guarantees existence, not how to find parameters
\item May need exponentially many neurons
\item Deeper networks often more efficient in practice
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Single Hidden Layer Architecture}

For 1D input $x$, network with $N_h$ hidden neurons:

\begin{columns}[T]
\begin{column}{0.5\textwidth}
\mode<beamer>{
\textbf{Forward Pass:}
\begin{align*}
\mathbf{z}^{(1)} &= W^{(1)}x + \mathbf{b}^{(1)} \\
\mathbf{h} &= g(\mathbf{z}^{(1)}) \\
z^{(2)} &= W^{(2)}\mathbf{h} + b^{(2)} \\
\hat{y} &= z^{(2)}
\end{align*}

where:
\begin{itemize}
\item $W^{(1)} \in \mathbb{R}^{N_h \times 1}$
\item $W^{(2)} \in \mathbb{R}^{1 \times N_h}$
\end{itemize}
}
\mode<handout>{
\vspace{4cm}
}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{figs/single-layer-nn2.png}
	\caption*{Single hidden layer network}
\end{figure}
\end{column}
\end{columns}

\end{frame}

%------------------------------------------------
\section{Training Neural Networks}

%------------------------------------------------
\begin{frame}
\frametitle{The Training Process}

\textbf{Objective:} Find parameters $\theta^* = \argmin_{\theta} \mathcal{L}(\theta)$

\textbf{Loss Function (MSE):}
\mode<beamer>{
$$\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^N \left(u_{NN}(x_i; \theta) - u_i\right)^2$$
}
\mode<handout>{
\vspace{1.5cm}
}

\textbf{Training Algorithm:}
\begin{enumerate}
\item \textbf{Initialize:} Random weights (small values)
\item \textbf{Forward Pass:} Compute predictions $\hat{y}_i$
\item \textbf{Compute Loss:} Evaluate $\mathcal{L}(\theta)$
\item \textbf{Backward Pass:} Compute gradients via backpropagation
\item \textbf{Update:} $\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}$
\item \textbf{Repeat:} Until convergence
\end{enumerate}

\end{frame}

%------------------------------------------------
% INSERT THESE 3 SLIDES INTO MLP PRESENTATION
% After "Training Process" slide, before "Gradient Descent Algorithm"
%------------------------------------------------

\begin{frame}
	\frametitle{The Gradient Problem}
	
	\textbf{Training neural networks requires gradients:} $\frac{\partial \mathcal{L}}{\partial \theta}$ for all parameters $\theta$.
	
	\begin{minipage}[t]{0.48\textwidth}
		\textbf{Traditional approaches have fundamental flaws:}
		\begin{itemize}
			\item \textbf{Manual:} Exact, but error-prone and doesn't scale
			\item \textbf{Symbolic:} Exact, but "expression swell"
			\item \textbf{Numerical:} $f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}$
			\begin{itemize}
				\item Inaccurate (rounding errors)
				\item Expensive (multiple evaluations)
			\end{itemize}
		\end{itemize}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\textwidth}
		\begin{figure}[ht]
			\centering
			\includegraphics[width=0.9\textwidth]{figs/differentiation.png}
		\end{figure}
	\end{minipage}
	
	\mode<beamer>{
		For a neural network with thousands of parameters:
		\begin{equation*}
			\theta = \{W^{(1)}, \mathbf{b}^{(1)}, W^{(2)}, \mathbf{b}^{(2)}, \ldots\}
		\end{equation*}
	}
	\mode<handout>{
		\vspace{1cm}
	}
	
	\textbf{We need a fourth approach: Automatic Differentiation!}
	
\end{frame}

%------------------------------------------------
\begin{frame}
	\frametitle{Automatic Differentiation: The Solution}
	
	Every function is a computational graph of elementary operations.
	
	\begin{columns}[T]
		\begin{column}{0.35\textwidth}
			Consider: $y = x_1^2 + x_2$
			
			\vspace{1em}
			
			\textbf{Evaluation Trace:}
			\begin{enumerate}
				\item $v_1 = x_1^2$
				\item $y = v_1 + x_2$
			\end{enumerate}
			
		\end{column}
		\begin{column}{0.65\textwidth}
			\begin{figure}[ht]
				\centering
				\includegraphics[width=\linewidth]{figs/ad3.png}
			\end{figure}
		\end{column}
	\end{columns}
	\vspace{1em}
	This decomposition is the key that makes AD possible. By applying the chain rule to each elementary step, we can compute exact derivatives.
\end{frame}

%----------------------------------------------------------------------------------------

\begin{frame}
	\frametitle{Forward Mode Automatic Differentiation}
	
	Forward mode AD propagates derivative information \textbf{forward} through the graph, alongside the function evaluation.
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=\textwidth]{figs/forward-mode-ad.png}
	\end{figure}
	\centering
	\href{https://kks32-courses.github.io/sciml/00-mlp/ad-graph/}{\beamergotobutton{AD Demo}}
\end{frame}

%------------------------------------------------
\begin{frame}
	\frametitle{Reverse Mode Automatic Differentiation (Backpropagation)}
	
	\textbf{Key Insight:} For scalar loss functions, reverse mode computes all gradients in one pass.
	
	\textbf{Algorithm:}
	\begin{enumerate}
		\item \textbf{Forward Pass:} Evaluate function, store intermediate values
		\item \textbf{Backward Pass:} Propagate gradients backward using chain rule
	\end{enumerate}
	
	\textbf{Example for Perceptron:}
	\mode<beamer>{
	\begin{align*}
	\text{Forward:} \quad & z = \mathbf{w}^T\mathbf{x} + b, \quad a = g(z), \quad L = \frac{1}{2}(y - a)^2 \\
	\text{Backward:} \quad & \frac{\partial L}{\partial a} = -(y - a) \\
	& \frac{\partial L}{\partial z} = \frac{\partial L}{\partial a} \cdot g'(z) \\
	& \frac{\partial L}{\partial \mathbf{w}} = \frac{\partial L}{\partial z} \cdot \mathbf{x} \\
	& \frac{\partial L}{\partial b} = \frac{\partial L}{\partial z}
	\end{align*}
	}
	\mode<handout>{
	\vspace{3cm}
	}
	
\end{frame}

%------------------------------------------------
\begin{frame}
	\frametitle{When to Use Forward vs. Reverse Mode}
	
	The choice depends on the dimensions of your function $f: \mathbb{R}^n \to \mathbb{R}^m$.
	
	\begin{columns}[T]
		\begin{column}{0.48\textwidth}
			\begin{block}{Forward Mode}
				\begin{itemize}
					\item Cost: One pass per input
					\item Efficient when: $n \ll m$
					\item Few inputs, many outputs
				\end{itemize}
			\end{block}
		\end{column}
		\hfill
		\begin{column}{0.48\textwidth}
			\begin{block}{Reverse Mode}
				\begin{itemize}
					\item Cost: One pass per output
					\item Efficient when: $n \gg m$
					\item Many inputs, few outputs
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
	
	\vspace{0.5cm}
	
	\textbf{Neural Networks:} Millions of parameters (inputs), single scalar loss (output)
	
	\begin{center}
		\Large{\textbf{Reverse mode (backpropagation) wins!}}
	\end{center}
	
	\centering
	\href{https://kks32-courses.github.io/sciml/00-mlp/ad}{\beamergotobutton{AD Notebook}}
	
\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Gradient Descent Variants}

\begin{columns}[T]
\begin{column}{0.5\textwidth}
\textbf{Basic SGD:}
$$\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}$$

\textbf{SGD with Momentum:}
\begin{align*}
v_{t+1} &= \beta v_t + \nabla_\theta \mathcal{L} \\
\theta_{t+1} &= \theta_t - \eta v_{t+1}
\end{align*}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Adam (Adaptive Moments):}
\begin{itemize}
\item Adapts learning rate per parameter
\item Combines momentum + RMSprop
\item De facto standard in deep learning
\end{itemize}
\end{column}
\end{columns}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6\textwidth]{figs/sgd.png}
	\caption*{Gradient descent trajectory}
\end{figure}

\href{https://kks32-courses.github.io/sciml/00-mlp/sgd}{\beamergotobutton{SGD Demo}}

\end{frame}

%------------------------------------------------
\section{Experimental Validation}

%------------------------------------------------
\begin{frame}
\frametitle{Width vs. Approximation Quality}

\textbf{Experiment:} How does network width affect approximation quality?

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figs/width-quality.png}
	\caption*{Approximation improves with more hidden neurons}
\end{figure}

\textbf{Observations:}
\begin{itemize}
\item More neurons $\rightarrow$ better approximation
\item Diminishing returns after certain width
\item Confirms Universal Approximation Theorem
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Training Dynamics}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figs/training-convergence.png}
	\caption*{Loss decreases during training}
\end{figure}

\textbf{Key Observations:}
\begin{itemize}
\item Exponential decay in loss
\item Larger networks converge to lower loss
\item Training can be unstable (oscillations)
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Challenges and Solutions}

%------------------------------------------------
\begin{frame}
\frametitle{Common Training Challenges}

\begin{columns}[T]
\begin{column}{0.5\textwidth}
\textbf{1. Vanishing/Exploding Gradients}
\begin{itemize}
\item Deep networks compound gradients
\item Solution: Careful initialization, normalization
\end{itemize}

\textbf{2. Local Minima}
\begin{itemize}
\item Non-convex optimization
\item Solution: Multiple runs, momentum
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{3. Overfitting}
\begin{itemize}
\item Network memorizes training data
\item Solution: Regularization, dropout
\end{itemize}

\textbf{4. Computational Cost}
\begin{itemize}
\item Many parameters to optimize
\item Solution: GPUs, efficient algorithms
\end{itemize}
\end{column}
\end{columns}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/overfitting.png}
	\caption*{Overfitting: perfect on training, poor on test}
\end{figure}

\end{frame}

%------------------------------------------------
\section{Connection to Scientific Computing}

%------------------------------------------------
\begin{frame}
\frametitle{Neural Networks and PDEs}

\textbf{Analogy with Numerical Methods:}

\begin{center}
\begin{tabular}{l|l}
\textbf{Neural Networks} & \textbf{Numerical PDEs} \\
\hline
Weights $\mathbf{w}$ & Solution coefficients \\
Loss function $L$ & Residual $\|\mathcal{L}[u] - f\|$ \\
Gradient descent & Iterative solver \\
Learning rate $\eta$ & Time step/relaxation parameter \\
Activation functions & Basis functions \\
\end{tabular}
\end{center}

\textbf{Physics-Informed Neural Networks (PINNs):}

Minimize combined loss:
$$L(\theta) = \underbrace{\|\mathcal{L}[u_{NN}] - f\|^2_{\Omega}}_{\text{PDE loss}} + \lambda \underbrace{\|u_{NN} - g\|^2_{\partial\Omega}}_{\text{Boundary loss}}$$

Network learns to satisfy both PDE and boundary conditions.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Summary}

\textbf{Key Concepts:}
\begin{itemize}
\item \textbf{Perceptron:} Linear transformation + activation
\item \textbf{Nonlinearity:} Essential for complex functions
\item \textbf{Training:} Gradient descent with backpropagation
\item \textbf{Universal Approximation:} Single layer can approximate any continuous function
\item \textbf{Automatic Differentiation:} Enables efficient gradient computation
\end{itemize}

\textbf{Practical Insights:}
\begin{itemize}
\item Start with linear model, add complexity as needed
\item XOR problem motivates deep networks
\item More neurons improve approximation (with diminishing returns)
\item Connection to traditional numerical methods
\end{itemize}

\textbf{Next Steps:} Deep networks, advanced architectures, PINNs

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Resources}

\textbf{Interactive Demos:}
\begin{itemize}
\item ReLU Activation: \href{https://kks32-courses.github.io/sciml/00-mlp/relu/}{\beamergotobutton{Demo}}
\item SGD Visualization: \href{https://kks32-courses.github.io/sciml/00-mlp/sgd}{\beamergotobutton{Demo}}
\item AD Graph: \href{https://kks32-courses.github.io/sciml/00-mlp/ad-graph/}{\beamergotobutton{Demo}}
\end{itemize}

\textbf{Jupyter Notebooks:}
\begin{itemize}
\item Complete MLP implementation: \href{https://github.com/kks32-courses/sciml}{\beamergotobutton{GitHub}}
\item Automatic Differentiation: \href{https://kks32-courses.github.io/sciml/00-mlp/ad}{\beamergotobutton{Notebook}}
\end{itemize}

\textbf{References:}
\begin{itemize}
\item Cybenko, G. (1989). "Approximation by superpositions of a sigmoidal function"
\item Hornik, K. (1991). "Approximation capabilities of multilayer feedforward networks"
\item Goodfellow et al. (2016). "Deep Learning" (MIT Press)
\end{itemize}

\end{frame}

\end{document}