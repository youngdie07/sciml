%**************************************************************************************
% License:
% CC BY-NC-SA 4.0 (http://creativecommons.org/licenses/by-nc-sa/4.0/)
%**************************************************************************************

\documentclass[notes]{beamer}

\mode<presentation> {

\usetheme{Madrid}

% Burnt orange
\definecolor{burntorange}{rgb}{0.8, 0.33, 0.0}
\colorlet{beamer@blendedblue}{burntorange}
% Pale yellow
\definecolor{paleyellow}{rgb}{1.0, 1.0, 0.953}
\setbeamercolor{background canvas}{bg=paleyellow}
% Secondary and tertiary palette
\setbeamercolor*{palette secondary}{use=structure,fg=white,bg=burntorange!80!black}
\setbeamercolor*{palette tertiary}{use=structure,fg=white,bg=burntorange!60!black}

% To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}
% To replace the footer line in all slides with a simple slide count uncomment this line
%\setbeamertemplate{footline}[page number]

% To remove the navigation symbols from the bottom of all slides uncomment this line
%\setbeamertemplate{navigation symbols}{}
}

\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{bm}
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{breqn}
\usepackage{cleveref}
\usepackage{graphicx} % for figures
\usepackage[labelsep=space,tableposition=top]{caption}
\renewcommand{\figurename}{Fig.} 
\usepackage{caption,subcaption}% http://ctan.org/pkg/{caption,subcaption}
\usepackage{xcolor}

% To print 2 slides on a page
%\usepackage{handoutWithNotes}
%\pgfpagesuselayout{2 on 1}[border shrink=2mm]

\AtBeginSection[]{
	\begin{frame}{Outline}
		\tableofcontents[
		currentsection,      % highlight the current section
		hideallsubsections   % show only section titles
		]
	\end{frame}
}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------
% The short title appears at the bottom of every slide, the full title is only on the title page
\title[Neural Networks \& Function Approximation]{Neural Networks and Function Approximation} 
\author{Krishna Kumar} % name
\institute[UT Austin] % institution 
{
University of Texas at Austin \\
\medskip
\textit{
  \url{krishnak@utexas.edu}} % Your email address
}
\date{} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % title page as the first slide
\end{frame}

\begin{frame}
 % Table of contents slide, comment this block out to remove it
 \frametitle{Overview}
 % Throughout your presentation, if you choose to use \section{} and \subsection{} 
 % commands, these %will automatically be printed on this slide as an overview 
 \tableofcontents
\end{frame}

%----------------------------------------------------------------------------------------
% slides
%----------------------------------------------------------------------------------------

\section{Introduction and Motivation}

%------------------------------------------------
\begin{frame}
\frametitle{The 1D Poisson Equation: Our Benchmark Problem}

Consider the one-dimensional Poisson equation on $[0, 1]$:

\mode<beamer>{
\begin{equation*}
-\frac{d^2u}{dx^2} = f(x), \quad x \in [0, 1]
\end{equation*}

with boundary conditions:
\begin{equation*}
u(0) = 0, \quad u(1) = 0
\end{equation*}

For $f(x) = \pi^2 \sin(\pi x)$, the analytical solution is:
\begin{equation*}
u(x) = \sin(\pi x)
\end{equation*}
}
\mode<handout>{
\vspace{3cm}
}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/poisson-analytical-solution.png}
	\caption*{Analytical solution and source function}
\end{figure}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Function Approximation Challenge}

\textbf{Key Question:} Can we learn to approximate $u(x) = \sin(\pi x)$ from sparse data?

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figs/sparse-data-challenge.png}
	\caption*{Learning from 15 noisy training points}
\end{figure}

\textbf{Challenge:} Reconstruct a smooth, continuous function from limited observations.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Traditional vs Neural Network Approaches}

\begin{minipage}[t]{0.53\textwidth}
\textbf{Finite Difference:}
\begin{itemize}
\item Discretize domain into grid points
\item Solve for values at specific locations
\item Result: Discrete representation
\end{itemize}


\textbf{Neural Network Approach:}
\begin{itemize}
	\item Learn continuous function $u_{NN}(x; \theta)$
	\item Approximate solution over entire domain
	\item Parameters $\theta$ trained from sparse data
\end{itemize}


\end{minipage}
\hfill
\begin{minipage}[t]{0.43\textwidth}
\vspace{1em}
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figs/finite-difference-methods.png}
	\caption*{Finite Difference Approximation}
\end{figure}

\mode<beamer>{
	\begin{equation*}
		\frac{d^2u}{dx^2} \approx \frac{u_{i+1} - 2u_i + u_{i-1}}{h^2}
	\end{equation*}
}
\mode<handout>{
	\vspace{1.5cm}
}

\end{minipage}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Finite Difference vs Neural Networks}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figs/fd-poisson.png}
	\caption*{Comparison of solution methods and discretization errors}
\end{figure}

\textbf{Key Insight:} Neural networks learn continuous functions, not just discrete values.

\end{frame}

%------------------------------------------------
\section{Neural Network Fundamentals}

%------------------------------------------------
\begin{frame}
\frametitle{The Perceptron: Basic Building Block}

A perceptron computes:
\mode<beamer>{
\begin{align*}
z &= \mathbf{w}^T\mathbf{x} + b = \sum_{i=1}^n w_i x_i + b \\
\hat{y} &= g(z)
\end{align*}

where $g$ is the activation function.
}
\mode<handout>{
\vspace{2cm}
}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/perceptron.png}
	\caption*{Perceptron architecture}
\end{figure}

\textbf{Key components:} Input vector, weights, bias, activation function, output.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{The Critical Role of Nonlinearity}

\textbf{Without activation functions:} Multiple linear layers collapse to single linear transformation.

\mode<beamer>{
Consider two linear layers:
\begin{align*}
h_1 &= W_1 x + b_1 \\
h_2 &= W_2 h_1 + b_2 = W_2(W_1 x + b_1) + b_2 \\
&= (W_2 W_1) x + (W_2 b_1 + b_2)
\end{align*}

This is equivalent to: $h_2 = W_{eq} x + b_{eq}$ (still linear!)
}
\mode<handout>{
\vspace{3cm}
}

\textbf{Problem:} Linear networks can only learn $y = mx + b$, but $\sin(\pi x)$ is curved!

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Demonstrating Linear Network Failure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figs/linear-network-failure.png}
	\caption*{Linear network cannot approximate $\sin(\pi x)$}
\end{figure}

\textbf{Conclusion:} Nonlinearity is essential for complex function approximation.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Common Activation Functions}

\textbf{Sigmoid:} $\sigma(x) = \frac{1}{1 + e^{-x}}$ (squashes to (0,1))

\textbf{Tanh:} $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ (squashes to (-1,1))

\textbf{ReLU:} $f(x) = \max(0, x)$ (efficient, prevents vanishing gradients)

\textbf{Leaky ReLU:} $f(x) = \max(\alpha x, x)$ (prevents dead neurons)

\href{https://kks32-courses.github.io/sciml/00-mlp/relu/}{\beamergotobutton{ReLU Demo}}
\end{frame}
%------------------------------------------------

\begin{frame}
	\frametitle{Common Activation Functions}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/activation-functions.png}
	\caption*{Common activation functions}
\end{figure}

\end{frame}

%------------------------------------------------
\section{Universal Approximation Theorem}

%------------------------------------------------
\begin{frame}
\frametitle{Universal Approximation Theorem}

\textbf{Theorem (Cybenko, 1989):} A single hidden layer network with sufficient neurons can approximate any continuous function to arbitrary accuracy.

\mode<beamer>{
\begin{equation*}
F(x) = \sum_{i=1}^{N} w_i \sigma(v_i x + b_i) + w_0
\end{equation*}

\textbf{Mathematical statement:} For any continuous $f: [0,1] \to \mathbb{R}$ and $\epsilon > 0$, there exists $N$ and parameters such that $|F(x) - f(x)| < \epsilon$ for all $x \in [0,1]$.
}
\mode<handout>{
\vspace{3cm}
}

\textbf{Key Questions:}
\begin{itemize}
\item How many neurons $N$ do we need?
\item Is this practical?
\item Can we verify experimentally?
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Single Hidden Layer Architecture}

For 1D input $x$, a single-layer network with $N_h$ hidden neurons:
 \begin{minipage}[t]{0.48\textwidth}
\mode<beamer>{
\begin{align*}
\mathbf{z}^{(1)} &= W^{(1)}x + \mathbf{b}^{(1)} \quad \text{(pre-activation)} \\
\mathbf{h} &= g(\mathbf{z}^{(1)}) \quad \text{(hidden layer output)} \\
z^{(2)} &= W^{(2)}\mathbf{h} + b^{(2)} \quad \text{(output layer)} \\
\hat{y} &= z^{(2)} \quad \text{(final prediction)}
\end{align*}
}
\mode<handout>{
\vspace{3cm}
}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/single-layer-nn2.png}
	\caption*{Single hidden layer neural network}
\end{figure}
\end{minipage}

\end{frame}

%------------------------------------------------
\section{Training Neural Networks}

%------------------------------------------------
\begin{frame}
\frametitle{Training Process}

\textbf{Goal:} Find optimal parameters $\theta^*$ that minimize loss function.

\mode<beamer>{
Loss function (MSE):
\begin{equation*}
\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^N \left(u_{NN}(x_i; \theta) - u_i\right)^2
\end{equation*}

Optimization problem:
\begin{equation*}
\theta^* = \argmin_{\theta} \mathcal{L}(\theta)
\end{equation*}
}
\mode<handout>{
\vspace{2.5cm}
}

\textbf{Training Steps:}
\begin{enumerate}
\item Forward pass: compute predictions
\item Calculate loss
\item Backward pass: compute gradients
\item Update parameters
\item Repeat until convergence
\end{enumerate}

\end{frame}


%------------------------------------------------
% INSERT THESE 3 SLIDES INTO MLP PRESENTATION
% After "Training Process" slide, before "Gradient Descent Algorithm"
%------------------------------------------------

\begin{frame}
	\frametitle{The Gradient Problem}
	
	\textbf{Training neural networks requires gradients:} $\frac{\partial \mathcal{L}}{\partial \theta}$ for all parameters $\theta$.
	
	\begin{minipage}[t]{0.48\textwidth}
		\textbf{Traditional approaches have fundamental flaws:}
		\begin{itemize}
			\item \textbf{Manual:} Exact, but error-prone and doesn't scale
			\item \textbf{Symbolic:} Exact, but "expression swell"
			\item \textbf{Numerical:} $f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}$
			\begin{itemize}
				\item Inaccurate (rounding errors)
				\item Expensive (multiple evaluations)
			\end{itemize}
		\end{itemize}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\textwidth}
		\begin{figure}[ht]
			\centering
			\includegraphics[width=0.9\textwidth]{figs/differentiation.png}
		\end{figure}
	\end{minipage}
	
	\mode<beamer>{
		For a neural network with thousands of parameters:
		\begin{equation*}
			\theta = \{W^{(1)}, \mathbf{b}^{(1)}, W^{(2)}, \mathbf{b}^{(2)}, \ldots\}
		\end{equation*}
	}
	\mode<handout>{
		\vspace{1cm}
	}
	
	\textbf{We need a fourth approach: Automatic Differentiation!}
	
\end{frame}

%------------------------------------------------
\begin{frame}
	\frametitle{Automatic Differentiation: The Solution}
	
	Every function is a computational graph of elementary operations.
	
	\begin{columns}[T]
		\begin{column}{0.35\textwidth}
			Consider: $y = x_1^2 + x_2$

			\vspace{1em}

			\textbf{Evaluation Trace:}
			\begin{enumerate}
				\item $v_1 = x_1^2$
				\item $y = v_1 + x_2$
			\end{enumerate}
			
		\end{column}
		\begin{column}{0.65\textwidth}
			\begin{figure}[ht]
				\centering
				\includegraphics[width=\linewidth]{figs/ad3.png}
			\end{figure}
		\end{column}
	\end{columns}
	\vspace{1em}
	This decomposition is the key that makes AD possible. By applying the chain rule to each elementary step, we can compute exact derivatives.
\end{frame}

%----------------------------------------------------------------------------------------

\begin{frame}
	\frametitle{Forward Mode Automatic Differentiation}
	
	Forward mode AD propagates derivative information \textbf{forward} through the graph, alongside the function evaluation.
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=\textwidth]{figs/forward-mode-ad.png}
	\end{figure}
	\centering
	\href{https://kks32-courses.github.io/sciml/00-mlp/ad-graph/}{\beamergotobutton{AD Demo}}
\end{frame}

%------------------------------------------------
\begin{frame}
	\frametitle{Forward Mode: Step-by-Step Example}
	
	Let's compute $\frac{\partial y}{\partial x_1}$ for $y = x_1^2 + x_2$.
	
	\begin{block}{1. Seed the input w.r.t. $x_1$}
		Set $\dot{x}_1 = 1$ and $\dot{x}_2 = 0$.
	\end{block}
	
	\begin{block}{2. Forward Propagation}
		\begin{itemize}
			\item $v_1 = x_1^2 \implies \dot{v}_1 = 2x_1 \cdot \dot{x}_1 = 2x_1 \cdot 1 = 2x_1$
			\item $y = v_1 + x_2 \implies \dot{y} = \dot{v}_1 + \dot{x}_2 = 2x_1 + 0 = 2x_1$
		\end{itemize}
	\end{block}
	
	\begin{alertblock}{Result}
		The final propagated tangent $\dot{y}$ is the derivative: $\frac{\partial y}{\partial x_1} = 2x_1$.
	\end{alertblock}
	
	\textbf{Key Idea:} To get the derivative w.r.t. $x_2$, we would need a \textit{new pass} with seeds $\dot{x}_1 = 0, \dot{x}_2 = 1$.
	
\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}
	\frametitle{Reverse Mode Automatic Differentiation}
	
	Reverse mode AD (or \textbf{backpropagation}) computes derivatives by propagating information \textbf{backward} from the output.
	
	\textbf{Algorithm:}
	\begin{enumerate}
		\item \textbf{Forward Pass:} Evaluate the function and store all intermediate values.
		\item \textbf{Backward Pass:} Starting from the output, use the chain rule to propagate "adjoints" ($\bar{v} = \frac{\partial y}{\partial v}$) backward through the graph.
	\end{enumerate}
	
\end{frame}

%------------------------------------------------
\begin{frame}
	\frametitle{Reverse Mode: One Pass for All Gradients}
	The backward pass systematically applies the chain rule.
	
	Let's trace the computation for $y = x_1^2 + x_2$:
	\begin{figure}[ht]
		\centering
		\includegraphics[width=\textwidth]{figs/ad7.png}
	\end{figure}
	Reverse mode computes \textbf{all} partial derivatives in a single backward pass.
	
\end{frame}

%-------------------------------------------------------------------------------

\begin{frame}
	\frametitle{When to Use Forward vs. Reverse Mode}
	
	The choice depends on the dimensions of your function $f: \mathbb{R}^n \to \mathbb{R}^m$.
	
	\begin{columns}[T]
		\begin{column}{0.48\textwidth}
			\begin{block}{Forward Mode}
				\begin{itemize}
					\item \textbf{Cost:} One pass per \textit{input} variable.
					\item \textbf{Efficient when:} Few inputs, many outputs ($n \ll m$).
					\item \textbf{Computes:} Jacobian-vector products ($J \cdot v$).
				\end{itemize}
			\end{block}
		\end{column}
		\hfill
		\begin{column}{0.48\textwidth}
			\begin{block}{Reverse Mode}
				\begin{itemize}
					\item \textbf{Cost:} One pass per \textit{output} variable.
					\item \textbf{Efficient when:} Many inputs, few outputs ($n \gg m$).
					\item \textbf{Computes:} Vector-Jacobian products ($v^T \cdot J$).
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
	
	\vspace{1cm}
	
	\textbf{Machine Learning context:} We have millions of parameters (inputs, $n$) and a single scalar loss function (output, $m=1$).
	\begin{center}
		\textbf{Reverse mode is the clear winner!}
	\end{center}
	\centering
	\href{https://kks32-courses.github.io/sciml/00-mlp/ad}{\beamergotobutton{AD Notebook}}
	
\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Gradient Descent Algorithm}

\begin{minipage}[t]{0.45\textwidth}
\textbf{Basic Algorithm:}
\begin{enumerate}
\item Initialize weights randomly
\item Loop until convergence:
\item \quad Compute gradient $\frac{\partial \mathcal{L}}{\partial \theta}$
\item \quad Update weights: $\theta \leftarrow \theta - \eta \frac{\partial \mathcal{L}}{\partial \theta}$
\item Return weights
\end{enumerate}

\mode<beamer>{
\begin{equation*}
\theta_{new} = \theta_{old} - \eta \nabla \mathcal{L}(\theta)
\end{equation*}

where $\eta$ is the learning rate.
}
\mode<handout>{
\vspace{1.5cm}
}

\end{minipage}
\hfill
\begin{minipage}[t]{0.53\textwidth}
\centering
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/sgd.png}
	\caption*{Gradient descent optimization}
\end{figure}
\href{https://kks32-courses.github.io/sciml/00-mlp/sgd/}{\beamergotobutton{SGD Demo}}
\end{minipage}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Width vs Approximation Quality}

\textbf{Hypothesis:} More neurons â†’ better approximation

\textbf{Experiment:} Train networks with 5, 10, 20, 50 neurons

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6\linewidth]{figs/width-quality.png}
	\caption*{Approximation quality vs network width}
\end{figure}
\end{frame}


%------------------------------------------------
\begin{frame}
\frametitle{Training/Validation}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figs/training-validation-fit.png}
	\caption*{Training and validation convergence}
\end{figure}
\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Training Convergence Analysis}
\begin{minipage}[t]{0.58\textwidth}
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.65\linewidth]{figs/training-convergence.png}
	\caption*{Training convergence and final loss vs width}
\end{figure}
\end{minipage}
\hfill
\begin{minipage}[t]{0.38\textwidth}
\textbf{Key Findings:}
\begin{itemize}
\item 50 neurons: $\sim$10x better than 5 neurons
\item Logarithmic improvement with width
\item Convergence rate similar across widths
\end{itemize}

\end{minipage}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Overfitting and Model Capacity}

\textbf{Problem:} High-capacity networks can memorize training data.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figs/overfitting.png}
	\caption*{Under and overfitting illustration}
\end{figure}

\textbf{Detection:} Monitor validation loss during training.

\textbf{Solutions:} More data, regularization, early stopping, simpler architectures.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Demonstrating Overfitting}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figs/overfitting-demo.png}
	\caption*{Normal (50 neurons) vs very wide (500 neurons) network}
\end{figure}

\textbf{Observation:} Very wide networks may generalize worse despite lower training loss.

\end{frame}

%------------------------------------------------
\section{The Need for Depth}

%------------------------------------------------
\begin{frame}
\frametitle{The XOR Problem: Historical Crisis}

\textbf{XOR Truth Table:}
\begin{center}
\begin{tabular}{cc|c}
$x_1$ & $x_2$ & $y$ \\
\hline
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0 \\
\end{tabular}
\end{center}

\textbf{The Crisis:} No single line can separate these classes!

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/xor-problem.png}
	\caption*{XOR is not linearly separable}
\end{figure}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{True Single-Layer vs Multi-Layer}

\textbf{Critical Distinction:}
\begin{itemize}
\item \textbf{True Single-Layer:} Input $\to$ Output (NO hidden layers)
\item \textbf{Multi-Layer:} Input $\to$ Hidden $\to$ Output (1+ hidden layers)
\end{itemize}

\mode<beamer>{
\textbf{True Single-Layer (fails):}
\begin{equation*}
y = \sigma(w_1 x_1 + w_2 x_2 + b)
\end{equation*}

\textbf{Multi-Layer (succeeds):}
\begin{align*}
h_1 &= \sigma(w_{11} x_1 + w_{12} x_2 + b_1) \\
h_2 &= \sigma(w_{21} x_1 + w_{22} x_2 + b_2) \\
y &= \sigma(v_1 h_1 + v_2 h_2 + b_3)
\end{align*}
}
\mode<handout>{
\vspace{3cm}
}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{XOR Solution: Decision Boundaries}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figs/xor-decision-boundaries.png}
	\caption*{Linear (fails) vs Non-linear (succeeds) decision boundaries}
\end{figure}

\textbf{Key Insight:} Hidden layers enable curved boundaries through non-linear transformations.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Mathematical Explanation}

\textbf{XOR Decomposition:}
\mode<beamer>{
\begin{align*}
h_1 &= \sigma(\text{weights}) \quad \text{($\approxeq$ OR gate)} \\
h_2 &= \sigma(\text{weights}) \quad \text{($\approxeq$ AND gate)} \\
y &= \sigma(v_1 h_1 + v_2 h_2 + b_3) \quad \text{($\approxeq$ OR AND NOT)}
\end{align*}

\textbf{Result:} XOR = (OR) AND (NOT AND) = compositional solution!
}
\mode<handout>{
\vspace{3cm}
}

\textbf{General Principle:} Complex functions = composition of simple functions.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Beyond XOR: High-Frequency Functions}

\textbf{Test Case:} $f(x) = \sin(\pi x) + 0.3\sin(10\pi x)$

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figs/high-freq-comparison.png}
	\caption*{Shallow (100 neurons) vs Deep (4 layers, 25 each) networks}
\end{figure}

\textbf{Result:} Deep networks achieve better performance with fewer parameters.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Historical Timeline: From Crisis to Revolution}

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Year} & \textbf{Event} & \textbf{Impact} \\
\midrule
1943 & McCulloch-Pitts neuron & Foundation laid \\
1957 & Rosenblatt's Perceptron & First learning success \\
\textbf{1969} & \textbf{Minsky \& Papert: XOR} & \textbf{Showed limits} \\
1970s-80s & ``AI Winter" & Funding dried up \\
1986 & Backpropagation & Enabled multi-layer training \\
1989 & Universal Approximation & Theoretical foundation \\
2006+ & Deep Learning Revolution & Depth proves essential \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Lesson:} XOR taught us that depth is necessity, not luxury.

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Four Key Insights on Depth}

\textbf{1. Representation Efficiency}
\begin{itemize}
\item Shallow: May need exponentially many neurons
\item Deep: Hierarchical composition is exponentially more efficient
\end{itemize}

\textbf{2. Feature Hierarchy}
\begin{itemize}
\item Layer 1: Simple features (edges, patterns)
\item Layer 2: Feature combinations (corners, textures)
\item Layer 3+: Complex abstractions (objects, concepts)
\end{itemize}

\textbf{3. Geometric Transformation}
\begin{itemize}
\item Each layer performs coordinate transformation
\item Deep networks "unfold" complex data manifolds
\end{itemize}

\textbf{4. Compositional Learning}
\begin{itemize}
\item Complex functions = composition of simple functions
\item Build complexity incrementally
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Summary and Conclusions}

\textbf{Key Takeaways:}
\begin{itemize}
\item Neural networks learn continuous functions from sparse data
\item Nonlinearity is essential for complex function approximation
\item Universal Approximation Theorem provides theoretical foundation
\item Width increases capacity but depth is more efficient
\item Historical XOR problem revealed importance of hidden layers
\item Deep networks enable hierarchical feature learning
\end{itemize}

\textbf{Applications:}
\begin{itemize}
\item Scientific computing and PDE solving
\item Function approximation and regression
\item Pattern recognition and classification
\item Physics-informed neural networks (PINNs)
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Questions?}

\centering
\Large Thank you!

\vspace{2cm}

\textbf{Contact:} \\
Krishna Kumar \\
\textit{krishnak@utexas.edu} \\
University of Texas at Austin

\end{frame}

\end{document}