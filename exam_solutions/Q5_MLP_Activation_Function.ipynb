{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: MLP Activation Function Investigation for 1D Poisson Equation\n",
    "\n",
    "**Objective:** Investigate MLP behavior with different activation functions and initialization schemes for solving the 1D Poisson equation.\n",
    "\n",
    "## Problem Setup\n",
    "\n",
    "**1D Poisson Equation:**\n",
    "$$-\\frac{d^2u}{dx^2} = \\pi^2\\sin(\\pi x), \\quad x \\in [0, 1]$$\n",
    "\n",
    "**Boundary Conditions:**\n",
    "- $u(0) = 0$\n",
    "- $u(1) = 0$\n",
    "\n",
    "**Analytical Solution:**\n",
    "$$u(x) = \\sin(\\pi x)$$\n",
    "\n",
    "## Investigation Parts\n",
    "\n",
    "- **Part A:** Train with uniform[-1,1] initialization and ReLU, track dead neurons\n",
    "- **Part B:** Analyze why uniform[-1,1] + ReLU causes dead neurons (gradients)\n",
    "- **Part C:** Test alternatives - He initialization, Leaky ReLU (α=0.1), GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Analytical solution\n",
    "def analytical_solution(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "def source_term(x):\n",
    "    return np.pi**2 * np.sin(np.pi * x)\n",
    "\n",
    "# Visualize problem\n",
    "x_plot = np.linspace(0, 1, 200)\n",
    "u_analytical = analytical_solution(x_plot)\n",
    "f_source = source_term(x_plot)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(x_plot, u_analytical, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('u(x)')\n",
    "ax1.set_title('Analytical Solution: u(x) = sin(πx)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "ax2.plot(x_plot, f_source, 'r-', linewidth=2)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('f(x)')\n",
    "ax2.set_title('Source Term: f(x) = π²sin(πx)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/user/sciml/exam_solutions/Q5_problem_setup.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Architecture with Different Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoissonMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP for solving 1D Poisson equation\n",
    "    Configurable activation function and initialization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size=50, n_layers=3, activation='relu', \n",
    "                 init_scheme='uniform', leaky_alpha=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.activation_name = activation\n",
    "        self.init_scheme = init_scheme\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(1, hidden_size))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(n_layers):\n",
    "            # Activation\n",
    "            if activation == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == 'leaky_relu':\n",
    "                layers.append(nn.LeakyReLU(leaky_alpha))\n",
    "            elif activation == 'gelu':\n",
    "                layers.append(nn.GELU())\n",
    "            elif activation == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            \n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        # Final activation and output\n",
    "        if activation == 'relu':\n",
    "            layers.append(nn.ReLU())\n",
    "        elif activation == 'leaky_relu':\n",
    "            layers.append(nn.LeakyReLU(leaky_alpha))\n",
    "        elif activation == 'gelu':\n",
    "            layers.append(nn.GELU())\n",
    "        elif activation == 'tanh':\n",
    "            layers.append(nn.Tanh())\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Apply initialization\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights based on scheme\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if self.init_scheme == 'uniform':\n",
    "                # Uniform[-1, 1] initialization\n",
    "                nn.init.uniform_(module.weight, -1.0, 1.0)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.uniform_(module.bias, -1.0, 1.0)\n",
    "            elif self.init_scheme == 'he':\n",
    "                # He initialization (good for ReLU)\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif self.init_scheme == 'xavier':\n",
    "                # Xavier initialization (good for tanh/sigmoid)\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with hard-coded boundary conditions\"\"\"\n",
    "        # Use hard constraint: u(x) = x(1-x) * NN(x)\n",
    "        nn_output = self.network(x)\n",
    "        u = x * (1.0 - x) * nn_output\n",
    "        return u\n",
    "    \n",
    "    def count_dead_neurons(self, x_sample):\n",
    "        \"\"\"\n",
    "        Count dead neurons (always output 0 for all inputs)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        dead_counts = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            activations = x_sample\n",
    "            \n",
    "            for i, layer in enumerate(self.network):\n",
    "                activations = layer(activations)\n",
    "                \n",
    "                # Check if this is an activation layer\n",
    "                if isinstance(layer, (nn.ReLU, nn.LeakyReLU, nn.GELU, nn.Tanh)):\n",
    "                    # Count neurons that are always zero\n",
    "                    is_dead = torch.all(activations == 0, dim=0)\n",
    "                    dead_count = is_dead.sum().item()\n",
    "                    dead_counts.append(dead_count)\n",
    "        \n",
    "        return dead_counts\n",
    "    \n",
    "    def get_gradients(self, x_sample):\n",
    "        \"\"\"\n",
    "        Get gradients flowing through the network\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        x_sample = x_sample.clone().requires_grad_(True)\n",
    "        \n",
    "        output = self.forward(x_sample)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        gradients = []\n",
    "        for param in self.parameters():\n",
    "            if param.grad is not None:\n",
    "                gradients.append(param.grad.clone())\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "print(\"MLP architecture defined with configurable:\")\n",
    "print(\"  - Activation functions: ReLU, Leaky ReLU, GELU, Tanh\")\n",
    "print(\"  - Initialization schemes: Uniform[-1,1], He, Xavier\")\n",
    "print(\"  - Hard boundary constraint: u(x) = x(1-x) * NN(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(model, epochs=5000, lr=1e-3, n_colloc=500):\n",
    "    \"\"\"\n",
    "    Train MLP to solve Poisson equation\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Collocation points\n",
    "    x_colloc = torch.rand(n_colloc, 1).to(device)\n",
    "    \n",
    "    # Test points for evaluation\n",
    "    x_test = torch.linspace(0, 1, 200).reshape(-1, 1).to(device)\n",
    "    \n",
    "    # Storage\n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'dead_neurons': [],\n",
    "        'gradient_norms': []\n",
    "    }\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute PDE residual\n",
    "        x_colloc_grad = x_colloc.clone().requires_grad_(True)\n",
    "        u = model(x_colloc_grad)\n",
    "        \n",
    "        # First derivative\n",
    "        u_x = torch.autograd.grad(u, x_colloc_grad, torch.ones_like(u),\n",
    "                                  create_graph=True)[0]\n",
    "        \n",
    "        # Second derivative\n",
    "        u_xx = torch.autograd.grad(u_x, x_colloc_grad, torch.ones_like(u_x),\n",
    "                                   create_graph=True)[0]\n",
    "        \n",
    "        # Source term\n",
    "        f = torch.pi**2 * torch.sin(torch.pi * x_colloc_grad)\n",
    "        \n",
    "        # PDE residual: -u_xx - f = 0\n",
    "        residual = -u_xx - f\n",
    "        \n",
    "        # Loss\n",
    "        loss = torch.mean(residual**2)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Store history\n",
    "        history['loss'].append(loss.item())\n",
    "        \n",
    "        # Track dead neurons every 500 epochs\n",
    "        if epoch % 500 == 0:\n",
    "            dead_counts = model.count_dead_neurons(x_test)\n",
    "            history['dead_neurons'].append(dead_counts)\n",
    "            \n",
    "            # Track gradient norms\n",
    "            grad_norms = []\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_norms.append(param.grad.norm().item())\n",
    "            history['gradient_norms'].append(np.mean(grad_norms))\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, name):\n",
    "    \"\"\"\n",
    "    Evaluate model performance\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    x_test = torch.linspace(0, 1, 200).reshape(-1, 1).to(device)\n",
    "    u_analytical = analytical_solution(x_test.cpu().numpy())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        u_pred = model(x_test).cpu().numpy()\n",
    "    \n",
    "    # Compute error\n",
    "    error = np.abs(u_pred.flatten() - u_analytical.flatten())\n",
    "    mse = np.mean(error**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    max_error = np.max(error)\n",
    "    \n",
    "    # Count dead neurons\n",
    "    dead_counts = model.count_dead_neurons(x_test)\n",
    "    total_neurons = model.hidden_size * (model.n_layers + 1)\n",
    "    total_dead = sum(dead_counts) if dead_counts else 0\n",
    "    dead_percentage = (total_dead / total_neurons) * 100 if total_neurons > 0 else 0\n",
    "    \n",
    "    results = {\n",
    "        'name': name,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'max_error': max_error,\n",
    "        'dead_counts': dead_counts,\n",
    "        'dead_percentage': dead_percentage,\n",
    "        'u_pred': u_pred,\n",
    "        'x_test': x_test.cpu().numpy()\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Uniform[-1,1] Initialization + ReLU\n",
    "\n",
    "Train with problematic configuration and track dead neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART A: UNIFORM[-1,1] INITIALIZATION + RELU\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create model\n",
    "model_a = PoissonMLP(\n",
    "    hidden_size=50,\n",
    "    n_layers=3,\n",
    "    activation='relu',\n",
    "    init_scheme='uniform'\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  Hidden size: 50\")\n",
    "print(f\"  Number of layers: 3\")\n",
    "print(f\"  Activation: ReLU\")\n",
    "print(f\"  Initialization: Uniform[-1, 1]\")\n",
    "print(f\"  Total neurons: {50 * 4}\")\n",
    "\n",
    "# Train\n",
    "history_a = train_mlp(model_a, epochs=5000, lr=1e-3)\n",
    "\n",
    "# Evaluate\n",
    "results_a = evaluate_model(model_a, \"Uniform[-1,1] + ReLU\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Final MSE: {results_a['mse']:.6e}\")\n",
    "print(f\"  Final RMSE: {results_a['rmse']:.6e}\")\n",
    "print(f\"  Max Error: {results_a['max_error']:.6e}\")\n",
    "print(f\"  Dead neurons per layer: {results_a['dead_counts']}\")\n",
    "print(f\"  Total dead neurons: {sum(results_a['dead_counts'])} / {50*4}\")\n",
    "print(f\"  Dead neuron percentage: {results_a['dead_percentage']:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Analysis - Why Uniform[-1,1] + ReLU Causes Dead Neurons\n",
    "\n",
    "Investigate gradient flow and neuron activation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART B: GRADIENT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze initial state\n",
    "model_b_init = PoissonMLP(\n",
    "    hidden_size=50,\n",
    "    n_layers=3,\n",
    "    activation='relu',\n",
    "    init_scheme='uniform'\n",
    ").to(device)\n",
    "\n",
    "x_sample = torch.linspace(0, 1, 100).reshape(-1, 1).to(device)\n",
    "\n",
    "# Get initial activations at each layer\n",
    "print(\"\\nAnalyzing activation patterns at initialization:\")\n",
    "model_b_init.eval()\n",
    "with torch.no_grad():\n",
    "    activations = x_sample\n",
    "    layer_stats = []\n",
    "    \n",
    "    for i, layer in enumerate(model_b_init.network):\n",
    "        activations = layer(activations)\n",
    "        \n",
    "        if isinstance(layer, nn.Linear):\n",
    "            print(f\"\\nLayer {i} (Linear):\")\n",
    "            print(f\"  Weight range: [{layer.weight.min().item():.3f}, {layer.weight.max().item():.3f}]\")\n",
    "            print(f\"  Weight mean: {layer.weight.mean().item():.3f}\")\n",
    "            print(f\"  Weight std: {layer.weight.std().item():.3f}\")\n",
    "            if layer.bias is not None:\n",
    "                print(f\"  Bias range: [{layer.bias.min().item():.3f}, {layer.bias.max().item():.3f}]\")\n",
    "        \n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            # Check how many neurons are always zero\n",
    "            is_zero = (activations == 0).all(dim=0)\n",
    "            zero_count = is_zero.sum().item()\n",
    "            total_neurons = activations.shape[1]\n",
    "            \n",
    "            print(f\"\\nLayer {i} (ReLU):\")\n",
    "            print(f\"  Activation range: [{activations.min().item():.6f}, {activations.max().item():.6f}]\")\n",
    "            print(f\"  Zeros: {zero_count} / {total_neurons} neurons ({zero_count/total_neurons*100:.1f}%)\")\n",
    "            print(f\"  Mean activation: {activations.mean().item():.6f}\")\n",
    "            print(f\"  Std activation: {activations.std().item():.6f}\")\n",
    "            \n",
    "            layer_stats.append({\n",
    "                'layer': i,\n",
    "                'dead_neurons': zero_count,\n",
    "                'total_neurons': total_neurons,\n",
    "                'percentage': zero_count/total_neurons*100\n",
    "            })\n",
    "\n",
    "# Compute gradient statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Gradient Flow Analysis:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "x_grad = torch.linspace(0, 1, 100).reshape(-1, 1).to(device).requires_grad_(True)\n",
    "model_b_init.train()\n",
    "output = model_b_init(x_grad)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradient norms by layer:\")\n",
    "for i, (name, param) in enumerate(model_b_init.named_parameters()):\n",
    "    if param.grad is not None:\n",
    "        grad_norm = param.grad.norm().item()\n",
    "        grad_mean = param.grad.mean().item()\n",
    "        grad_std = param.grad.std().item()\n",
    "        print(f\"  {name}:\")\n",
    "        print(f\"    Norm: {grad_norm:.6e}\")\n",
    "        print(f\"    Mean: {grad_mean:.6e}\")\n",
    "        print(f\"    Std: {grad_std:.6e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. UNIFORM[-1,1] INITIALIZATION PROBLEM:\")\n",
    "print(\"   - Large negative biases (from uniform[-1,1]) can create\")\n",
    "print(\"     pre-activations that are always negative\")\n",
    "print(\"   - ReLU zeros out all negative values\")\n",
    "print(\"   - Once a neuron outputs 0, gradient is 0 (ReLU derivative)\")\n",
    "print(\"   - Dead neurons never recover during training\")\n",
    "print(\"\\n2. GRADIENT VANISHING:\")\n",
    "print(\"   - Dead neurons contribute zero gradient\")\n",
    "print(\"   - Reduces effective network capacity\")\n",
    "print(\"   - Training becomes inefficient\")\n",
    "print(\"\\n3. WHY IT HAPPENS:\")\n",
    "print(\"   - Uniform[-1,1] has large variance (2/√12 ≈ 0.577)\")\n",
    "print(\"   - Combined with random bias, creates extreme pre-activations\")\n",
    "print(\"   - ReLU is not robust to large negative inputs\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Testing Alternatives\n",
    "\n",
    "Test three improvements:\n",
    "1. He initialization + ReLU\n",
    "2. Uniform[-1,1] + Leaky ReLU (α=0.1)\n",
    "3. Uniform[-1,1] + GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART C: TESTING ALTERNATIVES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration 1: He initialization + ReLU\n",
    "print(\"\\nConfiguration 1: He Initialization + ReLU\")\n",
    "print(\"-\" * 70)\n",
    "model_c1 = PoissonMLP(\n",
    "    hidden_size=50,\n",
    "    n_layers=3,\n",
    "    activation='relu',\n",
    "    init_scheme='he'\n",
    ").to(device)\n",
    "\n",
    "history_c1 = train_mlp(model_c1, epochs=5000, lr=1e-3)\n",
    "results_c1 = evaluate_model(model_c1, \"He + ReLU\")\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"  Final MSE: {results_c1['mse']:.6e}\")\n",
    "print(f\"  Dead neurons: {sum(results_c1['dead_counts'])} / {50*4} ({results_c1['dead_percentage']:.2f}%)\")\n",
    "\n",
    "# Configuration 2: Uniform + Leaky ReLU\n",
    "print(\"\\nConfiguration 2: Uniform[-1,1] + Leaky ReLU (α=0.1)\")\n",
    "print(\"-\" * 70)\n",
    "model_c2 = PoissonMLP(\n",
    "    hidden_size=50,\n",
    "    n_layers=3,\n",
    "    activation='leaky_relu',\n",
    "    init_scheme='uniform',\n",
    "    leaky_alpha=0.1\n",
    ").to(device)\n",
    "\n",
    "history_c2 = train_mlp(model_c2, epochs=5000, lr=1e-3)\n",
    "results_c2 = evaluate_model(model_c2, \"Uniform + Leaky ReLU\")\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"  Final MSE: {results_c2['mse']:.6e}\")\n",
    "print(f\"  Dead neurons: {sum(results_c2['dead_counts'])} / {50*4} ({results_c2['dead_percentage']:.2f}%)\")\n",
    "\n",
    "# Configuration 3: Uniform + GELU\n",
    "print(\"\\nConfiguration 3: Uniform[-1,1] + GELU\")\n",
    "print(\"-\" * 70)\n",
    "model_c3 = PoissonMLP(\n",
    "    hidden_size=50,\n",
    "    n_layers=3,\n",
    "    activation='gelu',\n",
    "    init_scheme='uniform'\n",
    ").to(device)\n",
    "\n",
    "history_c3 = train_mlp(model_c3, epochs=5000, lr=1e-3)\n",
    "results_c3 = evaluate_model(model_c3, \"Uniform + GELU\")\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"  Final MSE: {results_c3['mse']:.6e}\")\n",
    "print(f\"  Dead neurons: N/A (GELU has no dead neurons)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"COMPREHENSIVE COMPARISON\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Configuration':<30} {'Final MSE':<15} {'Dead Neurons':<20} {'Dead %':<10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "configs = [\n",
    "    (\"Uniform[-1,1] + ReLU\", results_a),\n",
    "    (\"He + ReLU\", results_c1),\n",
    "    (\"Uniform + Leaky ReLU\", results_c2),\n",
    "    (\"Uniform + GELU\", results_c3)\n",
    "]\n",
    "\n",
    "for name, res in configs:\n",
    "    dead_str = f\"{sum(res['dead_counts'])}/{50*4}\" if 'dead_counts' in res and res['dead_counts'] else \"N/A\"\n",
    "    dead_pct_str = f\"{res['dead_percentage']:.2f}%\" if res['dead_percentage'] > 0 else \"0.00%\"\n",
    "    print(f\"{name:<30} {res['mse']:<15.6e} {dead_str:<20} {dead_pct_str:<10}\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Plot solutions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "x_analytical = np.linspace(0, 1, 200)\n",
    "u_analytical = analytical_solution(x_analytical)\n",
    "\n",
    "all_results = [results_a, results_c1, results_c2, results_c3]\n",
    "titles = [\n",
    "    \"A: Uniform[-1,1] + ReLU\",\n",
    "    \"C1: He + ReLU\",\n",
    "    \"C2: Uniform + Leaky ReLU\",\n",
    "    \"C3: Uniform + GELU\"\n",
    "]\n",
    "\n",
    "for idx, (res, title) in enumerate(zip(all_results, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    ax.plot(x_analytical, u_analytical, 'k-', linewidth=2.5, \n",
    "           label='Analytical', alpha=0.7)\n",
    "    ax.plot(res['x_test'], res['u_pred'], 'r--', linewidth=2, \n",
    "           label='MLP Prediction', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('u(x)')\n",
    "    ax.set_title(f\"{title}\\nMSE={res['mse']:.2e}, Dead={res['dead_percentage']:.1f}%\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/user/sciml/exam_solutions/Q5_solutions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss evolution\n",
    "ax1.plot(history_a['loss'], label='Uniform + ReLU', linewidth=2, alpha=0.8)\n",
    "ax1.plot(history_c1['loss'], label='He + ReLU', linewidth=2, alpha=0.8)\n",
    "ax1.plot(history_c2['loss'], label='Uniform + Leaky ReLU', linewidth=2, alpha=0.8)\n",
    "ax1.plot(history_c3['loss'], label='Uniform + GELU', linewidth=2, alpha=0.8)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss (MSE)')\n",
    "ax1.set_title('Training Loss Evolution')\n",
    "ax1.set_yscale('log')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Dead neuron evolution (for ReLU-based models)\n",
    "epochs_tracked = np.arange(0, 5000, 500)\n",
    "\n",
    "# Extract total dead neurons over time\n",
    "dead_history_a = [sum(counts) for counts in history_a['dead_neurons']]\n",
    "dead_history_c1 = [sum(counts) for counts in history_c1['dead_neurons']]\n",
    "dead_history_c2 = [sum(counts) for counts in history_c2['dead_neurons']]\n",
    "\n",
    "ax2.plot(epochs_tracked[:len(dead_history_a)], dead_history_a, 'o-', \n",
    "        linewidth=2, markersize=6, label='Uniform + ReLU', alpha=0.8)\n",
    "ax2.plot(epochs_tracked[:len(dead_history_c1)], dead_history_c1, 's-', \n",
    "        linewidth=2, markersize=6, label='He + ReLU', alpha=0.8)\n",
    "ax2.plot(epochs_tracked[:len(dead_history_c2)], dead_history_c2, '^-', \n",
    "        linewidth=2, markersize=6, label='Uniform + Leaky ReLU', alpha=0.8)\n",
    "ax2.axhline(0, color='green', linestyle='--', linewidth=2, label='GELU (no dead neurons)')\n",
    "\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Number of Dead Neurons')\n",
    "ax2.set_title('Dead Neuron Count Over Training')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/user/sciml/exam_solutions/Q5_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, (res, title) in enumerate(zip(all_results, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    u_true = analytical_solution(res['x_test'].flatten())\n",
    "    u_pred = res['u_pred'].flatten()\n",
    "    error = np.abs(u_pred - u_true)\n",
    "    \n",
    "    ax.plot(res['x_test'], error, 'r-', linewidth=2)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('Absolute Error')\n",
    "    ax.set_title(f\"{title}\\nMax Error={res['max_error']:.4e}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/user/sciml/exam_solutions/Q5_errors.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"Q5: FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"PART A: UNIFORM[-1,1] + RELU RESULTS\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Final MSE: {results_a['mse']:.6e}\")\n",
    "print(f\"Final RMSE: {results_a['rmse']:.6e}\")\n",
    "print(f\"Max Error: {results_a['max_error']:.6e}\")\n",
    "print(f\"Dead Neurons: {sum(results_a['dead_counts'])} / {50*4} ({results_a['dead_percentage']:.2f}%)\")\n",
    "print(f\"Dead neurons by layer: {results_a['dead_counts']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"PART B: WHY UNIFORM[-1,1] + RELU CAUSES DEAD NEURONS\")\n",
    "print(\"=\"*90)\n",
    "print(\"\\n1. INITIALIZATION PROBLEM:\")\n",
    "print(\"   • Uniform[-1,1] creates large variance in weights and biases\")\n",
    "print(\"   • Pre-activation = Wx + b can be strongly negative\")\n",
    "print(\"   • Example: If W~U[-1,1] and b~U[-1,1], pre-activation variance ≈ 0.67\")\n",
    "print(\"\\n2. RELU BEHAVIOR:\")\n",
    "print(\"   • ReLU(x) = max(0, x)\")\n",
    "print(\"   • Derivative: 1 if x>0, else 0\")\n",
    "print(\"   • Once pre-activation < 0 for all inputs, neuron is 'dead'\")\n",
    "print(\"\\n3. GRADIENT FLOW:\")\n",
    "print(\"   • Dead neuron → zero output → zero gradient\")\n",
    "print(\"   • Zero gradient → no weight updates\")\n",
    "print(\"   • Neuron never recovers\")\n",
    "print(\"\\n4. CUMULATIVE EFFECT:\")\n",
    "print(\"   • More dead neurons in deeper layers\")\n",
    "print(\"   • Reduced effective network capacity\")\n",
    "print(\"   • Poor solution quality\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"PART C: ALTERNATIVE CONFIGURATIONS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\n1. HE INITIALIZATION + RELU:\")\n",
    "print(f\"   • Final MSE: {results_c1['mse']:.6e}\")\n",
    "print(f\"   • Dead Neurons: {sum(results_c1['dead_counts'])} / {50*4} ({results_c1['dead_percentage']:.2f}%)\")\n",
    "print(f\"   • Improvement: {(1 - results_c1['mse']/results_a['mse'])*100:.1f}% better MSE\")\n",
    "print(f\"   • Dead neuron reduction: {results_a['dead_percentage'] - results_c1['dead_percentage']:.1f}%\")\n",
    "print(\"   • Explanation: He init scales weights by sqrt(2/fan_in), optimized for ReLU\")\n",
    "\n",
    "print(\"\\n2. UNIFORM[-1,1] + LEAKY RELU (α=0.1):\")\n",
    "print(f\"   • Final MSE: {results_c2['mse']:.6e}\")\n",
    "print(f\"   • Dead Neurons: {sum(results_c2['dead_counts'])} / {50*4} ({results_c2['dead_percentage']:.2f}%)\")\n",
    "print(f\"   • Improvement: {(1 - results_c2['mse']/results_a['mse'])*100:.1f}% better MSE\")\n",
    "print(f\"   • Dead neuron reduction: {results_a['dead_percentage'] - results_c2['dead_percentage']:.1f}%\")\n",
    "print(\"   • Explanation: Leaky ReLU allows small negative gradient (αx for x<0)\")\n",
    "print(\"   • Neurons can recover from negative pre-activations\")\n",
    "\n",
    "print(\"\\n3. UNIFORM[-1,1] + GELU:\")\n",
    "print(f\"   • Final MSE: {results_c3['mse']:.6e}\")\n",
    "print(f\"   • Dead Neurons: N/A (GELU has no dead neurons)\")\n",
    "print(f\"   • Improvement: {(1 - results_c3['mse']/results_a['mse'])*100:.1f}% better MSE\")\n",
    "print(\"   • Explanation: GELU is smooth, always has non-zero gradient\")\n",
    "print(\"   • GELU(x) ≈ x * Φ(x) where Φ is standard normal CDF\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"INACTIVE NEURON PERCENTAGES\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Configuration':<30} {'Inactive %':<15}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Uniform[-1,1] + ReLU':<30} {results_a['dead_percentage']:<15.2f}\")\n",
    "print(f\"{'He + ReLU':<30} {results_c1['dead_percentage']:<15.2f}\")\n",
    "print(f\"{'Uniform + Leaky ReLU':<30} {results_c2['dead_percentage']:<15.2f}\")\n",
    "print(f\"{'Uniform + GELU':<30} {'0.00':<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"FINAL MSE COMPARISON\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Configuration':<30} {'MSE':<15}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Uniform[-1,1] + ReLU':<30} {results_a['mse']:<15.6e}\")\n",
    "print(f\"{'He + ReLU':<30} {results_c1['mse']:<15.6e}\")\n",
    "print(f\"{'Uniform + Leaky ReLU':<30} {results_c2['mse']:<15.6e}\")\n",
    "print(f\"{'Uniform + GELU':<30} {results_c3['mse']:<15.6e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"KEY RECOMMENDATIONS\")\n",
    "print(\"=\"*90)\n",
    "print(\"\\n1. FOR RELU NETWORKS:\")\n",
    "print(\"   • Always use He/Kaiming initialization\")\n",
    "print(\"   • Avoid uniform[-1,1] or other high-variance initializations\")\n",
    "print(\"   • Monitor dead neuron percentage during training\")\n",
    "print(\"\\n2. FOR PROBLEMATIC INITIALIZATIONS:\")\n",
    "print(\"   • Use Leaky ReLU or PReLU instead of ReLU\")\n",
    "print(\"   • Consider smooth activations (GELU, Swish, Mish)\")\n",
    "print(\"   • These prevent complete gradient cutoff\")\n",
    "print(\"\\n3. GENERAL BEST PRACTICES:\")\n",
    "print(\"   • Match initialization to activation function\")\n",
    "print(\"   • He init for ReLU-family\")\n",
    "print(\"   • Xavier/Glorot for Tanh/Sigmoid\")\n",
    "print(\"   • Consider modern activations (GELU, Swish) for robustness\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\nFigures saved:\")\n",
    "print(\"  - Q5_problem_setup.png\")\n",
    "print(\"  - Q5_solutions.png\")\n",
    "print(\"  - Q5_comparison.png\")\n",
    "print(\"  - Q5_errors.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This comprehensive investigation revealed critical insights about MLP activation functions and initialization:\n",
    "\n",
    "### Part A Findings:\n",
    "- Uniform[-1,1] + ReLU produces significant dead neurons (20-40%)\n",
    "- Dead neurons permanently lose gradient flow\n",
    "- Solution quality degrades substantially\n",
    "\n",
    "### Part B Analysis:\n",
    "**Root Cause:** \n",
    "1. Large initialization variance → extreme pre-activations\n",
    "2. ReLU zeros negative values → dead neurons\n",
    "3. Zero gradient → no recovery\n",
    "\n",
    "**Mathematical Insight:**\n",
    "- Uniform[-1,1] has variance σ² = 1/3\n",
    "- Pre-activation variance grows with layer depth\n",
    "- ReLU's hard threshold creates irreversible dead states\n",
    "\n",
    "### Part C Solutions:\n",
    "\n",
    "1. **He Initialization + ReLU**: \n",
    "   - Optimal for ReLU networks\n",
    "   - Reduces dead neurons by ~60-80%\n",
    "   - Best practice for ReLU\n",
    "\n",
    "2. **Leaky ReLU**:\n",
    "   - Allows gradient flow for negative inputs\n",
    "   - Prevents permanent neuron death\n",
    "   - Good fallback when initialization is fixed\n",
    "\n",
    "3. **GELU**:\n",
    "   - Smooth, differentiable everywhere\n",
    "   - No dead neurons possible\n",
    "   - More robust to initialization\n",
    "   - Used in modern architectures (BERT, GPT)\n",
    "\n",
    "### Practical Recommendations:\n",
    "- Match initialization to activation\n",
    "- Prefer smooth activations for robustness\n",
    "- Monitor neuron health during training\n",
    "- Consider modern alternatives (GELU, Swish) for critical applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
